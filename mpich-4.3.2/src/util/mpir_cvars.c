/*
 * Copyright (C) by Argonne National Laboratory
 *     See COPYRIGHT in top-level directory
 */
/* automatically generated
 *   by:   ./maint/extractcvars
 *   at:   Mon Oct  6 16:14:35 2025 UTC
 *
 * DO NOT EDIT!!!
 */

#include "mpiimpl.h"

/* Actual storage for cvars */
int MPIR_CVAR_BARRIER_INTRA_ALGORITHM;
int MPIR_CVAR_BARRIER_INTER_ALGORITHM;
int MPIR_CVAR_BARRIER_DISSEM_KVAL;
int MPIR_CVAR_BARRIER_RECEXCH_KVAL;
int MPIR_CVAR_BARRIER_RECEXCH_SINGLE_PHASE_RECV;
int MPIR_CVAR_IBARRIER_RECEXCH_KVAL;
int MPIR_CVAR_IBARRIER_DISSEM_KVAL;
int MPIR_CVAR_IBARRIER_INTRA_ALGORITHM;
int MPIR_CVAR_IBARRIER_INTER_ALGORITHM;
int MPIR_CVAR_BCAST_MIN_PROCS;
int MPIR_CVAR_BCAST_SHORT_MSG_SIZE;
int MPIR_CVAR_BCAST_LONG_MSG_SIZE;
int MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE;
int MPIR_CVAR_BCAST_INTRA_ALGORITHM;
int MPIR_CVAR_BCAST_TREE_KVAL;
const char * MPIR_CVAR_BCAST_TREE_TYPE;
int MPIR_CVAR_BCAST_TOPO_REORDER_ENABLE;
int MPIR_CVAR_BCAST_TOPO_OVERHEAD;
int MPIR_CVAR_BCAST_TOPO_DIFF_GROUPS;
int MPIR_CVAR_BCAST_TOPO_DIFF_SWITCHES;
int MPIR_CVAR_BCAST_TOPO_SAME_SWITCHES;
int MPIR_CVAR_BCAST_IS_NON_BLOCKING;
int MPIR_CVAR_BCAST_TREE_PIPELINE_CHUNK_SIZE;
int MPIR_CVAR_BCAST_RECV_PRE_POST;
int MPIR_CVAR_BCAST_INTER_ALGORITHM;
int MPIR_CVAR_IBCAST_TREE_KVAL;
const char * MPIR_CVAR_IBCAST_TREE_TYPE;
int MPIR_CVAR_IBCAST_TREE_PIPELINE_CHUNK_SIZE;
int MPIR_CVAR_IBCAST_RING_CHUNK_SIZE;
int MPIR_CVAR_IBCAST_INTRA_ALGORITHM;
int MPIR_CVAR_IBCAST_SCATTERV_KVAL;
int MPIR_CVAR_IBCAST_ALLGATHERV_RECEXCH_KVAL;
int MPIR_CVAR_IBCAST_INTER_ALGORITHM;
int MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE;
int MPIR_CVAR_GATHER_INTRA_ALGORITHM;
int MPIR_CVAR_GATHER_INTER_ALGORITHM;
int MPIR_CVAR_IGATHER_INTRA_ALGORITHM;
int MPIR_CVAR_IGATHER_TREE_KVAL;
int MPIR_CVAR_IGATHER_INTER_ALGORITHM;
int MPIR_CVAR_GATHERV_INTRA_ALGORITHM;
int MPIR_CVAR_GATHERV_INTER_ALGORITHM;
int MPIR_CVAR_IGATHERV_INTRA_ALGORITHM;
int MPIR_CVAR_IGATHERV_INTER_ALGORITHM;
int MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE;
int MPIR_CVAR_SCATTER_INTRA_ALGORITHM;
int MPIR_CVAR_SCATTER_INTER_ALGORITHM;
int MPIR_CVAR_ISCATTER_INTRA_ALGORITHM;
int MPIR_CVAR_ISCATTER_TREE_KVAL;
int MPIR_CVAR_ISCATTER_INTER_ALGORITHM;
int MPIR_CVAR_SCATTERV_INTRA_ALGORITHM;
int MPIR_CVAR_SCATTERV_INTER_ALGORITHM;
int MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM;
int MPIR_CVAR_ISCATTERV_INTER_ALGORITHM;
int MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE;
int MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE;
int MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM;
int MPIR_CVAR_ALLGATHER_BRUCKS_KVAL;
int MPIR_CVAR_ALLGATHER_RECEXCH_KVAL;
int MPIR_CVAR_ALLGATHER_RECEXCH_SINGLE_PHASE_RECV;
int MPIR_CVAR_ALLGATHER_INTER_ALGORITHM;
int MPIR_CVAR_IALLGATHER_RECEXCH_KVAL;
int MPIR_CVAR_IALLGATHER_BRUCKS_KVAL;
int MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM;
int MPIR_CVAR_IALLGATHER_INTER_ALGORITHM;
int MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE;
int MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM;
int MPIR_CVAR_ALLGATHERV_INTER_ALGORITHM;
int MPIR_CVAR_IALLGATHERV_RECEXCH_KVAL;
int MPIR_CVAR_IALLGATHERV_BRUCKS_KVAL;
int MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM;
int MPIR_CVAR_IALLGATHERV_INTER_ALGORITHM;
int MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE;
int MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE;
int MPIR_CVAR_ALLTOALL_THROTTLE;
int MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM;
int MPIR_CVAR_ALLTOALL_BRUCKS_KVAL;
int MPIR_CVAR_ALLTOALL_INTER_ALGORITHM;
int MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM;
int MPIR_CVAR_IALLTOALL_INTER_ALGORITHM;
int MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM;
int MPIR_CVAR_ALLTOALLV_INTER_ALGORITHM;
int MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM;
int MPIR_CVAR_IALLTOALLV_INTER_ALGORITHM;
int MPIR_CVAR_IALLTOALLV_SCATTERED_OUTSTANDING_TASKS;
int MPIR_CVAR_IALLTOALLV_SCATTERED_BATCH_SIZE;
int MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM;
int MPIR_CVAR_ALLTOALLW_INTER_ALGORITHM;
int MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM;
int MPIR_CVAR_IALLTOALLW_INTER_ALGORITHM;
int MPIR_CVAR_REDUCE_SHORT_MSG_SIZE;
int MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE;
int MPIR_CVAR_REDUCE_INTRA_ALGORITHM;
int MPIR_CVAR_REDUCE_INTER_ALGORITHM;
int MPIR_CVAR_IREDUCE_TREE_KVAL;
const char * MPIR_CVAR_IREDUCE_TREE_TYPE;
int MPIR_CVAR_IREDUCE_TOPO_REORDER_ENABLE;
int MPIR_CVAR_IREDUCE_TOPO_OVERHEAD;
int MPIR_CVAR_IREDUCE_TOPO_DIFF_GROUPS;
int MPIR_CVAR_IREDUCE_TOPO_DIFF_SWITCHES;
int MPIR_CVAR_IREDUCE_TOPO_SAME_SWITCHES;
int MPIR_CVAR_IREDUCE_TREE_PIPELINE_CHUNK_SIZE;
int MPIR_CVAR_IREDUCE_RING_CHUNK_SIZE;
int MPIR_CVAR_IREDUCE_TREE_BUFFER_PER_CHILD;
int MPIR_CVAR_IREDUCE_INTRA_ALGORITHM;
int MPIR_CVAR_IREDUCE_INTER_ALGORITHM;
int MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE;
int MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE;
int MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM;
int MPIR_CVAR_ALLREDUCE_RECURSIVE_MULTIPLYING_KVAL;
const char * MPIR_CVAR_ALLREDUCE_TREE_TYPE;
int MPIR_CVAR_ALLREDUCE_TREE_KVAL;
int MPIR_CVAR_ALLREDUCE_TOPO_REORDER_ENABLE;
int MPIR_CVAR_ALLREDUCE_TOPO_OVERHEAD;
int MPIR_CVAR_ALLREDUCE_TOPO_DIFF_GROUPS;
int MPIR_CVAR_ALLREDUCE_TOPO_DIFF_SWITCHES;
int MPIR_CVAR_ALLREDUCE_TOPO_SAME_SWITCHES;
int MPIR_CVAR_ALLREDUCE_TREE_PIPELINE_CHUNK_SIZE;
int MPIR_CVAR_ALLREDUCE_TREE_BUFFER_PER_CHILD;
int MPIR_CVAR_ALLREDUCE_RECEXCH_KVAL;
int MPIR_CVAR_ALLREDUCE_RECEXCH_SINGLE_PHASE_RECV;
int MPIR_CVAR_ALLREDUCE_INTER_ALGORITHM;
int MPIR_CVAR_IALLREDUCE_TREE_KVAL;
const char * MPIR_CVAR_IALLREDUCE_TREE_TYPE;
int MPIR_CVAR_IALLREDUCE_TREE_PIPELINE_CHUNK_SIZE;
int MPIR_CVAR_IALLREDUCE_TREE_BUFFER_PER_CHILD;
int MPIR_CVAR_IALLREDUCE_RECEXCH_KVAL;
int MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM;
int MPIR_CVAR_IALLREDUCE_INTER_ALGORITHM;
int MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE;
int MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM;
int MPIR_CVAR_REDUCE_SCATTER_INTER_ALGORITHM;
int MPIR_CVAR_IREDUCE_SCATTER_RECEXCH_KVAL;
int MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM;
int MPIR_CVAR_IREDUCE_SCATTER_INTER_ALGORITHM;
int MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM;
int MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM;
int MPIR_CVAR_IREDUCE_SCATTER_BLOCK_RECEXCH_KVAL;
int MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM;
int MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM;
int MPIR_CVAR_SCAN_INTRA_ALGORITHM;
int MPIR_CVAR_ISCAN_INTRA_ALGORITHM;
int MPIR_CVAR_EXSCAN_INTRA_ALGORITHM;
int MPIR_CVAR_IEXSCAN_INTRA_ALGORITHM;
int MPIR_CVAR_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM;
int MPIR_CVAR_NEIGHBOR_ALLGATHER_INTER_ALGORITHM;
int MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM;
int MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM;
int MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM;
int MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM;
int MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM;
int MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM;
int MPIR_CVAR_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM;
int MPIR_CVAR_NEIGHBOR_ALLTOALL_INTER_ALGORITHM;
int MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM;
int MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM;
int MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM;
int MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM;
int MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM;
int MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM;
int MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM;
int MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM;
int MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM;
int MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM;
int MPIR_CVAR_BARRIER_DEVICE_COLLECTIVE;
int MPIR_CVAR_IBARRIER_DEVICE_COLLECTIVE;
int MPIR_CVAR_BARRIER_INIT_DEVICE_COLLECTIVE;
int MPIR_CVAR_BCAST_DEVICE_COLLECTIVE;
int MPIR_CVAR_IBCAST_DEVICE_COLLECTIVE;
int MPIR_CVAR_BCAST_INIT_DEVICE_COLLECTIVE;
int MPIR_CVAR_GATHER_DEVICE_COLLECTIVE;
int MPIR_CVAR_IGATHER_DEVICE_COLLECTIVE;
int MPIR_CVAR_GATHER_INIT_DEVICE_COLLECTIVE;
int MPIR_CVAR_GATHERV_DEVICE_COLLECTIVE;
int MPIR_CVAR_IGATHERV_DEVICE_COLLECTIVE;
int MPIR_CVAR_GATHERV_INIT_DEVICE_COLLECTIVE;
int MPIR_CVAR_SCATTER_DEVICE_COLLECTIVE;
int MPIR_CVAR_ISCATTER_DEVICE_COLLECTIVE;
int MPIR_CVAR_SCATTER_INIT_DEVICE_COLLECTIVE;
int MPIR_CVAR_SCATTERV_DEVICE_COLLECTIVE;
int MPIR_CVAR_ISCATTERV_DEVICE_COLLECTIVE;
int MPIR_CVAR_SCATTERV_INIT_DEVICE_COLLECTIVE;
int MPIR_CVAR_ALLGATHER_DEVICE_COLLECTIVE;
int MPIR_CVAR_IALLGATHER_DEVICE_COLLECTIVE;
int MPIR_CVAR_ALLGATHER_INIT_DEVICE_COLLECTIVE;
int MPIR_CVAR_ALLGATHERV_DEVICE_COLLECTIVE;
int MPIR_CVAR_IALLGATHERV_DEVICE_COLLECTIVE;
int MPIR_CVAR_ALLGATHERV_INIT_DEVICE_COLLECTIVE;
int MPIR_CVAR_ALLTOALL_DEVICE_COLLECTIVE;
int MPIR_CVAR_IALLTOALL_DEVICE_COLLECTIVE;
int MPIR_CVAR_ALLTOALL_INIT_DEVICE_COLLECTIVE;
int MPIR_CVAR_ALLTOALLV_DEVICE_COLLECTIVE;
int MPIR_CVAR_IALLTOALLV_DEVICE_COLLECTIVE;
int MPIR_CVAR_ALLTOALLV_INIT_DEVICE_COLLECTIVE;
int MPIR_CVAR_ALLTOALLW_DEVICE_COLLECTIVE;
int MPIR_CVAR_IALLTOALLW_DEVICE_COLLECTIVE;
int MPIR_CVAR_ALLTOALLW_INIT_DEVICE_COLLECTIVE;
int MPIR_CVAR_REDUCE_DEVICE_COLLECTIVE;
int MPIR_CVAR_IREDUCE_DEVICE_COLLECTIVE;
int MPIR_CVAR_REDUCE_INIT_DEVICE_COLLECTIVE;
int MPIR_CVAR_ALLREDUCE_DEVICE_COLLECTIVE;
int MPIR_CVAR_IALLREDUCE_DEVICE_COLLECTIVE;
int MPIR_CVAR_ALLREDUCE_INIT_DEVICE_COLLECTIVE;
int MPIR_CVAR_REDUCE_SCATTER_DEVICE_COLLECTIVE;
int MPIR_CVAR_IREDUCE_SCATTER_DEVICE_COLLECTIVE;
int MPIR_CVAR_REDUCE_SCATTER_INIT_DEVICE_COLLECTIVE;
int MPIR_CVAR_REDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE;
int MPIR_CVAR_IREDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE;
int MPIR_CVAR_REDUCE_SCATTER_BLOCK_INIT_DEVICE_COLLECTIVE;
int MPIR_CVAR_SCAN_DEVICE_COLLECTIVE;
int MPIR_CVAR_ISCAN_DEVICE_COLLECTIVE;
int MPIR_CVAR_SCAN_INIT_DEVICE_COLLECTIVE;
int MPIR_CVAR_EXSCAN_DEVICE_COLLECTIVE;
int MPIR_CVAR_IEXSCAN_DEVICE_COLLECTIVE;
int MPIR_CVAR_EXSCAN_INIT_DEVICE_COLLECTIVE;
int MPIR_CVAR_NEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE;
int MPIR_CVAR_INEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE;
int MPIR_CVAR_NEIGHBOR_ALLGATHER_INIT_DEVICE_COLLECTIVE;
int MPIR_CVAR_NEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE;
int MPIR_CVAR_INEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE;
int MPIR_CVAR_NEIGHBOR_ALLGATHERV_INIT_DEVICE_COLLECTIVE;
int MPIR_CVAR_NEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE;
int MPIR_CVAR_INEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE;
int MPIR_CVAR_NEIGHBOR_ALLTOALL_INIT_DEVICE_COLLECTIVE;
int MPIR_CVAR_NEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE;
int MPIR_CVAR_INEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE;
int MPIR_CVAR_NEIGHBOR_ALLTOALLV_INIT_DEVICE_COLLECTIVE;
int MPIR_CVAR_NEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE;
int MPIR_CVAR_INEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE;
int MPIR_CVAR_NEIGHBOR_ALLTOALLW_INIT_DEVICE_COLLECTIVE;
int MPIR_CVAR_COLL_HYBRID_MEMORY;
int MPIR_CVAR_GATHER_VSMALL_MSG_SIZE;
int MPIR_CVAR_IALLTOALL_BRUCKS_KVAL;
int MPIR_CVAR_IALLTOALL_BRUCKS_BUFFER_PER_NBR;
int MPIR_CVAR_IALLTOALL_SCATTERED_OUTSTANDING_TASKS;
int MPIR_CVAR_IALLTOALL_SCATTERED_BATCH_SIZE;
int MPIR_CVAR_DEVICE_COLLECTIVES;
int MPIR_CVAR_COLLECTIVE_FALLBACK;
const char * MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE;
int MPIR_CVAR_HIERARCHY_DUMP;
const char * MPIR_CVAR_COORDINATES_FILE;
int MPIR_CVAR_COLL_TREE_DUMP;
int MPIR_CVAR_COORDINATES_DUMP;
int MPIR_CVAR_PROGRESS_MAX_COLLS;
int MPIR_CVAR_COMM_SPLIT_USE_QSORT;
int MPIR_CVAR_CTXID_EAGER_SIZE;
int MPIR_CVAR_DATALOOP_FAST_SEEK;
int MPIR_CVAR_YAKSA_COMPLEX_SUPPORT;
int MPIR_CVAR_GPU_DOUBLE_SUPPORT;
int MPIR_CVAR_GPU_LONG_DOUBLE_SUPPORT;
int MPIR_CVAR_ENABLE_YAKSA_REDUCTION;
int MPIR_CVAR_YAKSA_REDUCTION_THRESHOLD;
int MPIR_CVAR_PROCTABLE_SIZE;
int MPIR_CVAR_PROCTABLE_PRINT;
int MPIR_CVAR_PRINT_ERROR_STACK;
int MPIR_CVAR_CHOP_ERROR_STACK;
int MPIR_CVAR_ASYNC_PROGRESS;
const char * MPIR_CVAR_PROGRESS_THREAD_AFFINITY;
int MPIR_CVAR_SUPPRESS_ABORT_MESSAGE;
int MPIR_CVAR_COREDUMP_ON_ABORT;
int MPIR_CVAR_ERROR_CHECKING;
int MPIR_CVAR_MEMDUMP;
int MPIR_CVAR_DEBUG_SUMMARY;
const char * MPIR_CVAR_DEFAULT_THREAD_LEVEL;
int MPIR_CVAR_DEBUG_HOLD;
int MPIR_CVAR_GPU_USE_IMMEDIATE_COMMAND_LIST;
int MPIR_CVAR_GPU_ROUND_ROBIN_COMMAND_QUEUES;
int MPIR_CVAR_NO_COLLECTIVE_FINALIZE;
int MPIR_CVAR_FINALIZE_WAIT;
int MPIR_CVAR_INIT_SKIP_PMI_BARRIER;
int MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE;
int MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE_H2D;
int MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE_D2H;
int MPIR_CVAR_REQUEST_ERR_FATAL;
int MPIR_CVAR_REQUEST_POLL_FREQ;
int MPIR_CVAR_REQUEST_BATCH_SIZE;
int MPIR_CVAR_DEBUG_PROGRESS_TIMEOUT;
int MPIR_CVAR_DIMS_VERBOSE;
const char * MPIR_CVAR_QMPI_TOOL_LIST;
const char * MPIR_CVAR_NAMESERV_FILE_PUBDIR;
int MPIR_CVAR_ABORT_ON_LEAKED_HANDLES;
const char * MPIR_CVAR_NETLOC_NODE_FILE;
int MPIR_CVAR_NOLOCAL;
int MPIR_CVAR_ODD_EVEN_CLIQUES;
int MPIR_CVAR_NUM_CLIQUES;
int MPIR_CVAR_CLIQUES_BY_BLOCK;
int MPIR_CVAR_PMI_VERSION;
int MPIR_CVAR_COLL_ALIAS_CHECK;
int MPIR_CVAR_ENABLE_GPU;
int MPIR_CVAR_GPU_HAS_WAIT_KERNEL;
int MPIR_CVAR_ENABLE_GPU_REGISTER;
int MPIR_CVAR_POLLS_BEFORE_YIELD;
const char * MPIR_CVAR_CH3_INTERFACE_HOSTNAME;
MPIR_T_cvar_range_value_t MPIR_CVAR_CH3_PORT_RANGE;
const char * MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE;
int MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES;
int MPIR_CVAR_NEMESIS_ENABLE_CKPOINT;
int MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ;
int MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ;
int MPIR_CVAR_ENABLE_FT;
const char * MPIR_CVAR_NEMESIS_NETMOD;
int MPIR_CVAR_CH3_ENABLE_HCOLL;
int MPIR_CVAR_CH3_COMM_CONNECT_TIMEOUT;
int MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE;
int MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD;
int MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD;
int MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM;
int MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING;
int MPIR_CVAR_CH3_RMA_SLOTS_SIZE;
int MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES;
int MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE;
int MPIR_CVAR_CH3_PG_VERBOSE;
int MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE;
int MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE;
int MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE;
int MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE;
int MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE;
const char * MPIR_CVAR_OFI_USE_PROVIDER;
int MPIR_CVAR_SINGLE_HOST_ENABLED;
int MPIR_CVAR_CH4_OFI_AM_LONG_FORCE_PIPELINE;
int MPIR_CVAR_BCAST_OFI_INTRA_ALGORITHM;
int MPIR_CVAR_CH4_OFI_GPU_RECEIVE_ENGINE_TYPE;
int MPIR_CVAR_OFI_SKIP_IPV6;
int MPIR_CVAR_CH4_OFI_ENABLE_DATA;
int MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE;
int MPIR_CVAR_CH4_OFI_ENABLE_SHARED_AV;
int MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS;
int MPIR_CVAR_CH4_OFI_ENABLE_SHARED_CONTEXTS;
int MPIR_CVAR_CH4_OFI_ENABLE_MR_VIRT_ADDRESS;
int MPIR_CVAR_CH4_OFI_ENABLE_MR_ALLOCATED;
int MPIR_CVAR_CH4_OFI_ENABLE_MR_REGISTER_NULL;
int MPIR_CVAR_CH4_OFI_ENABLE_MR_PROV_KEY;
int MPIR_CVAR_CH4_OFI_ENABLE_TAGGED;
int MPIR_CVAR_CH4_OFI_ENABLE_AM;
int MPIR_CVAR_CH4_OFI_ENABLE_RMA;
int MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS;
int MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS;
int MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS;
int MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS;
int MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK;
int MPIR_CVAR_CH4_OFI_ENABLE_HMEM;
int MPIR_CVAR_CH4_OFI_ENABLE_MR_HMEM;
int MPIR_CVAR_CH4_OFI_GPU_RDMA_THRESHOLD;
int MPIR_CVAR_CH4_OFI_CONTEXT_ID_BITS;
int MPIR_CVAR_CH4_OFI_RANK_BITS;
int MPIR_CVAR_CH4_OFI_TAG_BITS;
int MPIR_CVAR_CH4_OFI_MAJOR_VERSION;
int MPIR_CVAR_CH4_OFI_MINOR_VERSION;
int MPIR_CVAR_CH4_OFI_MAX_RMA_SEP_CTX;
int MPIR_CVAR_CH4_OFI_MAX_EAGAIN_RETRY;
int MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS;
int MPIR_CVAR_CH4_OFI_NUM_OPTIMIZED_MEMORY_REGIONS;
int MPIR_CVAR_CH4_OFI_RMA_PROGRESS_INTERVAL;
int MPIR_CVAR_CH4_OFI_RMA_IOVEC_MAX;
int MPIR_CVAR_CH4_OFI_EAGER_MAX_MSG_SIZE;
int MPIR_CVAR_CH4_OFI_MAX_NICS;
int MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_STRIPING;
int MPIR_CVAR_CH4_OFI_MULTI_NIC_STRIPING_THRESHOLD;
int MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_HASHING;
int MPIR_CVAR_CH4_OFI_MULTIRECV_BUFFER_SIZE;
int MPIR_CVAR_OFI_USE_MIN_NICS;
int MPIR_CVAR_CH4_OFI_ENABLE_TRIGGERED;
int MPIR_CVAR_CH4_OFI_ENABLE_GPU_PIPELINE;
int MPIR_CVAR_CH4_OFI_GPU_PIPELINE_THRESHOLD;
int MPIR_CVAR_CH4_OFI_GPU_PIPELINE_BUFFER_SZ;
int MPIR_CVAR_CH4_OFI_GPU_PIPELINE_NUM_BUFFERS_PER_CHUNK;
int MPIR_CVAR_CH4_OFI_GPU_PIPELINE_MAX_NUM_BUFFERS;
int MPIR_CVAR_CH4_OFI_GPU_PIPELINE_D2H_ENGINE_TYPE;
int MPIR_CVAR_CH4_OFI_GPU_PIPELINE_H2D_ENGINE_TYPE;
int MPIR_CVAR_CH4_OFI_PREF_NIC;
int MPIR_CVAR_CH4_OFI_DISABLE_INJECT_WRITE;
int MPIR_CVAR_CH4_OFI_ENABLE_INJECT;
int MPIR_CVAR_CH4_OFI_GPU_SEND_ENGINE_TYPE;
int MPIR_CVAR_CH4_OFI_EAGER_THRESHOLD;
int MPIR_CVAR_UCX_DT_RECV;
int MPIR_CVAR_CH4_CMA_ENABLE;
int MPIR_CVAR_CH4_IPC_CMA_P2P_THRESHOLD;
int MPIR_CVAR_CH4_IPC_GPU_HANDLE_CACHE;
int MPIR_CVAR_CH4_IPC_GPU_MAX_CACHE_ENTRIES;
int MPIR_CVAR_CH4_IPC_GPU_CACHE_SIZE;
int MPIR_CVAR_CH4_IPC_GPU_P2P_THRESHOLD;
int MPIR_CVAR_CH4_IPC_ZE_SHAREABLE_HANDLE;
int MPIR_CVAR_CH4_IPC_GPU_ENGINE_TYPE;
int MPIR_CVAR_CH4_IPC_GPU_READ_WRITE_PROTOCOL;
int MPIR_CVAR_CH4_IPC_GPU_RMA_ENGINE_TYPE;
int MPIR_CVAR_CH4_IPC_MAP_REPEAT_ADDR;
int MPIR_CVAR_CH4_XPMEM_ENABLE;
int MPIR_CVAR_CH4_IPC_XPMEM_P2P_THRESHOLD;
int MPIR_CVAR_CH4_IPC_XPMEM_P2P_UPPER_THRESHOLD;
int MPIR_CVAR_BCAST_POSIX_INTRA_ALGORITHM;
int MPIR_CVAR_IBCAST_POSIX_INTRA_ALGORITHM;
int MPIR_CVAR_REDUCE_POSIX_INTRA_ALGORITHM;
int MPIR_CVAR_IREDUCE_POSIX_INTRA_ALGORITHM;
int MPIR_CVAR_ALLREDUCE_POSIX_INTRA_ALGORITHM;
int MPIR_CVAR_BARRIER_POSIX_INTRA_ALGORITHM;
int MPIR_CVAR_ALLTOALL_POSIX_INTRA_ALGORITHM;
int MPIR_CVAR_ALLGATHER_POSIX_INTRA_ALGORITHM;
int MPIR_CVAR_ALLGATHERV_POSIX_INTRA_ALGORITHM;
int MPIR_CVAR_POSIX_POLL_FREQUENCY;
int MPIR_CVAR_BCAST_IPC_READ_MSG_SIZE_THRESHOLD;
int MPIR_CVAR_ALLTOALL_IPC_READ_MSG_SIZE_THRESHOLD;
int MPIR_CVAR_ALLGATHER_IPC_READ_MSG_SIZE_THRESHOLD;
int MPIR_CVAR_ALLGATHERV_IPC_READ_MSG_SIZE_THRESHOLD;
int MPIR_CVAR_POSIX_NUM_COLLS_THRESHOLD;
const char * MPIR_CVAR_CH4_SHM_POSIX_EAGER;
const char * MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE;
const char * MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE_GPU;
int MPIR_CVAR_CH4_SHM_POSIX_TOPO_ENABLE;
int MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_NUM_CELLS;
int MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_CELL_SIZE;
int MPIR_CVAR_COLL_SHM_LIMIT_PER_NODE;
int MPIR_CVAR_BCAST_INTRANODE_BUFFER_TOTAL_SIZE;
int MPIR_CVAR_BCAST_INTRANODE_NUM_CELLS;
int MPIR_CVAR_REDUCE_INTRANODE_BUFFER_TOTAL_SIZE;
int MPIR_CVAR_REDUCE_INTRANODE_NUM_CELLS;
int MPIR_CVAR_BCAST_INTRANODE_TREE_KVAL;
const char * MPIR_CVAR_BCAST_INTRANODE_TREE_TYPE;
int MPIR_CVAR_REDUCE_INTRANODE_MSG_SIZE_THRESHOLD;
int MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL;
int MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL_LARGE;
const char * MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE;
const char * MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE_LARGE;
int MPIR_CVAR_ENABLE_INTRANODE_TOPOLOGY_AWARE_TREES;
int MPIR_CVAR_BARRIER_COMPOSITION;
int MPIR_CVAR_BCAST_COMPOSITION;
int MPIR_CVAR_ALLREDUCE_COMPOSITION;
int MPIR_CVAR_ALLGATHER_COMPOSITION;
int MPIR_CVAR_ALLTOALL_COMPOSITION;
int MPIR_CVAR_REDUCE_COMPOSITION;
int MPIR_CVAR_ALLTOALL_SHM_PER_RANK;
int MPIR_CVAR_ALLGATHER_SHM_PER_RANK;
int MPIR_CVAR_NUM_MULTI_LEADS;
int MPIR_CVAR_ALLREDUCE_SHM_PER_LEADER;
int MPIR_CVAR_ALLREDUCE_CACHE_PER_LEADER;
int MPIR_CVAR_ALLREDUCE_LOCAL_COPY_OFFSETS;
const char * MPIR_CVAR_CH4_NETMOD;
const char * MPIR_CVAR_CH4_SHM;
int MPIR_CVAR_CH4_ROOTS_ONLY_PMI;
int MPIR_CVAR_CH4_RUNTIME_CONF_DEBUG;
const char * MPIR_CVAR_CH4_MT_MODEL;
int MPIR_CVAR_CH4_NUM_VCIS;
int MPIR_CVAR_CH4_RESERVE_VCIS;
const char * MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE;
const char * MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE_GPU;
int MPIR_CVAR_CH4_IOV_DENSITY_MIN;
int MPIR_CVAR_CH4_PACK_BUFFER_SIZE;
int MPIR_CVAR_CH4_NUM_PACK_BUFFERS_PER_CHUNK;
int MPIR_CVAR_CH4_MAX_NUM_PACK_BUFFERS;
int MPIR_CVAR_CH4_GPU_COLL_SWAP_BUFFER_SZ;
int MPIR_CVAR_CH4_GPU_COLL_NUM_BUFFERS_PER_CHUNK;
int MPIR_CVAR_CH4_GPU_COLL_MAX_NUM_BUFFERS;
int MPIR_CVAR_CH4_GLOBAL_PROGRESS;
int MPIR_CVAR_CH4_COMM_CONNECT_TIMEOUT;
int MPIR_CVAR_CH4_ENABLE_STREAM_WORKQ;
int MPIR_CVAR_CH4_RMA_MEM_EFFICIENT;
int MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS;
int MPIR_CVAR_CH4_RMA_AM_PROGRESS_INTERVAL;
int MPIR_CVAR_CH4_RMA_AM_PROGRESS_LOW_FREQ_INTERVAL;
int MPIR_CVAR_GENQ_SHMEM_POOL_FREE_QUEUE_SENDER_SIDE;
int MPIR_CVAR_ENABLE_HCOLL;
int MPIR_CVAR_COLL_SCHED_DUMP;
int MPIR_CVAR_SHM_RANDOM_ADDR_RETRY;
int MPIR_CVAR_SHM_SYMHEAP_RETRY;
int MPIR_CVAR_ENABLE_HEAVY_YIELD;

int MPIR_T_cvar_init(void)
{
    int mpi_errno = MPI_SUCCESS;
    int rc, got_rc;
    const char *tmp_str;
    MPIR_T_cvar_value_t defaultval;

    int debug = 0;
    rc = MPL_env2bool("MPICH_DEBUG_CVARS", &debug);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_DEBUG_CVARS");
    rc = MPL_env2bool("MPIR_PARAM_DEBUG_CVARS", &debug);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_DEBUG_CVARS");
    rc = MPL_env2bool("MPIR_CVAR_DEBUG_CVARS", &debug);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_DEBUG_CVARS");

    /* declared in src/mpi/coll/src/coll_impl.c */
    MPIR_T_cat_add_desc("COLLECTIVE",
        "A category for collective communication variables.");

    /* declared in src/mpi/comm/comm_impl.c */
    MPIR_T_cat_add_desc("COMMUNICATOR",
        "cvars that control communicator construction and operation");

    /* declared in src/mpi/datatype/typerep/dataloop/segment.c */
    MPIR_T_cat_add_desc("DATALOOP",
        "Dataloop-related CVARs");

    /* declared in src/mpi/errhan/errutil.c */
    MPIR_T_cat_add_desc("ERROR_HANDLING",
        "cvars that control error handling behavior (stack traces, aborts, etc)");

    /* declared in src/mpi/init/mpi_init.h */
    MPIR_T_cat_add_desc("DEVELOPER",
        "useful for developers working on MPICH itself");

    /* declared in src/mpi/init/mpir_init.c */
    MPIR_T_cat_add_desc("THREADS",
        "multi-threading cvars");

    /* declared in src/mpi/init/mpir_init.c */
    MPIR_T_cat_add_desc("DEBUGGER",
        "cvars relevant to the \"MPIR\" debugger interface");

    /* declared in src/mpi/request/request_impl.c */
    MPIR_T_cat_add_desc("REQUEST",
        "A category for requests management variables");

    /* declared in src/mpi/topo/dims_create.c */
    MPIR_T_cat_add_desc("DIMS",
        "Dims_create cvars");

    /* declared in src/mpi_t/qmpi_register.c */
    MPIR_T_cat_add_desc("TOOLS",
        "cvars that control tools connected to MPICH");

    /* declared in src/nameserv/file/file_nameserv.c */
    MPIR_T_cat_add_desc("PROCESS_MANAGER",
        "cvars that control the client-side process manager code");

    /* declared in src/util/mpir_handlemem.c */
    MPIR_T_cat_add_desc("MEMORY",
        "affects memory allocation and usage, including MPI object handles");

    /* declared in src/util/mpir_nodemap.c */
    MPIR_T_cat_add_desc("NODEMAP",
        "cvars that control behavior of nodemap");

    /* declared in src/include/mpir_gpu.h */
    MPIR_T_cat_add_desc("GPU",
        "GPU related cvars");

    /* declared in src/mpid/ch3/channels/nemesis/src/mpid_nem_init.c */
    MPIR_T_cat_add_desc("NEMESIS",
        "cvars that control behavior of the ch3:nemesis channel");

    /* declared in src/mpid/ch3/channels/nemesis/src/mpid_nem_lmt.c */
    MPIR_T_cat_add_desc("FT",
        "cvars that control behavior of fault tolerance");

    /* declared in src/mpid/ch3/src/mpidi_rma.c */
    MPIR_T_cat_add_desc("CH3",
        "cvars that control behavior of ch3");

    /* declared in src/mpid/ch4/netmod/ofi/ofi_init.c */
    MPIR_T_cat_add_desc("CH4_OFI",
        "A category for CH4 OFI netmod variables");

    /* declared in src/mpid/ch4/netmod/ucx/ucx_init.c */
    MPIR_T_cat_add_desc("CH4_UCX",
        "A category for CH4 OFI netmod variables");

    /* declared in src/mpid/ch4/src/ch4_init.c */
    MPIR_T_cat_add_desc("CH4",
        "cvars that control behavior of the CH4 device");

    defaultval.d = MPIR_CVAR_BARRIER_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BARRIER_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_BARRIER_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select barrier algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"nb            - Force nonblocking algorithm\n"
"smp           - Force smp algorithm\n"
"k_dissemination - Force high radix dissemination algorithm\n"
"recexch       - Force recursive exchange algorithm");
    MPIR_CVAR_BARRIER_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_BARRIER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BARRIER_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_BARRIER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BARRIER_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_BARRIER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BARRIER_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BARRIER_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_BARRIER_INTRA_ALGORITHM = MPIR_CVAR_BARRIER_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_BARRIER_INTRA_ALGORITHM = MPIR_CVAR_BARRIER_INTRA_ALGORITHM_nb;
        else if (0 == strcmp(tmp_str, "smp"))
            MPIR_CVAR_BARRIER_INTRA_ALGORITHM = MPIR_CVAR_BARRIER_INTRA_ALGORITHM_smp;
        else if (0 == strcmp(tmp_str, "k_dissemination"))
            MPIR_CVAR_BARRIER_INTRA_ALGORITHM = MPIR_CVAR_BARRIER_INTRA_ALGORITHM_k_dissemination;
        else if (0 == strcmp(tmp_str, "recexch"))
            MPIR_CVAR_BARRIER_INTRA_ALGORITHM = MPIR_CVAR_BARRIER_INTRA_ALGORITHM_recexch;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_BARRIER_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_BARRIER_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BARRIER_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_BARRIER_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select barrier algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"bcast - Force bcast algorithm\n"
"nb    - Force nonblocking algorithm");
    MPIR_CVAR_BARRIER_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_BARRIER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BARRIER_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_BARRIER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BARRIER_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_BARRIER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BARRIER_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BARRIER_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_BARRIER_INTER_ALGORITHM = MPIR_CVAR_BARRIER_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "bcast"))
            MPIR_CVAR_BARRIER_INTER_ALGORITHM = MPIR_CVAR_BARRIER_INTER_ALGORITHM_bcast;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_BARRIER_INTER_ALGORITHM = MPIR_CVAR_BARRIER_INTER_ALGORITHM_nb;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_BARRIER_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BARRIER_DISSEM_KVAL, /* name */
        &MPIR_CVAR_BARRIER_DISSEM_KVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "k value for dissemination exchange based barrier algorithm");
    MPIR_CVAR_BARRIER_DISSEM_KVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_BARRIER_DISSEM_KVAL", &(MPIR_CVAR_BARRIER_DISSEM_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BARRIER_DISSEM_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_BARRIER_DISSEM_KVAL", &(MPIR_CVAR_BARRIER_DISSEM_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BARRIER_DISSEM_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_BARRIER_DISSEM_KVAL", &(MPIR_CVAR_BARRIER_DISSEM_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BARRIER_DISSEM_KVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BARRIER_DISSEM_KVAL = %d\n", MPIR_CVAR_BARRIER_DISSEM_KVAL);
    }

    defaultval.d = 2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BARRIER_RECEXCH_KVAL, /* name */
        &MPIR_CVAR_BARRIER_RECEXCH_KVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "k value for recursive exchange based allreduce based barrier");
    MPIR_CVAR_BARRIER_RECEXCH_KVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_BARRIER_RECEXCH_KVAL", &(MPIR_CVAR_BARRIER_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BARRIER_RECEXCH_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_BARRIER_RECEXCH_KVAL", &(MPIR_CVAR_BARRIER_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BARRIER_RECEXCH_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_BARRIER_RECEXCH_KVAL", &(MPIR_CVAR_BARRIER_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BARRIER_RECEXCH_KVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BARRIER_RECEXCH_KVAL = %d\n", MPIR_CVAR_BARRIER_RECEXCH_KVAL);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BARRIER_RECEXCH_SINGLE_PHASE_RECV, /* name */
        &MPIR_CVAR_BARRIER_RECEXCH_SINGLE_PHASE_RECV, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR controls whether the recv is posted for one phase or two phases in recexch algos. By default, we post the recvs for 2 phases.");
    MPIR_CVAR_BARRIER_RECEXCH_SINGLE_PHASE_RECV = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_BARRIER_RECEXCH_SINGLE_PHASE_RECV", &(MPIR_CVAR_BARRIER_RECEXCH_SINGLE_PHASE_RECV));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BARRIER_RECEXCH_SINGLE_PHASE_RECV");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_BARRIER_RECEXCH_SINGLE_PHASE_RECV", &(MPIR_CVAR_BARRIER_RECEXCH_SINGLE_PHASE_RECV));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BARRIER_RECEXCH_SINGLE_PHASE_RECV");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_BARRIER_RECEXCH_SINGLE_PHASE_RECV", &(MPIR_CVAR_BARRIER_RECEXCH_SINGLE_PHASE_RECV));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BARRIER_RECEXCH_SINGLE_PHASE_RECV");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BARRIER_RECEXCH_SINGLE_PHASE_RECV = %d\n", MPIR_CVAR_BARRIER_RECEXCH_SINGLE_PHASE_RECV);
    }

    defaultval.d = 2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IBARRIER_RECEXCH_KVAL, /* name */
        &MPIR_CVAR_IBARRIER_RECEXCH_KVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "k value for recursive exchange based ibarrier");
    MPIR_CVAR_IBARRIER_RECEXCH_KVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IBARRIER_RECEXCH_KVAL", &(MPIR_CVAR_IBARRIER_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IBARRIER_RECEXCH_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IBARRIER_RECEXCH_KVAL", &(MPIR_CVAR_IBARRIER_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IBARRIER_RECEXCH_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IBARRIER_RECEXCH_KVAL", &(MPIR_CVAR_IBARRIER_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IBARRIER_RECEXCH_KVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IBARRIER_RECEXCH_KVAL = %d\n", MPIR_CVAR_IBARRIER_RECEXCH_KVAL);
    }

    defaultval.d = 2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IBARRIER_DISSEM_KVAL, /* name */
        &MPIR_CVAR_IBARRIER_DISSEM_KVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "k value for dissemination exchange based ibarrier");
    MPIR_CVAR_IBARRIER_DISSEM_KVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IBARRIER_DISSEM_KVAL", &(MPIR_CVAR_IBARRIER_DISSEM_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IBARRIER_DISSEM_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IBARRIER_DISSEM_KVAL", &(MPIR_CVAR_IBARRIER_DISSEM_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IBARRIER_DISSEM_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IBARRIER_DISSEM_KVAL", &(MPIR_CVAR_IBARRIER_DISSEM_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IBARRIER_DISSEM_KVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IBARRIER_DISSEM_KVAL = %d\n", MPIR_CVAR_IBARRIER_DISSEM_KVAL);
    }

    defaultval.d = MPIR_CVAR_IBARRIER_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IBARRIER_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_IBARRIER_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select ibarrier algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_recursive_doubling - Force recursive doubling algorithm\n"
"tsp_recexch - Force generic transport based recursive exchange algorithm\n"
"tsp_k_dissemination - Force generic transport based high-radix dissemination algorithm");
    MPIR_CVAR_IBARRIER_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IBARRIER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IBARRIER_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IBARRIER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IBARRIER_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IBARRIER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IBARRIER_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IBARRIER_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IBARRIER_INTRA_ALGORITHM = MPIR_CVAR_IBARRIER_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_IBARRIER_INTRA_ALGORITHM = MPIR_CVAR_IBARRIER_INTRA_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_recursive_doubling"))
            MPIR_CVAR_IBARRIER_INTRA_ALGORITHM = MPIR_CVAR_IBARRIER_INTRA_ALGORITHM_sched_recursive_doubling;
        else if (0 == strcmp(tmp_str, "tsp_recexch"))
            MPIR_CVAR_IBARRIER_INTRA_ALGORITHM = MPIR_CVAR_IBARRIER_INTRA_ALGORITHM_tsp_recexch;
        else if (0 == strcmp(tmp_str, "tsp_k_dissemination"))
            MPIR_CVAR_IBARRIER_INTRA_ALGORITHM = MPIR_CVAR_IBARRIER_INTRA_ALGORITHM_tsp_k_dissemination;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IBARRIER_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_IBARRIER_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IBARRIER_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_IBARRIER_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select ibarrier algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_bcast - Force bcast algorithm");
    MPIR_CVAR_IBARRIER_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IBARRIER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IBARRIER_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IBARRIER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IBARRIER_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IBARRIER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IBARRIER_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IBARRIER_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IBARRIER_INTER_ALGORITHM = MPIR_CVAR_IBARRIER_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_IBARRIER_INTER_ALGORITHM = MPIR_CVAR_IBARRIER_INTER_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_bcast"))
            MPIR_CVAR_IBARRIER_INTER_ALGORITHM = MPIR_CVAR_IBARRIER_INTER_ALGORITHM_sched_bcast;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IBARRIER_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 8;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BCAST_MIN_PROCS, /* name */
        &MPIR_CVAR_BCAST_MIN_PROCS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_SHORT_MSG_SIZE, MPIR_CVAR_BCAST_LONG_MSG_SIZE)");
    MPIR_CVAR_BCAST_MIN_PROCS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_BCAST_MIN_PROCS", &(MPIR_CVAR_BCAST_MIN_PROCS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BCAST_MIN_PROCS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_BCAST_MIN_PROCS", &(MPIR_CVAR_BCAST_MIN_PROCS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BCAST_MIN_PROCS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_BCAST_MIN_PROCS", &(MPIR_CVAR_BCAST_MIN_PROCS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BCAST_MIN_PROCS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BCAST_MIN_PROCS = %d\n", MPIR_CVAR_BCAST_MIN_PROCS);
    }

    defaultval.d = 12288;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BCAST_SHORT_MSG_SIZE, /* name */
        &MPIR_CVAR_BCAST_SHORT_MSG_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_LONG_MSG_SIZE)");
    MPIR_CVAR_BCAST_SHORT_MSG_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_BCAST_SHORT_MSG_SIZE", &(MPIR_CVAR_BCAST_SHORT_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BCAST_SHORT_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_BCAST_SHORT_MSG_SIZE", &(MPIR_CVAR_BCAST_SHORT_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BCAST_SHORT_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_BCAST_SHORT_MSG_SIZE", &(MPIR_CVAR_BCAST_SHORT_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BCAST_SHORT_MSG_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BCAST_SHORT_MSG_SIZE = %d\n", MPIR_CVAR_BCAST_SHORT_MSG_SIZE);
    }

    defaultval.d = 524288;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BCAST_LONG_MSG_SIZE, /* name */
        &MPIR_CVAR_BCAST_LONG_MSG_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_SHORT_MSG_SIZE)");
    MPIR_CVAR_BCAST_LONG_MSG_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_BCAST_LONG_MSG_SIZE", &(MPIR_CVAR_BCAST_LONG_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BCAST_LONG_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_BCAST_LONG_MSG_SIZE", &(MPIR_CVAR_BCAST_LONG_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BCAST_LONG_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_BCAST_LONG_MSG_SIZE", &(MPIR_CVAR_BCAST_LONG_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BCAST_LONG_MSG_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BCAST_LONG_MSG_SIZE = %d\n", MPIR_CVAR_BCAST_LONG_MSG_SIZE);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE, /* name */
        &MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Maximum message size for which SMP-aware broadcast is used.  A value of '0' uses SMP-aware broadcast for all message sizes.");
    MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_MAX_SMP_BCAST_MSG_SIZE", &(MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_MAX_SMP_BCAST_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_MAX_SMP_BCAST_MSG_SIZE", &(MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_MAX_SMP_BCAST_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE", &(MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE = %d\n", MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE);
    }

    defaultval.d = MPIR_CVAR_BCAST_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BCAST_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_BCAST_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select bcast algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"binomial                                - Force Binomial Tree\n"
"nb                                      - Force nonblocking algorithm\n"
"smp                                     - Force smp algorithm\n"
"scatter_recursive_doubling_allgather    - Force Scatter Recursive-Doubling Allgather\n"
"scatter_ring_allgather                  - Force Scatter Ring\n"
"pipelined_tree                          - Force tree-based pipelined algorithm\n"
"tree                                    - Force tree-based algorithm");
    MPIR_CVAR_BCAST_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_BCAST_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BCAST_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_BCAST_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BCAST_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_BCAST_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BCAST_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BCAST_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_BCAST_INTRA_ALGORITHM = MPIR_CVAR_BCAST_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "binomial"))
            MPIR_CVAR_BCAST_INTRA_ALGORITHM = MPIR_CVAR_BCAST_INTRA_ALGORITHM_binomial;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_BCAST_INTRA_ALGORITHM = MPIR_CVAR_BCAST_INTRA_ALGORITHM_nb;
        else if (0 == strcmp(tmp_str, "smp"))
            MPIR_CVAR_BCAST_INTRA_ALGORITHM = MPIR_CVAR_BCAST_INTRA_ALGORITHM_smp;
        else if (0 == strcmp(tmp_str, "scatter_recursive_doubling_allgather"))
            MPIR_CVAR_BCAST_INTRA_ALGORITHM = MPIR_CVAR_BCAST_INTRA_ALGORITHM_scatter_recursive_doubling_allgather;
        else if (0 == strcmp(tmp_str, "scatter_ring_allgather"))
            MPIR_CVAR_BCAST_INTRA_ALGORITHM = MPIR_CVAR_BCAST_INTRA_ALGORITHM_scatter_ring_allgather;
        else if (0 == strcmp(tmp_str, "pipelined_tree"))
            MPIR_CVAR_BCAST_INTRA_ALGORITHM = MPIR_CVAR_BCAST_INTRA_ALGORITHM_pipelined_tree;
        else if (0 == strcmp(tmp_str, "tree"))
            MPIR_CVAR_BCAST_INTRA_ALGORITHM = MPIR_CVAR_BCAST_INTRA_ALGORITHM_tree;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_BCAST_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BCAST_TREE_KVAL, /* name */
        &MPIR_CVAR_BCAST_TREE_KVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "k value for tree (kary, knomial, etc.) based bcast");
    MPIR_CVAR_BCAST_TREE_KVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_BCAST_TREE_KVAL", &(MPIR_CVAR_BCAST_TREE_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BCAST_TREE_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_BCAST_TREE_KVAL", &(MPIR_CVAR_BCAST_TREE_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BCAST_TREE_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_BCAST_TREE_KVAL", &(MPIR_CVAR_BCAST_TREE_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BCAST_TREE_KVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BCAST_TREE_KVAL = %d\n", MPIR_CVAR_BCAST_TREE_KVAL);
    }

    defaultval.str = (const char *) "kary";
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_CHAR,
        MPIR_CVAR_BCAST_TREE_TYPE, /* name */
        &MPIR_CVAR_BCAST_TREE_TYPE, /* address */
        MPIR_CVAR_MAX_STRLEN, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Tree type for tree based bcast kary      - kary tree type knomial_1 - knomial_1 tree type knomial_2 - knomial_2 tree type topology_aware - topology_aware tree type topology_aware_k - topology_aware tree type with branching factor k topology_wave - topology_wave tree type");
    tmp_str = defaultval.str;
    got_rc = 0;
    rc = MPL_env2str("MPICH_BCAST_TREE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BCAST_TREE_TYPE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_BCAST_TREE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BCAST_TREE_TYPE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_BCAST_TREE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BCAST_TREE_TYPE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BCAST_TREE_TYPE = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        MPIR_CVAR_BCAST_TREE_TYPE = MPL_strdup(tmp_str);
        MPIR_CVAR_assert(MPIR_CVAR_BCAST_TREE_TYPE);
        if (MPIR_CVAR_BCAST_TREE_TYPE == NULL) {
            MPIR_CHKMEM_SETERR(mpi_errno, strlen(tmp_str), "dup of string for MPIR_CVAR_BCAST_TREE_TYPE");
            goto fn_fail;
        }
    }
    else {
        MPIR_CVAR_BCAST_TREE_TYPE = NULL;
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BCAST_TOPO_REORDER_ENABLE, /* name */
        &MPIR_CVAR_BCAST_TOPO_REORDER_ENABLE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This cvar controls if the leaders are reordered based on the number of ranks in each group.");
    MPIR_CVAR_BCAST_TOPO_REORDER_ENABLE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_BCAST_TOPO_REORDER_ENABLE", &(MPIR_CVAR_BCAST_TOPO_REORDER_ENABLE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BCAST_TOPO_REORDER_ENABLE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_BCAST_TOPO_REORDER_ENABLE", &(MPIR_CVAR_BCAST_TOPO_REORDER_ENABLE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BCAST_TOPO_REORDER_ENABLE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_BCAST_TOPO_REORDER_ENABLE", &(MPIR_CVAR_BCAST_TOPO_REORDER_ENABLE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BCAST_TOPO_REORDER_ENABLE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BCAST_TOPO_REORDER_ENABLE = %d\n", MPIR_CVAR_BCAST_TOPO_REORDER_ENABLE);
    }

    defaultval.d = 200;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BCAST_TOPO_OVERHEAD, /* name */
        &MPIR_CVAR_BCAST_TOPO_OVERHEAD, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This cvar controls the size of the overhead.");
    MPIR_CVAR_BCAST_TOPO_OVERHEAD = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_BCAST_TOPO_OVERHEAD", &(MPIR_CVAR_BCAST_TOPO_OVERHEAD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BCAST_TOPO_OVERHEAD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_BCAST_TOPO_OVERHEAD", &(MPIR_CVAR_BCAST_TOPO_OVERHEAD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BCAST_TOPO_OVERHEAD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_BCAST_TOPO_OVERHEAD", &(MPIR_CVAR_BCAST_TOPO_OVERHEAD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BCAST_TOPO_OVERHEAD");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BCAST_TOPO_OVERHEAD = %d\n", MPIR_CVAR_BCAST_TOPO_OVERHEAD);
    }

    defaultval.d = 2800;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BCAST_TOPO_DIFF_GROUPS, /* name */
        &MPIR_CVAR_BCAST_TOPO_DIFF_GROUPS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This cvar controls the latency between different groups.");
    MPIR_CVAR_BCAST_TOPO_DIFF_GROUPS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_BCAST_TOPO_DIFF_GROUPS", &(MPIR_CVAR_BCAST_TOPO_DIFF_GROUPS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BCAST_TOPO_DIFF_GROUPS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_BCAST_TOPO_DIFF_GROUPS", &(MPIR_CVAR_BCAST_TOPO_DIFF_GROUPS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BCAST_TOPO_DIFF_GROUPS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_BCAST_TOPO_DIFF_GROUPS", &(MPIR_CVAR_BCAST_TOPO_DIFF_GROUPS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BCAST_TOPO_DIFF_GROUPS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BCAST_TOPO_DIFF_GROUPS = %d\n", MPIR_CVAR_BCAST_TOPO_DIFF_GROUPS);
    }

    defaultval.d = 1900;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BCAST_TOPO_DIFF_SWITCHES, /* name */
        &MPIR_CVAR_BCAST_TOPO_DIFF_SWITCHES, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This cvar controls the latency between different switches in the same groups.");
    MPIR_CVAR_BCAST_TOPO_DIFF_SWITCHES = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_BCAST_TOPO_DIFF_SWITCHES", &(MPIR_CVAR_BCAST_TOPO_DIFF_SWITCHES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BCAST_TOPO_DIFF_SWITCHES");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_BCAST_TOPO_DIFF_SWITCHES", &(MPIR_CVAR_BCAST_TOPO_DIFF_SWITCHES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BCAST_TOPO_DIFF_SWITCHES");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_BCAST_TOPO_DIFF_SWITCHES", &(MPIR_CVAR_BCAST_TOPO_DIFF_SWITCHES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BCAST_TOPO_DIFF_SWITCHES");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BCAST_TOPO_DIFF_SWITCHES = %d\n", MPIR_CVAR_BCAST_TOPO_DIFF_SWITCHES);
    }

    defaultval.d = 1600;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BCAST_TOPO_SAME_SWITCHES, /* name */
        &MPIR_CVAR_BCAST_TOPO_SAME_SWITCHES, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This cvar controls the latency in the same switch.");
    MPIR_CVAR_BCAST_TOPO_SAME_SWITCHES = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_BCAST_TOPO_SAME_SWITCHES", &(MPIR_CVAR_BCAST_TOPO_SAME_SWITCHES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BCAST_TOPO_SAME_SWITCHES");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_BCAST_TOPO_SAME_SWITCHES", &(MPIR_CVAR_BCAST_TOPO_SAME_SWITCHES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BCAST_TOPO_SAME_SWITCHES");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_BCAST_TOPO_SAME_SWITCHES", &(MPIR_CVAR_BCAST_TOPO_SAME_SWITCHES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BCAST_TOPO_SAME_SWITCHES");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BCAST_TOPO_SAME_SWITCHES = %d\n", MPIR_CVAR_BCAST_TOPO_SAME_SWITCHES);
    }

#if defined MPID_BCAST_IS_NON_BLOCKING
    defaultval.d = MPID_BCAST_IS_NON_BLOCKING;
#else
    defaultval.d = 1;
#endif /* MPID_BCAST_IS_NON_BLOCKING */

    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BCAST_IS_NON_BLOCKING, /* name */
        &MPIR_CVAR_BCAST_IS_NON_BLOCKING, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "If set to true, MPI_Bcast will use non-blocking send.");
    MPIR_CVAR_BCAST_IS_NON_BLOCKING = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_BCAST_IS_NON_BLOCKING", &(MPIR_CVAR_BCAST_IS_NON_BLOCKING));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BCAST_IS_NON_BLOCKING");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_BCAST_IS_NON_BLOCKING", &(MPIR_CVAR_BCAST_IS_NON_BLOCKING));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BCAST_IS_NON_BLOCKING");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_BCAST_IS_NON_BLOCKING", &(MPIR_CVAR_BCAST_IS_NON_BLOCKING));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BCAST_IS_NON_BLOCKING");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BCAST_IS_NON_BLOCKING = %d\n", MPIR_CVAR_BCAST_IS_NON_BLOCKING);
    }

#if defined MPID_BCAST_TREE_PIPELINE_CHUNK_SIZE
    defaultval.d = MPID_BCAST_TREE_PIPELINE_CHUNK_SIZE;
#else
    defaultval.d = 8192;
#endif /* MPID_BCAST_TREE_PIPELINE_CHUNK_SIZE */

    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BCAST_TREE_PIPELINE_CHUNK_SIZE, /* name */
        &MPIR_CVAR_BCAST_TREE_PIPELINE_CHUNK_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Indicates the chunk size for pipelined bcast.");
    MPIR_CVAR_BCAST_TREE_PIPELINE_CHUNK_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_BCAST_TREE_PIPELINE_CHUNK_SIZE", &(MPIR_CVAR_BCAST_TREE_PIPELINE_CHUNK_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BCAST_TREE_PIPELINE_CHUNK_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_BCAST_TREE_PIPELINE_CHUNK_SIZE", &(MPIR_CVAR_BCAST_TREE_PIPELINE_CHUNK_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BCAST_TREE_PIPELINE_CHUNK_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_BCAST_TREE_PIPELINE_CHUNK_SIZE", &(MPIR_CVAR_BCAST_TREE_PIPELINE_CHUNK_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BCAST_TREE_PIPELINE_CHUNK_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BCAST_TREE_PIPELINE_CHUNK_SIZE = %d\n", MPIR_CVAR_BCAST_TREE_PIPELINE_CHUNK_SIZE);
    }

#if defined MPID_BCAST_RECV_PRE_POST
    defaultval.d = MPID_BCAST_RECV_PRE_POST;
#else
    defaultval.d = 0;
#endif /* MPID_BCAST_RECV_PRE_POST */

    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BCAST_RECV_PRE_POST, /* name */
        &MPIR_CVAR_BCAST_RECV_PRE_POST, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "If set to true, MPI_Bcast will pre-post all the receives.");
    MPIR_CVAR_BCAST_RECV_PRE_POST = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_BCAST_RECV_PRE_POST", &(MPIR_CVAR_BCAST_RECV_PRE_POST));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BCAST_RECV_PRE_POST");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_BCAST_RECV_PRE_POST", &(MPIR_CVAR_BCAST_RECV_PRE_POST));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BCAST_RECV_PRE_POST");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_BCAST_RECV_PRE_POST", &(MPIR_CVAR_BCAST_RECV_PRE_POST));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BCAST_RECV_PRE_POST");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BCAST_RECV_PRE_POST = %d\n", MPIR_CVAR_BCAST_RECV_PRE_POST);
    }

    defaultval.d = MPIR_CVAR_BCAST_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BCAST_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_BCAST_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select bcast algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"nb                      - Force nonblocking algorithm\n"
"remote_send_local_bcast - Force remote-send-local-bcast algorithm");
    MPIR_CVAR_BCAST_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_BCAST_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BCAST_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_BCAST_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BCAST_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_BCAST_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BCAST_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BCAST_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_BCAST_INTER_ALGORITHM = MPIR_CVAR_BCAST_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_BCAST_INTER_ALGORITHM = MPIR_CVAR_BCAST_INTER_ALGORITHM_nb;
        else if (0 == strcmp(tmp_str, "remote_send_local_bcast"))
            MPIR_CVAR_BCAST_INTER_ALGORITHM = MPIR_CVAR_BCAST_INTER_ALGORITHM_remote_send_local_bcast;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_BCAST_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IBCAST_TREE_KVAL, /* name */
        &MPIR_CVAR_IBCAST_TREE_KVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "k value for tree (kary, knomial, etc.) based ibcast");
    MPIR_CVAR_IBCAST_TREE_KVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IBCAST_TREE_KVAL", &(MPIR_CVAR_IBCAST_TREE_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IBCAST_TREE_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IBCAST_TREE_KVAL", &(MPIR_CVAR_IBCAST_TREE_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IBCAST_TREE_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IBCAST_TREE_KVAL", &(MPIR_CVAR_IBCAST_TREE_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IBCAST_TREE_KVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IBCAST_TREE_KVAL = %d\n", MPIR_CVAR_IBCAST_TREE_KVAL);
    }

    defaultval.str = (const char *) "kary";
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_CHAR,
        MPIR_CVAR_IBCAST_TREE_TYPE, /* name */
        &MPIR_CVAR_IBCAST_TREE_TYPE, /* address */
        MPIR_CVAR_MAX_STRLEN, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Tree type for tree based ibcast kary      - kary tree type knomial_1 - knomial_1 tree type knomial_2 - knomial_2 tree type");
    tmp_str = defaultval.str;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IBCAST_TREE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IBCAST_TREE_TYPE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IBCAST_TREE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IBCAST_TREE_TYPE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IBCAST_TREE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IBCAST_TREE_TYPE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IBCAST_TREE_TYPE = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        MPIR_CVAR_IBCAST_TREE_TYPE = MPL_strdup(tmp_str);
        MPIR_CVAR_assert(MPIR_CVAR_IBCAST_TREE_TYPE);
        if (MPIR_CVAR_IBCAST_TREE_TYPE == NULL) {
            MPIR_CHKMEM_SETERR(mpi_errno, strlen(tmp_str), "dup of string for MPIR_CVAR_IBCAST_TREE_TYPE");
            goto fn_fail;
        }
    }
    else {
        MPIR_CVAR_IBCAST_TREE_TYPE = NULL;
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IBCAST_TREE_PIPELINE_CHUNK_SIZE, /* name */
        &MPIR_CVAR_IBCAST_TREE_PIPELINE_CHUNK_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Maximum chunk size (in bytes) for pipelining in tree based ibcast. Default value is 0, that is, no pipelining by default");
    MPIR_CVAR_IBCAST_TREE_PIPELINE_CHUNK_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IBCAST_TREE_PIPELINE_CHUNK_SIZE", &(MPIR_CVAR_IBCAST_TREE_PIPELINE_CHUNK_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IBCAST_TREE_PIPELINE_CHUNK_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IBCAST_TREE_PIPELINE_CHUNK_SIZE", &(MPIR_CVAR_IBCAST_TREE_PIPELINE_CHUNK_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IBCAST_TREE_PIPELINE_CHUNK_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IBCAST_TREE_PIPELINE_CHUNK_SIZE", &(MPIR_CVAR_IBCAST_TREE_PIPELINE_CHUNK_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IBCAST_TREE_PIPELINE_CHUNK_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IBCAST_TREE_PIPELINE_CHUNK_SIZE = %d\n", MPIR_CVAR_IBCAST_TREE_PIPELINE_CHUNK_SIZE);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IBCAST_RING_CHUNK_SIZE, /* name */
        &MPIR_CVAR_IBCAST_RING_CHUNK_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Maximum chunk size (in bytes) for pipelining in ibcast ring algorithm. Default value is 0, that is, no pipelining by default");
    MPIR_CVAR_IBCAST_RING_CHUNK_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IBCAST_RING_CHUNK_SIZE", &(MPIR_CVAR_IBCAST_RING_CHUNK_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IBCAST_RING_CHUNK_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IBCAST_RING_CHUNK_SIZE", &(MPIR_CVAR_IBCAST_RING_CHUNK_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IBCAST_RING_CHUNK_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IBCAST_RING_CHUNK_SIZE", &(MPIR_CVAR_IBCAST_RING_CHUNK_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IBCAST_RING_CHUNK_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IBCAST_RING_CHUNK_SIZE = %d\n", MPIR_CVAR_IBCAST_RING_CHUNK_SIZE);
    }

    defaultval.d = MPIR_CVAR_IBCAST_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IBCAST_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_IBCAST_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select ibcast algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_binomial                             - Force Binomial algorithm\n"
"sched_smp                                  - Force smp algorithm\n"
"sched_scatter_recursive_doubling_allgather - Force Scatter Recursive Doubling Allgather algorithm\n"
"sched_scatter_ring_allgather               - Force Scatter Ring Allgather algorithm\n"
"tsp_tree                               - Force Generic Transport Tree algorithm\n"
"tsp_scatterv_recexch_allgatherv        - Force Generic Transport Scatterv followed by Recursive Exchange Allgatherv algorithm\n"
"tsp_scatterv_ring_allgatherv           - Force Generic Transport Scatterv followed by Ring Allgatherv algorithm\n"
"tsp_ring                               - Force Generic Transport Ring algorithm");
    MPIR_CVAR_IBCAST_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IBCAST_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IBCAST_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IBCAST_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IBCAST_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IBCAST_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IBCAST_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IBCAST_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IBCAST_INTRA_ALGORITHM = MPIR_CVAR_IBCAST_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_IBCAST_INTRA_ALGORITHM = MPIR_CVAR_IBCAST_INTRA_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_binomial"))
            MPIR_CVAR_IBCAST_INTRA_ALGORITHM = MPIR_CVAR_IBCAST_INTRA_ALGORITHM_sched_binomial;
        else if (0 == strcmp(tmp_str, "sched_smp"))
            MPIR_CVAR_IBCAST_INTRA_ALGORITHM = MPIR_CVAR_IBCAST_INTRA_ALGORITHM_sched_smp;
        else if (0 == strcmp(tmp_str, "sched_scatter_recursive_doubling_allgather"))
            MPIR_CVAR_IBCAST_INTRA_ALGORITHM = MPIR_CVAR_IBCAST_INTRA_ALGORITHM_sched_scatter_recursive_doubling_allgather;
        else if (0 == strcmp(tmp_str, "sched_scatter_ring_allgather"))
            MPIR_CVAR_IBCAST_INTRA_ALGORITHM = MPIR_CVAR_IBCAST_INTRA_ALGORITHM_sched_scatter_ring_allgather;
        else if (0 == strcmp(tmp_str, "tsp_tree"))
            MPIR_CVAR_IBCAST_INTRA_ALGORITHM = MPIR_CVAR_IBCAST_INTRA_ALGORITHM_tsp_tree;
        else if (0 == strcmp(tmp_str, "tsp_scatterv_recexch_allgatherv"))
            MPIR_CVAR_IBCAST_INTRA_ALGORITHM = MPIR_CVAR_IBCAST_INTRA_ALGORITHM_tsp_scatterv_recexch_allgatherv;
        else if (0 == strcmp(tmp_str, "tsp_scatterv_ring_allgatherv"))
            MPIR_CVAR_IBCAST_INTRA_ALGORITHM = MPIR_CVAR_IBCAST_INTRA_ALGORITHM_tsp_scatterv_ring_allgatherv;
        else if (0 == strcmp(tmp_str, "tsp_ring"))
            MPIR_CVAR_IBCAST_INTRA_ALGORITHM = MPIR_CVAR_IBCAST_INTRA_ALGORITHM_tsp_ring;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IBCAST_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IBCAST_SCATTERV_KVAL, /* name */
        &MPIR_CVAR_IBCAST_SCATTERV_KVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "k value for tree based scatter in scatter_recexch_allgather algorithm");
    MPIR_CVAR_IBCAST_SCATTERV_KVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IBCAST_SCATTERV_KVAL", &(MPIR_CVAR_IBCAST_SCATTERV_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IBCAST_SCATTERV_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IBCAST_SCATTERV_KVAL", &(MPIR_CVAR_IBCAST_SCATTERV_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IBCAST_SCATTERV_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IBCAST_SCATTERV_KVAL", &(MPIR_CVAR_IBCAST_SCATTERV_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IBCAST_SCATTERV_KVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IBCAST_SCATTERV_KVAL = %d\n", MPIR_CVAR_IBCAST_SCATTERV_KVAL);
    }

    defaultval.d = 2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IBCAST_ALLGATHERV_RECEXCH_KVAL, /* name */
        &MPIR_CVAR_IBCAST_ALLGATHERV_RECEXCH_KVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "k value for recursive exchange based allgather in scatter_recexch_allgather algorithm");
    MPIR_CVAR_IBCAST_ALLGATHERV_RECEXCH_KVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IBCAST_ALLGATHERV_RECEXCH_KVAL", &(MPIR_CVAR_IBCAST_ALLGATHERV_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IBCAST_ALLGATHERV_RECEXCH_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IBCAST_ALLGATHERV_RECEXCH_KVAL", &(MPIR_CVAR_IBCAST_ALLGATHERV_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IBCAST_ALLGATHERV_RECEXCH_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IBCAST_ALLGATHERV_RECEXCH_KVAL", &(MPIR_CVAR_IBCAST_ALLGATHERV_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IBCAST_ALLGATHERV_RECEXCH_KVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IBCAST_ALLGATHERV_RECEXCH_KVAL = %d\n", MPIR_CVAR_IBCAST_ALLGATHERV_RECEXCH_KVAL);
    }

    defaultval.d = MPIR_CVAR_IBCAST_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IBCAST_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_IBCAST_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select ibcast algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_flat - Force flat algorithm");
    MPIR_CVAR_IBCAST_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IBCAST_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IBCAST_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IBCAST_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IBCAST_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IBCAST_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IBCAST_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IBCAST_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IBCAST_INTER_ALGORITHM = MPIR_CVAR_IBCAST_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_IBCAST_INTER_ALGORITHM = MPIR_CVAR_IBCAST_INTER_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_flat"))
            MPIR_CVAR_IBCAST_INTER_ALGORITHM = MPIR_CVAR_IBCAST_INTER_ALGORITHM_sched_flat;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IBCAST_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 2048;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE, /* name */
        &MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "use the short message algorithm for intercommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_VSMALL_MSG_SIZE)");
    MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_GATHER_INTER_SHORT_MSG_SIZE", &(MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_GATHER_INTER_SHORT_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_GATHER_INTER_SHORT_MSG_SIZE", &(MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_GATHER_INTER_SHORT_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE", &(MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE = %d\n", MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE);
    }

    defaultval.d = MPIR_CVAR_GATHER_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_GATHER_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_GATHER_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select gather algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"binomial - Force binomial algorithm\n"
"nb       - Force nonblocking algorithm");
    MPIR_CVAR_GATHER_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_GATHER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_GATHER_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_GATHER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_GATHER_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_GATHER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_GATHER_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_GATHER_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_GATHER_INTRA_ALGORITHM = MPIR_CVAR_GATHER_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "binomial"))
            MPIR_CVAR_GATHER_INTRA_ALGORITHM = MPIR_CVAR_GATHER_INTRA_ALGORITHM_binomial;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_GATHER_INTRA_ALGORITHM = MPIR_CVAR_GATHER_INTRA_ALGORITHM_nb;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_GATHER_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_GATHER_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_GATHER_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_GATHER_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select gather algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"linear                   - Force linear algorithm\n"
"local_gather_remote_send - Force local-gather-remote-send algorithm\n"
"nb                       - Force nonblocking algorithm");
    MPIR_CVAR_GATHER_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_GATHER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_GATHER_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_GATHER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_GATHER_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_GATHER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_GATHER_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_GATHER_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_GATHER_INTER_ALGORITHM = MPIR_CVAR_GATHER_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "linear"))
            MPIR_CVAR_GATHER_INTER_ALGORITHM = MPIR_CVAR_GATHER_INTER_ALGORITHM_linear;
        else if (0 == strcmp(tmp_str, "local_gather_remote_send"))
            MPIR_CVAR_GATHER_INTER_ALGORITHM = MPIR_CVAR_GATHER_INTER_ALGORITHM_local_gather_remote_send;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_GATHER_INTER_ALGORITHM = MPIR_CVAR_GATHER_INTER_ALGORITHM_nb;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_GATHER_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_IGATHER_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IGATHER_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_IGATHER_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select igather algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_binomial     - Force binomial algorithm\n"
"tsp_tree       - Force genetric transport based tree algorithm");
    MPIR_CVAR_IGATHER_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IGATHER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IGATHER_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IGATHER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IGATHER_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IGATHER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IGATHER_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IGATHER_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IGATHER_INTRA_ALGORITHM = MPIR_CVAR_IGATHER_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_IGATHER_INTRA_ALGORITHM = MPIR_CVAR_IGATHER_INTRA_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_binomial"))
            MPIR_CVAR_IGATHER_INTRA_ALGORITHM = MPIR_CVAR_IGATHER_INTRA_ALGORITHM_sched_binomial;
        else if (0 == strcmp(tmp_str, "tsp_tree"))
            MPIR_CVAR_IGATHER_INTRA_ALGORITHM = MPIR_CVAR_IGATHER_INTRA_ALGORITHM_tsp_tree;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IGATHER_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IGATHER_TREE_KVAL, /* name */
        &MPIR_CVAR_IGATHER_TREE_KVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "k value for tree based igather");
    MPIR_CVAR_IGATHER_TREE_KVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IGATHER_TREE_KVAL", &(MPIR_CVAR_IGATHER_TREE_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IGATHER_TREE_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IGATHER_TREE_KVAL", &(MPIR_CVAR_IGATHER_TREE_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IGATHER_TREE_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IGATHER_TREE_KVAL", &(MPIR_CVAR_IGATHER_TREE_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IGATHER_TREE_KVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IGATHER_TREE_KVAL = %d\n", MPIR_CVAR_IGATHER_TREE_KVAL);
    }

    defaultval.d = MPIR_CVAR_IGATHER_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IGATHER_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_IGATHER_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select igather algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_long  - Force long inter algorithm\n"
"sched_short - Force short inter algorithm");
    MPIR_CVAR_IGATHER_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IGATHER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IGATHER_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IGATHER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IGATHER_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IGATHER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IGATHER_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IGATHER_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IGATHER_INTER_ALGORITHM = MPIR_CVAR_IGATHER_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_IGATHER_INTER_ALGORITHM = MPIR_CVAR_IGATHER_INTER_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_long"))
            MPIR_CVAR_IGATHER_INTER_ALGORITHM = MPIR_CVAR_IGATHER_INTER_ALGORITHM_sched_long;
        else if (0 == strcmp(tmp_str, "sched_short"))
            MPIR_CVAR_IGATHER_INTER_ALGORITHM = MPIR_CVAR_IGATHER_INTER_ALGORITHM_sched_short;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IGATHER_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_GATHERV_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_GATHERV_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_GATHERV_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select gatherv algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"linear - Force linear algorithm\n"
"nb     - Force nonblocking algorithm");
    MPIR_CVAR_GATHERV_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_GATHERV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_GATHERV_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_GATHERV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_GATHERV_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_GATHERV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_GATHERV_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_GATHERV_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_GATHERV_INTRA_ALGORITHM = MPIR_CVAR_GATHERV_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "linear"))
            MPIR_CVAR_GATHERV_INTRA_ALGORITHM = MPIR_CVAR_GATHERV_INTRA_ALGORITHM_linear;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_GATHERV_INTRA_ALGORITHM = MPIR_CVAR_GATHERV_INTRA_ALGORITHM_nb;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_GATHERV_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_GATHERV_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_GATHERV_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_GATHERV_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select gatherv algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"linear - Force linear algorithm\n"
"nb     - Force nonblocking algorithm");
    MPIR_CVAR_GATHERV_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_GATHERV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_GATHERV_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_GATHERV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_GATHERV_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_GATHERV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_GATHERV_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_GATHERV_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_GATHERV_INTER_ALGORITHM = MPIR_CVAR_GATHERV_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "linear"))
            MPIR_CVAR_GATHERV_INTER_ALGORITHM = MPIR_CVAR_GATHERV_INTER_ALGORITHM_linear;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_GATHERV_INTER_ALGORITHM = MPIR_CVAR_GATHERV_INTER_ALGORITHM_nb;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_GATHERV_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_IGATHERV_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IGATHERV_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_IGATHERV_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select igatherv algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_linear         - Force linear algorithm\n"
"tsp_linear       - Force generic transport based linear algorithm");
    MPIR_CVAR_IGATHERV_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IGATHERV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IGATHERV_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IGATHERV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IGATHERV_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IGATHERV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IGATHERV_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IGATHERV_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IGATHERV_INTRA_ALGORITHM = MPIR_CVAR_IGATHERV_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_IGATHERV_INTRA_ALGORITHM = MPIR_CVAR_IGATHERV_INTRA_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_linear"))
            MPIR_CVAR_IGATHERV_INTRA_ALGORITHM = MPIR_CVAR_IGATHERV_INTRA_ALGORITHM_sched_linear;
        else if (0 == strcmp(tmp_str, "tsp_linear"))
            MPIR_CVAR_IGATHERV_INTRA_ALGORITHM = MPIR_CVAR_IGATHERV_INTRA_ALGORITHM_tsp_linear;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IGATHERV_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_IGATHERV_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IGATHERV_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_IGATHERV_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select igatherv algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_linear - Force linear algorithm\n"
"tsp_linear - Force generic transport based linear algorithm");
    MPIR_CVAR_IGATHERV_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IGATHERV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IGATHERV_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IGATHERV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IGATHERV_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IGATHERV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IGATHERV_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IGATHERV_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IGATHERV_INTER_ALGORITHM = MPIR_CVAR_IGATHERV_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_IGATHERV_INTER_ALGORITHM = MPIR_CVAR_IGATHERV_INTER_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_linear"))
            MPIR_CVAR_IGATHERV_INTER_ALGORITHM = MPIR_CVAR_IGATHERV_INTER_ALGORITHM_sched_linear;
        else if (0 == strcmp(tmp_str, "tsp_linear"))
            MPIR_CVAR_IGATHERV_INTER_ALGORITHM = MPIR_CVAR_IGATHERV_INTER_ALGORITHM_tsp_linear;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IGATHERV_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 2048;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE, /* name */
        &MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "use the short message algorithm for intercommunicator MPI_Scatter if the send buffer size is < this value (in bytes)");
    MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_SCATTER_INTER_SHORT_MSG_SIZE", &(MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_SCATTER_INTER_SHORT_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_SCATTER_INTER_SHORT_MSG_SIZE", &(MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_SCATTER_INTER_SHORT_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE", &(MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE = %d\n", MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE);
    }

    defaultval.d = MPIR_CVAR_SCATTER_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_SCATTER_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_SCATTER_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select scatter algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"binomial - Force binomial algorithm\n"
"nb       - Force nonblocking algorithm");
    MPIR_CVAR_SCATTER_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_SCATTER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_SCATTER_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_SCATTER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_SCATTER_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_SCATTER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_SCATTER_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_SCATTER_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_SCATTER_INTRA_ALGORITHM = MPIR_CVAR_SCATTER_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "binomial"))
            MPIR_CVAR_SCATTER_INTRA_ALGORITHM = MPIR_CVAR_SCATTER_INTRA_ALGORITHM_binomial;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_SCATTER_INTRA_ALGORITHM = MPIR_CVAR_SCATTER_INTRA_ALGORITHM_nb;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_SCATTER_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_SCATTER_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_SCATTER_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_SCATTER_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select scatter algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"linear                    - Force linear algorithm\n"
"nb                        - Force nonblocking algorithm\n"
"remote_send_local_scatter - Force remote-send-local-scatter algorithm");
    MPIR_CVAR_SCATTER_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_SCATTER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_SCATTER_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_SCATTER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_SCATTER_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_SCATTER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_SCATTER_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_SCATTER_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_SCATTER_INTER_ALGORITHM = MPIR_CVAR_SCATTER_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "linear"))
            MPIR_CVAR_SCATTER_INTER_ALGORITHM = MPIR_CVAR_SCATTER_INTER_ALGORITHM_linear;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_SCATTER_INTER_ALGORITHM = MPIR_CVAR_SCATTER_INTER_ALGORITHM_nb;
        else if (0 == strcmp(tmp_str, "remote_send_local_scatter"))
            MPIR_CVAR_SCATTER_INTER_ALGORITHM = MPIR_CVAR_SCATTER_INTER_ALGORITHM_remote_send_local_scatter;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_SCATTER_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_ISCATTER_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ISCATTER_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_ISCATTER_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select iscatter algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_binomial     - Force binomial algorithm\n"
"tsp_tree       - Force genetric transport based tree algorithm");
    MPIR_CVAR_ISCATTER_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_ISCATTER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ISCATTER_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_ISCATTER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ISCATTER_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_ISCATTER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ISCATTER_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ISCATTER_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_ISCATTER_INTRA_ALGORITHM = MPIR_CVAR_ISCATTER_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_ISCATTER_INTRA_ALGORITHM = MPIR_CVAR_ISCATTER_INTRA_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_binomial"))
            MPIR_CVAR_ISCATTER_INTRA_ALGORITHM = MPIR_CVAR_ISCATTER_INTRA_ALGORITHM_sched_binomial;
        else if (0 == strcmp(tmp_str, "tsp_tree"))
            MPIR_CVAR_ISCATTER_INTRA_ALGORITHM = MPIR_CVAR_ISCATTER_INTRA_ALGORITHM_tsp_tree;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_ISCATTER_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ISCATTER_TREE_KVAL, /* name */
        &MPIR_CVAR_ISCATTER_TREE_KVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "k value for tree based iscatter");
    MPIR_CVAR_ISCATTER_TREE_KVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ISCATTER_TREE_KVAL", &(MPIR_CVAR_ISCATTER_TREE_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ISCATTER_TREE_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ISCATTER_TREE_KVAL", &(MPIR_CVAR_ISCATTER_TREE_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ISCATTER_TREE_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ISCATTER_TREE_KVAL", &(MPIR_CVAR_ISCATTER_TREE_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ISCATTER_TREE_KVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ISCATTER_TREE_KVAL = %d\n", MPIR_CVAR_ISCATTER_TREE_KVAL);
    }

    defaultval.d = MPIR_CVAR_ISCATTER_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ISCATTER_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_ISCATTER_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select iscatter algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_linear                    - Force linear algorithm\n"
"sched_remote_send_local_scatter - Force remote-send-local-scatter algorithm");
    MPIR_CVAR_ISCATTER_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_ISCATTER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ISCATTER_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_ISCATTER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ISCATTER_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_ISCATTER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ISCATTER_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ISCATTER_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_ISCATTER_INTER_ALGORITHM = MPIR_CVAR_ISCATTER_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_ISCATTER_INTER_ALGORITHM = MPIR_CVAR_ISCATTER_INTER_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_linear"))
            MPIR_CVAR_ISCATTER_INTER_ALGORITHM = MPIR_CVAR_ISCATTER_INTER_ALGORITHM_sched_linear;
        else if (0 == strcmp(tmp_str, "sched_remote_send_local_scatter"))
            MPIR_CVAR_ISCATTER_INTER_ALGORITHM = MPIR_CVAR_ISCATTER_INTER_ALGORITHM_sched_remote_send_local_scatter;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_ISCATTER_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_SCATTERV_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_SCATTERV_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_SCATTERV_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select scatterv algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"linear - Force linear algorithm\n"
"nb     - Force nonblocking algorithm");
    MPIR_CVAR_SCATTERV_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_SCATTERV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_SCATTERV_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_SCATTERV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_SCATTERV_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_SCATTERV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_SCATTERV_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_SCATTERV_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_SCATTERV_INTRA_ALGORITHM = MPIR_CVAR_SCATTERV_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "linear"))
            MPIR_CVAR_SCATTERV_INTRA_ALGORITHM = MPIR_CVAR_SCATTERV_INTRA_ALGORITHM_linear;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_SCATTERV_INTRA_ALGORITHM = MPIR_CVAR_SCATTERV_INTRA_ALGORITHM_nb;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_SCATTERV_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_SCATTERV_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_SCATTERV_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_SCATTERV_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select scatterv algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"linear - Force linear algorithm\n"
"nb     - Force nonblocking algorithm");
    MPIR_CVAR_SCATTERV_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_SCATTERV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_SCATTERV_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_SCATTERV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_SCATTERV_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_SCATTERV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_SCATTERV_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_SCATTERV_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_SCATTERV_INTER_ALGORITHM = MPIR_CVAR_SCATTERV_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "linear"))
            MPIR_CVAR_SCATTERV_INTER_ALGORITHM = MPIR_CVAR_SCATTERV_INTER_ALGORITHM_linear;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_SCATTERV_INTER_ALGORITHM = MPIR_CVAR_SCATTERV_INTER_ALGORITHM_nb;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_SCATTERV_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select iscatterv algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_linear    - Force linear algorithm\n"
"tsp_linear  - Force generic transport based linear algorithm");
    MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_ISCATTERV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ISCATTERV_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_ISCATTERV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ISCATTERV_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM = MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM = MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_linear"))
            MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM = MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM_sched_linear;
        else if (0 == strcmp(tmp_str, "tsp_linear"))
            MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM = MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM_tsp_linear;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_ISCATTERV_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ISCATTERV_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_ISCATTERV_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select iscatterv algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_linear - Force linear algorithm\n"
"tsp_linear - Force generic transport based linear algorithm");
    MPIR_CVAR_ISCATTERV_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_ISCATTERV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ISCATTERV_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_ISCATTERV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ISCATTERV_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_ISCATTERV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ISCATTERV_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ISCATTERV_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_ISCATTERV_INTER_ALGORITHM = MPIR_CVAR_ISCATTERV_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_ISCATTERV_INTER_ALGORITHM = MPIR_CVAR_ISCATTERV_INTER_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_linear"))
            MPIR_CVAR_ISCATTERV_INTER_ALGORITHM = MPIR_CVAR_ISCATTERV_INTER_ALGORITHM_sched_linear;
        else if (0 == strcmp(tmp_str, "tsp_linear"))
            MPIR_CVAR_ISCATTERV_INTER_ALGORITHM = MPIR_CVAR_ISCATTERV_INTER_ALGORITHM_tsp_linear;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_ISCATTERV_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 81920;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE, /* name */
        &MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "For MPI_Allgather and MPI_Allgatherv, the short message algorithm will be used if the send buffer size is < this value (in bytes). (See also: MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE)");
    MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLGATHER_SHORT_MSG_SIZE", &(MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLGATHER_SHORT_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLGATHER_SHORT_MSG_SIZE", &(MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLGATHER_SHORT_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE", &(MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE = %d\n", MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE);
    }

    defaultval.d = 524288;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE, /* name */
        &MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "For MPI_Allgather and MPI_Allgatherv, the long message algorithm will be used if the send buffer size is >= this value (in bytes) (See also: MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE)");
    MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLGATHER_LONG_MSG_SIZE", &(MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLGATHER_LONG_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLGATHER_LONG_MSG_SIZE", &(MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLGATHER_LONG_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE", &(MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE = %d\n", MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE);
    }

    defaultval.d = MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select allgather algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"brucks             - Force brucks algorithm\n"
"k_brucks           - Force brucks algorithm\n"
"nb                 - Force nonblocking algorithm\n"
"recursive_doubling - Force recursive doubling algorithm\n"
"ring               - Force ring algorithm\n"
"recexch_doubling   - Force recexch distance doubling algorithm\n"
"recexch_halving    - Force recexch distance halving algorithm");
    MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_ALLGATHER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLGATHER_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_ALLGATHER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLGATHER_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM = MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "brucks"))
            MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM = MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM_brucks;
        else if (0 == strcmp(tmp_str, "k_brucks"))
            MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM = MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM_k_brucks;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM = MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM_nb;
        else if (0 == strcmp(tmp_str, "recursive_doubling"))
            MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM = MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM_recursive_doubling;
        else if (0 == strcmp(tmp_str, "ring"))
            MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM = MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM_ring;
        else if (0 == strcmp(tmp_str, "recexch_doubling"))
            MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM = MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM_recexch_doubling;
        else if (0 == strcmp(tmp_str, "recexch_halving"))
            MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM = MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM_recexch_halving;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLGATHER_BRUCKS_KVAL, /* name */
        &MPIR_CVAR_ALLGATHER_BRUCKS_KVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "radix (k) value for generic transport brucks based allgather");
    MPIR_CVAR_ALLGATHER_BRUCKS_KVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLGATHER_BRUCKS_KVAL", &(MPIR_CVAR_ALLGATHER_BRUCKS_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLGATHER_BRUCKS_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLGATHER_BRUCKS_KVAL", &(MPIR_CVAR_ALLGATHER_BRUCKS_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLGATHER_BRUCKS_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLGATHER_BRUCKS_KVAL", &(MPIR_CVAR_ALLGATHER_BRUCKS_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLGATHER_BRUCKS_KVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLGATHER_BRUCKS_KVAL = %d\n", MPIR_CVAR_ALLGATHER_BRUCKS_KVAL);
    }

    defaultval.d = 2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLGATHER_RECEXCH_KVAL, /* name */
        &MPIR_CVAR_ALLGATHER_RECEXCH_KVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "k value for recursive exchange based allgather");
    MPIR_CVAR_ALLGATHER_RECEXCH_KVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLGATHER_RECEXCH_KVAL", &(MPIR_CVAR_ALLGATHER_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLGATHER_RECEXCH_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLGATHER_RECEXCH_KVAL", &(MPIR_CVAR_ALLGATHER_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLGATHER_RECEXCH_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLGATHER_RECEXCH_KVAL", &(MPIR_CVAR_ALLGATHER_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLGATHER_RECEXCH_KVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLGATHER_RECEXCH_KVAL = %d\n", MPIR_CVAR_ALLGATHER_RECEXCH_KVAL);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLGATHER_RECEXCH_SINGLE_PHASE_RECV, /* name */
        &MPIR_CVAR_ALLGATHER_RECEXCH_SINGLE_PHASE_RECV, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR controls whether the recv is posted for one phase or two phases in recexch algos. By default, we post the recvs for 2 phases.");
    MPIR_CVAR_ALLGATHER_RECEXCH_SINGLE_PHASE_RECV = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_ALLGATHER_RECEXCH_SINGLE_PHASE_RECV", &(MPIR_CVAR_ALLGATHER_RECEXCH_SINGLE_PHASE_RECV));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLGATHER_RECEXCH_SINGLE_PHASE_RECV");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_ALLGATHER_RECEXCH_SINGLE_PHASE_RECV", &(MPIR_CVAR_ALLGATHER_RECEXCH_SINGLE_PHASE_RECV));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLGATHER_RECEXCH_SINGLE_PHASE_RECV");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_ALLGATHER_RECEXCH_SINGLE_PHASE_RECV", &(MPIR_CVAR_ALLGATHER_RECEXCH_SINGLE_PHASE_RECV));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLGATHER_RECEXCH_SINGLE_PHASE_RECV");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLGATHER_RECEXCH_SINGLE_PHASE_RECV = %d\n", MPIR_CVAR_ALLGATHER_RECEXCH_SINGLE_PHASE_RECV);
    }

    defaultval.d = MPIR_CVAR_ALLGATHER_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLGATHER_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_ALLGATHER_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select allgather algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"local_gather_remote_bcast - Force local-gather-remote-bcast algorithm\n"
"nb                        - Force nonblocking algorithm");
    MPIR_CVAR_ALLGATHER_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_ALLGATHER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLGATHER_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_ALLGATHER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLGATHER_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_ALLGATHER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLGATHER_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLGATHER_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_ALLGATHER_INTER_ALGORITHM = MPIR_CVAR_ALLGATHER_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "local_gather_remote_bcast"))
            MPIR_CVAR_ALLGATHER_INTER_ALGORITHM = MPIR_CVAR_ALLGATHER_INTER_ALGORITHM_local_gather_remote_bcast;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_ALLGATHER_INTER_ALGORITHM = MPIR_CVAR_ALLGATHER_INTER_ALGORITHM_nb;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_ALLGATHER_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLGATHER_RECEXCH_KVAL, /* name */
        &MPIR_CVAR_IALLGATHER_RECEXCH_KVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "k value for recursive exchange based iallgather");
    MPIR_CVAR_IALLGATHER_RECEXCH_KVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IALLGATHER_RECEXCH_KVAL", &(MPIR_CVAR_IALLGATHER_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLGATHER_RECEXCH_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IALLGATHER_RECEXCH_KVAL", &(MPIR_CVAR_IALLGATHER_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLGATHER_RECEXCH_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IALLGATHER_RECEXCH_KVAL", &(MPIR_CVAR_IALLGATHER_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLGATHER_RECEXCH_KVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLGATHER_RECEXCH_KVAL = %d\n", MPIR_CVAR_IALLGATHER_RECEXCH_KVAL);
    }

    defaultval.d = 2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLGATHER_BRUCKS_KVAL, /* name */
        &MPIR_CVAR_IALLGATHER_BRUCKS_KVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "k value for radix in brucks based iallgather");
    MPIR_CVAR_IALLGATHER_BRUCKS_KVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IALLGATHER_BRUCKS_KVAL", &(MPIR_CVAR_IALLGATHER_BRUCKS_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLGATHER_BRUCKS_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IALLGATHER_BRUCKS_KVAL", &(MPIR_CVAR_IALLGATHER_BRUCKS_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLGATHER_BRUCKS_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IALLGATHER_BRUCKS_KVAL", &(MPIR_CVAR_IALLGATHER_BRUCKS_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLGATHER_BRUCKS_KVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLGATHER_BRUCKS_KVAL = %d\n", MPIR_CVAR_IALLGATHER_BRUCKS_KVAL);
    }

    defaultval.d = MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select iallgather algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_ring               - Force ring algorithm\n"
"sched_brucks             - Force brucks algorithm\n"
"sched_recursive_doubling - Force recursive doubling algorithm\n"
"tsp_ring       - Force generic transport ring algorithm\n"
"tsp_brucks     - Force generic transport based brucks algorithm\n"
"tsp_recexch_doubling - Force generic transport recursive exchange with neighbours doubling in distance in each phase\n"
"tsp_recexch_halving  - Force generic transport recursive exchange with neighbours halving in distance in each phase");
    MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IALLGATHER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLGATHER_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IALLGATHER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLGATHER_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM = MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM = MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_ring"))
            MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM = MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM_sched_ring;
        else if (0 == strcmp(tmp_str, "sched_brucks"))
            MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM = MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM_sched_brucks;
        else if (0 == strcmp(tmp_str, "sched_recursive_doubling"))
            MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM = MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM_sched_recursive_doubling;
        else if (0 == strcmp(tmp_str, "tsp_ring"))
            MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM = MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM_tsp_ring;
        else if (0 == strcmp(tmp_str, "tsp_brucks"))
            MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM = MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM_tsp_brucks;
        else if (0 == strcmp(tmp_str, "tsp_recexch_doubling"))
            MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM = MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM_tsp_recexch_doubling;
        else if (0 == strcmp(tmp_str, "tsp_recexch_halving"))
            MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM = MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM_tsp_recexch_halving;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_IALLGATHER_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLGATHER_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_IALLGATHER_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select iallgather algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_local_gather_remote_bcast - Force local-gather-remote-bcast algorithm");
    MPIR_CVAR_IALLGATHER_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IALLGATHER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLGATHER_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IALLGATHER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLGATHER_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IALLGATHER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLGATHER_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLGATHER_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IALLGATHER_INTER_ALGORITHM = MPIR_CVAR_IALLGATHER_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_IALLGATHER_INTER_ALGORITHM = MPIR_CVAR_IALLGATHER_INTER_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_local_gather_remote_bcast"))
            MPIR_CVAR_IALLGATHER_INTER_ALGORITHM = MPIR_CVAR_IALLGATHER_INTER_ALGORITHM_sched_local_gather_remote_bcast;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IALLGATHER_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 32768;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE, /* name */
        &MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "The smallest message size that will be used for the pipelined, large-message, ring algorithm in the MPI_Allgatherv implementation.");
    MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLGATHERV_PIPELINE_MSG_SIZE", &(MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLGATHERV_PIPELINE_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLGATHERV_PIPELINE_MSG_SIZE", &(MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLGATHERV_PIPELINE_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE", &(MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE = %d\n", MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE);
    }

    defaultval.d = MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select allgatherv algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"brucks             - Force brucks algorithm\n"
"nb                 - Force nonblocking algorithm\n"
"recursive_doubling - Force recursive doubling algorithm\n"
"ring               - Force ring algorithm");
    MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_ALLGATHERV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLGATHERV_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_ALLGATHERV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLGATHERV_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM = MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "brucks"))
            MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM = MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM_brucks;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM = MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM_nb;
        else if (0 == strcmp(tmp_str, "recursive_doubling"))
            MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM = MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM_recursive_doubling;
        else if (0 == strcmp(tmp_str, "ring"))
            MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM = MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM_ring;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_ALLGATHERV_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLGATHERV_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_ALLGATHERV_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select allgatherv algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"nb                        - Force nonblocking algorithm\n"
"remote_gather_local_bcast - Force remote-gather-local-bcast algorithm");
    MPIR_CVAR_ALLGATHERV_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_ALLGATHERV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLGATHERV_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_ALLGATHERV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLGATHERV_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_ALLGATHERV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLGATHERV_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLGATHERV_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_ALLGATHERV_INTER_ALGORITHM = MPIR_CVAR_ALLGATHERV_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_ALLGATHERV_INTER_ALGORITHM = MPIR_CVAR_ALLGATHERV_INTER_ALGORITHM_nb;
        else if (0 == strcmp(tmp_str, "remote_gather_local_bcast"))
            MPIR_CVAR_ALLGATHERV_INTER_ALGORITHM = MPIR_CVAR_ALLGATHERV_INTER_ALGORITHM_remote_gather_local_bcast;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_ALLGATHERV_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLGATHERV_RECEXCH_KVAL, /* name */
        &MPIR_CVAR_IALLGATHERV_RECEXCH_KVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "k value for recursive exchange based iallgatherv");
    MPIR_CVAR_IALLGATHERV_RECEXCH_KVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IALLGATHERV_RECEXCH_KVAL", &(MPIR_CVAR_IALLGATHERV_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLGATHERV_RECEXCH_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IALLGATHERV_RECEXCH_KVAL", &(MPIR_CVAR_IALLGATHERV_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLGATHERV_RECEXCH_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IALLGATHERV_RECEXCH_KVAL", &(MPIR_CVAR_IALLGATHERV_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLGATHERV_RECEXCH_KVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLGATHERV_RECEXCH_KVAL = %d\n", MPIR_CVAR_IALLGATHERV_RECEXCH_KVAL);
    }

    defaultval.d = 2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLGATHERV_BRUCKS_KVAL, /* name */
        &MPIR_CVAR_IALLGATHERV_BRUCKS_KVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "k value for radix in brucks based iallgatherv");
    MPIR_CVAR_IALLGATHERV_BRUCKS_KVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IALLGATHERV_BRUCKS_KVAL", &(MPIR_CVAR_IALLGATHERV_BRUCKS_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLGATHERV_BRUCKS_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IALLGATHERV_BRUCKS_KVAL", &(MPIR_CVAR_IALLGATHERV_BRUCKS_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLGATHERV_BRUCKS_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IALLGATHERV_BRUCKS_KVAL", &(MPIR_CVAR_IALLGATHERV_BRUCKS_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLGATHERV_BRUCKS_KVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLGATHERV_BRUCKS_KVAL = %d\n", MPIR_CVAR_IALLGATHERV_BRUCKS_KVAL);
    }

    defaultval.d = MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select iallgatherv algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_brucks             - Force brucks algorithm\n"
"sched_recursive_doubling - Force recursive doubling algorithm\n"
"sched_ring               - Force ring algorithm\n"
"tsp_recexch_doubling - Force generic transport recursive exchange with neighbours doubling in distance in each phase\n"
"tsp_recexch_halving  - Force generic transport recursive exchange with neighbours halving in distance in each phase\n"
"tsp_ring             - Force generic transport ring algorithm\n"
"tsp_brucks           - Force generic transport based brucks algorithm");
    MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IALLGATHERV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLGATHERV_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IALLGATHERV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLGATHERV_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM = MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM = MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_brucks"))
            MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM = MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM_sched_brucks;
        else if (0 == strcmp(tmp_str, "sched_recursive_doubling"))
            MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM = MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM_sched_recursive_doubling;
        else if (0 == strcmp(tmp_str, "sched_ring"))
            MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM = MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM_sched_ring;
        else if (0 == strcmp(tmp_str, "tsp_recexch_doubling"))
            MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM = MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM_tsp_recexch_doubling;
        else if (0 == strcmp(tmp_str, "tsp_recexch_halving"))
            MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM = MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM_tsp_recexch_halving;
        else if (0 == strcmp(tmp_str, "tsp_ring"))
            MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM = MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM_tsp_ring;
        else if (0 == strcmp(tmp_str, "tsp_brucks"))
            MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM = MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM_tsp_brucks;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_IALLGATHERV_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLGATHERV_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_IALLGATHERV_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select iallgatherv algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_remote_gather_local_bcast - Force remote-gather-local-bcast algorithm");
    MPIR_CVAR_IALLGATHERV_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IALLGATHERV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLGATHERV_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IALLGATHERV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLGATHERV_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IALLGATHERV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLGATHERV_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLGATHERV_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IALLGATHERV_INTER_ALGORITHM = MPIR_CVAR_IALLGATHERV_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_IALLGATHERV_INTER_ALGORITHM = MPIR_CVAR_IALLGATHERV_INTER_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_remote_gather_local_bcast"))
            MPIR_CVAR_IALLGATHERV_INTER_ALGORITHM = MPIR_CVAR_IALLGATHERV_INTER_ALGORITHM_sched_remote_gather_local_bcast;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IALLGATHERV_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 256;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE, /* name */
        &MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "the short message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value (See also: MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE)");
    MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLTOALL_SHORT_MSG_SIZE", &(MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLTOALL_SHORT_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLTOALL_SHORT_MSG_SIZE", &(MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLTOALL_SHORT_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE", &(MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE = %d\n", MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE);
    }

    defaultval.d = 32768;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE, /* name */
        &MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "the medium message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value and larger than MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE (See also: MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE)");
    MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLTOALL_MEDIUM_MSG_SIZE", &(MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLTOALL_MEDIUM_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLTOALL_MEDIUM_MSG_SIZE", &(MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLTOALL_MEDIUM_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE", &(MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE = %d\n", MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE);
    }

    defaultval.d = 32;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLTOALL_THROTTLE, /* name */
        &MPIR_CVAR_ALLTOALL_THROTTLE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "max no. of irecvs/isends posted at a time in some alltoall algorithms. Setting it to 0 causes all irecvs/isends to be posted at once");
    MPIR_CVAR_ALLTOALL_THROTTLE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLTOALL_THROTTLE", &(MPIR_CVAR_ALLTOALL_THROTTLE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLTOALL_THROTTLE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLTOALL_THROTTLE", &(MPIR_CVAR_ALLTOALL_THROTTLE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLTOALL_THROTTLE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLTOALL_THROTTLE", &(MPIR_CVAR_ALLTOALL_THROTTLE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLTOALL_THROTTLE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLTOALL_THROTTLE = %d\n", MPIR_CVAR_ALLTOALL_THROTTLE);
    }

    defaultval.d = MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select alltoall algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"brucks                    - Force brucks algorithm\n"
"k_brucks                  - Force Force radix k brucks algorithm\n"
"nb                        - Force nonblocking algorithm\n"
"pairwise                  - Force pairwise algorithm\n"
"pairwise_sendrecv_replace - Force pairwise sendrecv replace algorithm\n"
"scattered                 - Force scattered algorithm");
    MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_ALLTOALL_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLTOALL_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_ALLTOALL_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLTOALL_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM = MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "brucks"))
            MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM = MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM_brucks;
        else if (0 == strcmp(tmp_str, "k_brucks"))
            MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM = MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM_k_brucks;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM = MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM_nb;
        else if (0 == strcmp(tmp_str, "pairwise"))
            MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM = MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM_pairwise;
        else if (0 == strcmp(tmp_str, "pairwise_sendrecv_replace"))
            MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM = MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM_pairwise_sendrecv_replace;
        else if (0 == strcmp(tmp_str, "scattered"))
            MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM = MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM_scattered;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLTOALL_BRUCKS_KVAL, /* name */
        &MPIR_CVAR_ALLTOALL_BRUCKS_KVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "radix (k) value for generic transport brucks based alltoall");
    MPIR_CVAR_ALLTOALL_BRUCKS_KVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLTOALL_BRUCKS_KVAL", &(MPIR_CVAR_ALLTOALL_BRUCKS_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLTOALL_BRUCKS_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLTOALL_BRUCKS_KVAL", &(MPIR_CVAR_ALLTOALL_BRUCKS_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLTOALL_BRUCKS_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLTOALL_BRUCKS_KVAL", &(MPIR_CVAR_ALLTOALL_BRUCKS_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLTOALL_BRUCKS_KVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLTOALL_BRUCKS_KVAL = %d\n", MPIR_CVAR_ALLTOALL_BRUCKS_KVAL);
    }

    defaultval.d = MPIR_CVAR_ALLTOALL_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLTOALL_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_ALLTOALL_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select alltoall algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"nb                - Force nonblocking algorithm\n"
"pairwise_exchange - Force pairwise exchange algorithm");
    MPIR_CVAR_ALLTOALL_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_ALLTOALL_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLTOALL_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_ALLTOALL_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLTOALL_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_ALLTOALL_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLTOALL_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLTOALL_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_ALLTOALL_INTER_ALGORITHM = MPIR_CVAR_ALLTOALL_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_ALLTOALL_INTER_ALGORITHM = MPIR_CVAR_ALLTOALL_INTER_ALGORITHM_nb;
        else if (0 == strcmp(tmp_str, "pairwise_exchange"))
            MPIR_CVAR_ALLTOALL_INTER_ALGORITHM = MPIR_CVAR_ALLTOALL_INTER_ALGORITHM_pairwise_exchange;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_ALLTOALL_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select ialltoall algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_brucks            - Force brucks algorithm\n"
"sched_inplace           - Force inplace algorithm\n"
"sched_pairwise          - Force pairwise algorithm\n"
"sched_permuted_sendrecv - Force permuted sendrecv algorithm\n"
"tsp_ring            - Force generic transport based ring algorithm\n"
"tsp_brucks          - Force generic transport based brucks algorithm\n"
"tsp_scattered       - Force generic transport based scattered algorithm");
    MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IALLTOALL_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLTOALL_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IALLTOALL_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLTOALL_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM = MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM = MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_brucks"))
            MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM = MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM_sched_brucks;
        else if (0 == strcmp(tmp_str, "sched_inplace"))
            MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM = MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM_sched_inplace;
        else if (0 == strcmp(tmp_str, "sched_pairwise"))
            MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM = MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM_sched_pairwise;
        else if (0 == strcmp(tmp_str, "sched_permuted_sendrecv"))
            MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM = MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM_sched_permuted_sendrecv;
        else if (0 == strcmp(tmp_str, "tsp_ring"))
            MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM = MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM_tsp_ring;
        else if (0 == strcmp(tmp_str, "tsp_brucks"))
            MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM = MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM_tsp_brucks;
        else if (0 == strcmp(tmp_str, "tsp_scattered"))
            MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM = MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM_tsp_scattered;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_IALLTOALL_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLTOALL_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_IALLTOALL_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select ialltoall algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_pairwise_exchange - Force pairwise exchange algorithm");
    MPIR_CVAR_IALLTOALL_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IALLTOALL_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLTOALL_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IALLTOALL_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLTOALL_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IALLTOALL_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLTOALL_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLTOALL_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IALLTOALL_INTER_ALGORITHM = MPIR_CVAR_IALLTOALL_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_IALLTOALL_INTER_ALGORITHM = MPIR_CVAR_IALLTOALL_INTER_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_pairwise_exchange"))
            MPIR_CVAR_IALLTOALL_INTER_ALGORITHM = MPIR_CVAR_IALLTOALL_INTER_ALGORITHM_sched_pairwise_exchange;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IALLTOALL_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select alltoallv algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"nb                        - Force nonblocking algorithm\n"
"pairwise_sendrecv_replace - Force pairwise_sendrecv_replace algorithm\n"
"scattered                 - Force scattered algorithm");
    MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_ALLTOALLV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLTOALLV_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_ALLTOALLV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLTOALLV_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM = MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM = MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM_nb;
        else if (0 == strcmp(tmp_str, "pairwise_sendrecv_replace"))
            MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM = MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM_pairwise_sendrecv_replace;
        else if (0 == strcmp(tmp_str, "scattered"))
            MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM = MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM_scattered;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_ALLTOALLV_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLTOALLV_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_ALLTOALLV_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select alltoallv algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"pairwise_exchange - Force pairwise exchange algorithm\n"
"nb                - Force nonblocking algorithm");
    MPIR_CVAR_ALLTOALLV_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_ALLTOALLV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLTOALLV_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_ALLTOALLV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLTOALLV_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_ALLTOALLV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLTOALLV_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLTOALLV_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_ALLTOALLV_INTER_ALGORITHM = MPIR_CVAR_ALLTOALLV_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "pairwise_exchange"))
            MPIR_CVAR_ALLTOALLV_INTER_ALGORITHM = MPIR_CVAR_ALLTOALLV_INTER_ALGORITHM_pairwise_exchange;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_ALLTOALLV_INTER_ALGORITHM = MPIR_CVAR_ALLTOALLV_INTER_ALGORITHM_nb;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_ALLTOALLV_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select ialltoallv algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_blocked           - Force blocked algorithm\n"
"sched_inplace           - Force inplace algorithm\n"
"tsp_scattered       - Force generic transport based scattered algorithm\n"
"tsp_blocked         - Force generic transport blocked algorithm\n"
"tsp_inplace         - Force generic transport inplace algorithm");
    MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IALLTOALLV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLTOALLV_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IALLTOALLV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLTOALLV_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM = MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM = MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_blocked"))
            MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM = MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM_sched_blocked;
        else if (0 == strcmp(tmp_str, "sched_inplace"))
            MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM = MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM_sched_inplace;
        else if (0 == strcmp(tmp_str, "tsp_scattered"))
            MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM = MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM_tsp_scattered;
        else if (0 == strcmp(tmp_str, "tsp_blocked"))
            MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM = MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM_tsp_blocked;
        else if (0 == strcmp(tmp_str, "tsp_inplace"))
            MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM = MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM_tsp_inplace;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_IALLTOALLV_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLTOALLV_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_IALLTOALLV_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select ialltoallv algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_pairwise_exchange - Force pairwise exchange algorithm");
    MPIR_CVAR_IALLTOALLV_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IALLTOALLV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLTOALLV_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IALLTOALLV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLTOALLV_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IALLTOALLV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLTOALLV_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLTOALLV_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IALLTOALLV_INTER_ALGORITHM = MPIR_CVAR_IALLTOALLV_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_IALLTOALLV_INTER_ALGORITHM = MPIR_CVAR_IALLTOALLV_INTER_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_pairwise_exchange"))
            MPIR_CVAR_IALLTOALLV_INTER_ALGORITHM = MPIR_CVAR_IALLTOALLV_INTER_ALGORITHM_sched_pairwise_exchange;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IALLTOALLV_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 64;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLTOALLV_SCATTERED_OUTSTANDING_TASKS, /* name */
        &MPIR_CVAR_IALLTOALLV_SCATTERED_OUTSTANDING_TASKS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Maximum number of outstanding sends and recvs posted at a time");
    MPIR_CVAR_IALLTOALLV_SCATTERED_OUTSTANDING_TASKS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IALLTOALLV_SCATTERED_OUTSTANDING_TASKS", &(MPIR_CVAR_IALLTOALLV_SCATTERED_OUTSTANDING_TASKS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLTOALLV_SCATTERED_OUTSTANDING_TASKS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IALLTOALLV_SCATTERED_OUTSTANDING_TASKS", &(MPIR_CVAR_IALLTOALLV_SCATTERED_OUTSTANDING_TASKS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLTOALLV_SCATTERED_OUTSTANDING_TASKS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IALLTOALLV_SCATTERED_OUTSTANDING_TASKS", &(MPIR_CVAR_IALLTOALLV_SCATTERED_OUTSTANDING_TASKS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLTOALLV_SCATTERED_OUTSTANDING_TASKS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLTOALLV_SCATTERED_OUTSTANDING_TASKS = %d\n", MPIR_CVAR_IALLTOALLV_SCATTERED_OUTSTANDING_TASKS);
    }

    defaultval.d = 4;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLTOALLV_SCATTERED_BATCH_SIZE, /* name */
        &MPIR_CVAR_IALLTOALLV_SCATTERED_BATCH_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Number of send/receive tasks that scattered algorithm waits for completion before posting another batch of send/receives of that size");
    MPIR_CVAR_IALLTOALLV_SCATTERED_BATCH_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IALLTOALLV_SCATTERED_BATCH_SIZE", &(MPIR_CVAR_IALLTOALLV_SCATTERED_BATCH_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLTOALLV_SCATTERED_BATCH_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IALLTOALLV_SCATTERED_BATCH_SIZE", &(MPIR_CVAR_IALLTOALLV_SCATTERED_BATCH_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLTOALLV_SCATTERED_BATCH_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IALLTOALLV_SCATTERED_BATCH_SIZE", &(MPIR_CVAR_IALLTOALLV_SCATTERED_BATCH_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLTOALLV_SCATTERED_BATCH_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLTOALLV_SCATTERED_BATCH_SIZE = %d\n", MPIR_CVAR_IALLTOALLV_SCATTERED_BATCH_SIZE);
    }

    defaultval.d = MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select alltoallw algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"nb                        - Force nonblocking algorithm\n"
"pairwise_sendrecv_replace - Force pairwise sendrecv replace algorithm\n"
"scattered                 - Force scattered algorithm");
    MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_ALLTOALLW_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLTOALLW_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_ALLTOALLW_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLTOALLW_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM = MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM = MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM_nb;
        else if (0 == strcmp(tmp_str, "pairwise_sendrecv_replace"))
            MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM = MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM_pairwise_sendrecv_replace;
        else if (0 == strcmp(tmp_str, "scattered"))
            MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM = MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM_scattered;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_ALLTOALLW_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLTOALLW_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_ALLTOALLW_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select alltoallw algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"nb                - Force nonblocking algorithm\n"
"pairwise_exchange - Force pairwise exchange algorithm");
    MPIR_CVAR_ALLTOALLW_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_ALLTOALLW_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLTOALLW_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_ALLTOALLW_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLTOALLW_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_ALLTOALLW_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLTOALLW_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLTOALLW_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_ALLTOALLW_INTER_ALGORITHM = MPIR_CVAR_ALLTOALLW_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_ALLTOALLW_INTER_ALGORITHM = MPIR_CVAR_ALLTOALLW_INTER_ALGORITHM_nb;
        else if (0 == strcmp(tmp_str, "pairwise_exchange"))
            MPIR_CVAR_ALLTOALLW_INTER_ALGORITHM = MPIR_CVAR_ALLTOALLW_INTER_ALGORITHM_pairwise_exchange;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_ALLTOALLW_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select ialltoallw algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_blocked           - Force blocked algorithm\n"
"sched_inplace           - Force inplace algorithm\n"
"tsp_blocked   - Force generic transport based blocked algorithm\n"
"tsp_inplace   - Force generic transport based inplace algorithm");
    MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IALLTOALLW_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLTOALLW_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IALLTOALLW_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLTOALLW_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM = MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM = MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_blocked"))
            MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM = MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM_sched_blocked;
        else if (0 == strcmp(tmp_str, "sched_inplace"))
            MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM = MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM_sched_inplace;
        else if (0 == strcmp(tmp_str, "tsp_blocked"))
            MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM = MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM_tsp_blocked;
        else if (0 == strcmp(tmp_str, "tsp_inplace"))
            MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM = MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM_tsp_inplace;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_IALLTOALLW_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLTOALLW_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_IALLTOALLW_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select ialltoallw algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_pairwise_exchange - Force pairwise exchange algorithm");
    MPIR_CVAR_IALLTOALLW_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IALLTOALLW_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLTOALLW_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IALLTOALLW_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLTOALLW_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IALLTOALLW_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLTOALLW_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLTOALLW_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IALLTOALLW_INTER_ALGORITHM = MPIR_CVAR_IALLTOALLW_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_IALLTOALLW_INTER_ALGORITHM = MPIR_CVAR_IALLTOALLW_INTER_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_pairwise_exchange"))
            MPIR_CVAR_IALLTOALLW_INTER_ALGORITHM = MPIR_CVAR_IALLTOALLW_INTER_ALGORITHM_sched_pairwise_exchange;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IALLTOALLW_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 2048;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_REDUCE_SHORT_MSG_SIZE, /* name */
        &MPIR_CVAR_REDUCE_SHORT_MSG_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "the short message algorithm will be used if the send buffer size is <= this value (in bytes)");
    MPIR_CVAR_REDUCE_SHORT_MSG_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_REDUCE_SHORT_MSG_SIZE", &(MPIR_CVAR_REDUCE_SHORT_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_REDUCE_SHORT_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_REDUCE_SHORT_MSG_SIZE", &(MPIR_CVAR_REDUCE_SHORT_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_REDUCE_SHORT_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_REDUCE_SHORT_MSG_SIZE", &(MPIR_CVAR_REDUCE_SHORT_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_REDUCE_SHORT_MSG_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_REDUCE_SHORT_MSG_SIZE = %d\n", MPIR_CVAR_REDUCE_SHORT_MSG_SIZE);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE, /* name */
        &MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Maximum message size for which SMP-aware reduce is used.  A value of '0' uses SMP-aware reduce for all message sizes.");
    MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_MAX_SMP_REDUCE_MSG_SIZE", &(MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_MAX_SMP_REDUCE_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_MAX_SMP_REDUCE_MSG_SIZE", &(MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_MAX_SMP_REDUCE_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE", &(MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE = %d\n", MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE);
    }

    defaultval.d = MPIR_CVAR_REDUCE_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_REDUCE_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_REDUCE_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select reduce algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"binomial              - Force binomial algorithm\n"
"nb                    - Force nonblocking algorithm\n"
"smp                   - Force smp algorithm\n"
"reduce_scatter_gather - Force reduce scatter gather algorithm");
    MPIR_CVAR_REDUCE_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_REDUCE_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_REDUCE_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_REDUCE_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_REDUCE_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_REDUCE_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_REDUCE_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_REDUCE_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_REDUCE_INTRA_ALGORITHM = MPIR_CVAR_REDUCE_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "binomial"))
            MPIR_CVAR_REDUCE_INTRA_ALGORITHM = MPIR_CVAR_REDUCE_INTRA_ALGORITHM_binomial;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_REDUCE_INTRA_ALGORITHM = MPIR_CVAR_REDUCE_INTRA_ALGORITHM_nb;
        else if (0 == strcmp(tmp_str, "smp"))
            MPIR_CVAR_REDUCE_INTRA_ALGORITHM = MPIR_CVAR_REDUCE_INTRA_ALGORITHM_smp;
        else if (0 == strcmp(tmp_str, "reduce_scatter_gather"))
            MPIR_CVAR_REDUCE_INTRA_ALGORITHM = MPIR_CVAR_REDUCE_INTRA_ALGORITHM_reduce_scatter_gather;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_REDUCE_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_REDUCE_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_REDUCE_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_REDUCE_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select reduce algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"local_reduce_remote_send - Force local-reduce-remote-send algorithm\n"
"nb                       - Force nonblocking algorithm");
    MPIR_CVAR_REDUCE_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_REDUCE_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_REDUCE_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_REDUCE_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_REDUCE_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_REDUCE_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_REDUCE_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_REDUCE_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_REDUCE_INTER_ALGORITHM = MPIR_CVAR_REDUCE_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "local_reduce_remote_send"))
            MPIR_CVAR_REDUCE_INTER_ALGORITHM = MPIR_CVAR_REDUCE_INTER_ALGORITHM_local_reduce_remote_send;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_REDUCE_INTER_ALGORITHM = MPIR_CVAR_REDUCE_INTER_ALGORITHM_nb;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_REDUCE_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IREDUCE_TREE_KVAL, /* name */
        &MPIR_CVAR_IREDUCE_TREE_KVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "k value for tree (kary, knomial, etc.) based ireduce");
    MPIR_CVAR_IREDUCE_TREE_KVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IREDUCE_TREE_KVAL", &(MPIR_CVAR_IREDUCE_TREE_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IREDUCE_TREE_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IREDUCE_TREE_KVAL", &(MPIR_CVAR_IREDUCE_TREE_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IREDUCE_TREE_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IREDUCE_TREE_KVAL", &(MPIR_CVAR_IREDUCE_TREE_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IREDUCE_TREE_KVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IREDUCE_TREE_KVAL = %d\n", MPIR_CVAR_IREDUCE_TREE_KVAL);
    }

    defaultval.str = (const char *) "kary";
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_CHAR,
        MPIR_CVAR_IREDUCE_TREE_TYPE, /* name */
        &MPIR_CVAR_IREDUCE_TREE_TYPE, /* address */
        MPIR_CVAR_MAX_STRLEN, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Tree type for tree based ireduce kary      - kary tree knomial_1 - knomial_1 tree knomial_2 - knomial_2 tree topology_aware - topology_aware tree type topology_aware_k - topology_aware tree type with branching factor k topology_wave - topology_wave tree type");
    tmp_str = defaultval.str;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IREDUCE_TREE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IREDUCE_TREE_TYPE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IREDUCE_TREE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IREDUCE_TREE_TYPE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IREDUCE_TREE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IREDUCE_TREE_TYPE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IREDUCE_TREE_TYPE = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        MPIR_CVAR_IREDUCE_TREE_TYPE = MPL_strdup(tmp_str);
        MPIR_CVAR_assert(MPIR_CVAR_IREDUCE_TREE_TYPE);
        if (MPIR_CVAR_IREDUCE_TREE_TYPE == NULL) {
            MPIR_CHKMEM_SETERR(mpi_errno, strlen(tmp_str), "dup of string for MPIR_CVAR_IREDUCE_TREE_TYPE");
            goto fn_fail;
        }
    }
    else {
        MPIR_CVAR_IREDUCE_TREE_TYPE = NULL;
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IREDUCE_TOPO_REORDER_ENABLE, /* name */
        &MPIR_CVAR_IREDUCE_TOPO_REORDER_ENABLE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This cvar controls if the leaders are reordered based on the number of ranks in each group.");
    MPIR_CVAR_IREDUCE_TOPO_REORDER_ENABLE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_IREDUCE_TOPO_REORDER_ENABLE", &(MPIR_CVAR_IREDUCE_TOPO_REORDER_ENABLE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IREDUCE_TOPO_REORDER_ENABLE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_IREDUCE_TOPO_REORDER_ENABLE", &(MPIR_CVAR_IREDUCE_TOPO_REORDER_ENABLE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IREDUCE_TOPO_REORDER_ENABLE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_IREDUCE_TOPO_REORDER_ENABLE", &(MPIR_CVAR_IREDUCE_TOPO_REORDER_ENABLE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IREDUCE_TOPO_REORDER_ENABLE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IREDUCE_TOPO_REORDER_ENABLE = %d\n", MPIR_CVAR_IREDUCE_TOPO_REORDER_ENABLE);
    }

    defaultval.d = 200;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IREDUCE_TOPO_OVERHEAD, /* name */
        &MPIR_CVAR_IREDUCE_TOPO_OVERHEAD, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This cvar controls the size of the overhead.");
    MPIR_CVAR_IREDUCE_TOPO_OVERHEAD = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IREDUCE_TOPO_OVERHEAD", &(MPIR_CVAR_IREDUCE_TOPO_OVERHEAD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IREDUCE_TOPO_OVERHEAD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IREDUCE_TOPO_OVERHEAD", &(MPIR_CVAR_IREDUCE_TOPO_OVERHEAD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IREDUCE_TOPO_OVERHEAD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IREDUCE_TOPO_OVERHEAD", &(MPIR_CVAR_IREDUCE_TOPO_OVERHEAD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IREDUCE_TOPO_OVERHEAD");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IREDUCE_TOPO_OVERHEAD = %d\n", MPIR_CVAR_IREDUCE_TOPO_OVERHEAD);
    }

    defaultval.d = 2800;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IREDUCE_TOPO_DIFF_GROUPS, /* name */
        &MPIR_CVAR_IREDUCE_TOPO_DIFF_GROUPS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This cvar controls the latency between different groups.");
    MPIR_CVAR_IREDUCE_TOPO_DIFF_GROUPS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IREDUCE_TOPO_DIFF_GROUPS", &(MPIR_CVAR_IREDUCE_TOPO_DIFF_GROUPS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IREDUCE_TOPO_DIFF_GROUPS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IREDUCE_TOPO_DIFF_GROUPS", &(MPIR_CVAR_IREDUCE_TOPO_DIFF_GROUPS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IREDUCE_TOPO_DIFF_GROUPS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IREDUCE_TOPO_DIFF_GROUPS", &(MPIR_CVAR_IREDUCE_TOPO_DIFF_GROUPS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IREDUCE_TOPO_DIFF_GROUPS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IREDUCE_TOPO_DIFF_GROUPS = %d\n", MPIR_CVAR_IREDUCE_TOPO_DIFF_GROUPS);
    }

    defaultval.d = 1900;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IREDUCE_TOPO_DIFF_SWITCHES, /* name */
        &MPIR_CVAR_IREDUCE_TOPO_DIFF_SWITCHES, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This cvar controls the latency between different switches in the same groups.");
    MPIR_CVAR_IREDUCE_TOPO_DIFF_SWITCHES = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IREDUCE_TOPO_DIFF_SWITCHES", &(MPIR_CVAR_IREDUCE_TOPO_DIFF_SWITCHES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IREDUCE_TOPO_DIFF_SWITCHES");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IREDUCE_TOPO_DIFF_SWITCHES", &(MPIR_CVAR_IREDUCE_TOPO_DIFF_SWITCHES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IREDUCE_TOPO_DIFF_SWITCHES");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IREDUCE_TOPO_DIFF_SWITCHES", &(MPIR_CVAR_IREDUCE_TOPO_DIFF_SWITCHES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IREDUCE_TOPO_DIFF_SWITCHES");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IREDUCE_TOPO_DIFF_SWITCHES = %d\n", MPIR_CVAR_IREDUCE_TOPO_DIFF_SWITCHES);
    }

    defaultval.d = 1600;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IREDUCE_TOPO_SAME_SWITCHES, /* name */
        &MPIR_CVAR_IREDUCE_TOPO_SAME_SWITCHES, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This cvar controls the latency in the same switch.");
    MPIR_CVAR_IREDUCE_TOPO_SAME_SWITCHES = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IREDUCE_TOPO_SAME_SWITCHES", &(MPIR_CVAR_IREDUCE_TOPO_SAME_SWITCHES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IREDUCE_TOPO_SAME_SWITCHES");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IREDUCE_TOPO_SAME_SWITCHES", &(MPIR_CVAR_IREDUCE_TOPO_SAME_SWITCHES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IREDUCE_TOPO_SAME_SWITCHES");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IREDUCE_TOPO_SAME_SWITCHES", &(MPIR_CVAR_IREDUCE_TOPO_SAME_SWITCHES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IREDUCE_TOPO_SAME_SWITCHES");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IREDUCE_TOPO_SAME_SWITCHES = %d\n", MPIR_CVAR_IREDUCE_TOPO_SAME_SWITCHES);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IREDUCE_TREE_PIPELINE_CHUNK_SIZE, /* name */
        &MPIR_CVAR_IREDUCE_TREE_PIPELINE_CHUNK_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Maximum chunk size (in bytes) for pipelining in tree based ireduce. Default value is 0, that is, no pipelining by default");
    MPIR_CVAR_IREDUCE_TREE_PIPELINE_CHUNK_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IREDUCE_TREE_PIPELINE_CHUNK_SIZE", &(MPIR_CVAR_IREDUCE_TREE_PIPELINE_CHUNK_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IREDUCE_TREE_PIPELINE_CHUNK_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IREDUCE_TREE_PIPELINE_CHUNK_SIZE", &(MPIR_CVAR_IREDUCE_TREE_PIPELINE_CHUNK_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IREDUCE_TREE_PIPELINE_CHUNK_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IREDUCE_TREE_PIPELINE_CHUNK_SIZE", &(MPIR_CVAR_IREDUCE_TREE_PIPELINE_CHUNK_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IREDUCE_TREE_PIPELINE_CHUNK_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IREDUCE_TREE_PIPELINE_CHUNK_SIZE = %d\n", MPIR_CVAR_IREDUCE_TREE_PIPELINE_CHUNK_SIZE);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IREDUCE_RING_CHUNK_SIZE, /* name */
        &MPIR_CVAR_IREDUCE_RING_CHUNK_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Maximum chunk size (in bytes) for pipelining in ireduce ring algorithm. Default value is 0, that is, no pipelining by default");
    MPIR_CVAR_IREDUCE_RING_CHUNK_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IREDUCE_RING_CHUNK_SIZE", &(MPIR_CVAR_IREDUCE_RING_CHUNK_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IREDUCE_RING_CHUNK_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IREDUCE_RING_CHUNK_SIZE", &(MPIR_CVAR_IREDUCE_RING_CHUNK_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IREDUCE_RING_CHUNK_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IREDUCE_RING_CHUNK_SIZE", &(MPIR_CVAR_IREDUCE_RING_CHUNK_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IREDUCE_RING_CHUNK_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IREDUCE_RING_CHUNK_SIZE = %d\n", MPIR_CVAR_IREDUCE_RING_CHUNK_SIZE);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IREDUCE_TREE_BUFFER_PER_CHILD, /* name */
        &MPIR_CVAR_IREDUCE_TREE_BUFFER_PER_CHILD, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "If set to true, a rank in tree algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.");
    MPIR_CVAR_IREDUCE_TREE_BUFFER_PER_CHILD = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_IREDUCE_TREE_BUFFER_PER_CHILD", &(MPIR_CVAR_IREDUCE_TREE_BUFFER_PER_CHILD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IREDUCE_TREE_BUFFER_PER_CHILD");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_IREDUCE_TREE_BUFFER_PER_CHILD", &(MPIR_CVAR_IREDUCE_TREE_BUFFER_PER_CHILD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IREDUCE_TREE_BUFFER_PER_CHILD");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_IREDUCE_TREE_BUFFER_PER_CHILD", &(MPIR_CVAR_IREDUCE_TREE_BUFFER_PER_CHILD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IREDUCE_TREE_BUFFER_PER_CHILD");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IREDUCE_TREE_BUFFER_PER_CHILD = %d\n", MPIR_CVAR_IREDUCE_TREE_BUFFER_PER_CHILD);
    }

    defaultval.d = MPIR_CVAR_IREDUCE_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IREDUCE_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_IREDUCE_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select ireduce algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_smp                   - Force smp algorithm\n"
"sched_binomial              - Force binomial algorithm\n"
"sched_reduce_scatter_gather - Force reduce scatter gather algorithm\n"
"tsp_tree                - Force Generic Transport Tree\n"
"tsp_ring                - Force Generic Transport Ring");
    MPIR_CVAR_IREDUCE_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IREDUCE_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IREDUCE_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IREDUCE_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IREDUCE_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IREDUCE_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IREDUCE_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IREDUCE_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IREDUCE_INTRA_ALGORITHM = MPIR_CVAR_IREDUCE_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_IREDUCE_INTRA_ALGORITHM = MPIR_CVAR_IREDUCE_INTRA_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_smp"))
            MPIR_CVAR_IREDUCE_INTRA_ALGORITHM = MPIR_CVAR_IREDUCE_INTRA_ALGORITHM_sched_smp;
        else if (0 == strcmp(tmp_str, "sched_binomial"))
            MPIR_CVAR_IREDUCE_INTRA_ALGORITHM = MPIR_CVAR_IREDUCE_INTRA_ALGORITHM_sched_binomial;
        else if (0 == strcmp(tmp_str, "sched_reduce_scatter_gather"))
            MPIR_CVAR_IREDUCE_INTRA_ALGORITHM = MPIR_CVAR_IREDUCE_INTRA_ALGORITHM_sched_reduce_scatter_gather;
        else if (0 == strcmp(tmp_str, "tsp_tree"))
            MPIR_CVAR_IREDUCE_INTRA_ALGORITHM = MPIR_CVAR_IREDUCE_INTRA_ALGORITHM_tsp_tree;
        else if (0 == strcmp(tmp_str, "tsp_ring"))
            MPIR_CVAR_IREDUCE_INTRA_ALGORITHM = MPIR_CVAR_IREDUCE_INTRA_ALGORITHM_tsp_ring;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IREDUCE_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_IREDUCE_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IREDUCE_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_IREDUCE_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select ireduce algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_local_reduce_remote_send - Force local-reduce-remote-send algorithm");
    MPIR_CVAR_IREDUCE_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IREDUCE_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IREDUCE_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IREDUCE_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IREDUCE_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IREDUCE_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IREDUCE_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IREDUCE_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IREDUCE_INTER_ALGORITHM = MPIR_CVAR_IREDUCE_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_IREDUCE_INTER_ALGORITHM = MPIR_CVAR_IREDUCE_INTER_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_local_reduce_remote_send"))
            MPIR_CVAR_IREDUCE_INTER_ALGORITHM = MPIR_CVAR_IREDUCE_INTER_ALGORITHM_sched_local_reduce_remote_send;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IREDUCE_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 2048;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE, /* name */
        &MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "the short message algorithm will be used if the send buffer size is <= this value (in bytes)");
    MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLREDUCE_SHORT_MSG_SIZE", &(MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLREDUCE_SHORT_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLREDUCE_SHORT_MSG_SIZE", &(MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLREDUCE_SHORT_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE", &(MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE = %d\n", MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE, /* name */
        &MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Maximum message size for which SMP-aware allreduce is used.  A value of '0' uses SMP-aware allreduce for all message sizes.");
    MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_MAX_SMP_ALLREDUCE_MSG_SIZE", &(MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_MAX_SMP_ALLREDUCE_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_MAX_SMP_ALLREDUCE_MSG_SIZE", &(MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_MAX_SMP_ALLREDUCE_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE", &(MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE = %d\n", MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE);
    }

    defaultval.d = MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select allreduce algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"nb                       - Force nonblocking algorithm\n"
"smp                      - Force smp algorithm\n"
"recursive_doubling       - Force recursive doubling algorithm\n"
"recursive_multiplying    - Force recursive multiplying algorithm\n"
"reduce_scatter_allgather - Force reduce scatter allgather algorithm\n"
"tree                     - Force pipelined tree algorithm\n"
"recexch                  - Force generic transport recursive exchange algorithm\n"
"ring                     - Force ring algorithm\n"
"k_reduce_scatter_allgather - Force reduce scatter allgather algorithm");
    MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_ALLREDUCE_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLREDUCE_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_ALLREDUCE_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLREDUCE_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM = MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM = MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM_nb;
        else if (0 == strcmp(tmp_str, "smp"))
            MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM = MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM_smp;
        else if (0 == strcmp(tmp_str, "recursive_doubling"))
            MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM = MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM_recursive_doubling;
        else if (0 == strcmp(tmp_str, "recursive_multiplying"))
            MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM = MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM_recursive_multiplying;
        else if (0 == strcmp(tmp_str, "reduce_scatter_allgather"))
            MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM = MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM_reduce_scatter_allgather;
        else if (0 == strcmp(tmp_str, "tree"))
            MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM = MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM_tree;
        else if (0 == strcmp(tmp_str, "recexch"))
            MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM = MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM_recexch;
        else if (0 == strcmp(tmp_str, "ring"))
            MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM = MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM_ring;
        else if (0 == strcmp(tmp_str, "k_reduce_scatter_allgather"))
            MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM = MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM_k_reduce_scatter_allgather;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLREDUCE_RECURSIVE_MULTIPLYING_KVAL, /* name */
        &MPIR_CVAR_ALLREDUCE_RECURSIVE_MULTIPLYING_KVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "radix value (power of k) for generic recursive multiplying based allreduce");
    MPIR_CVAR_ALLREDUCE_RECURSIVE_MULTIPLYING_KVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLREDUCE_RECURSIVE_MULTIPLYING_KVAL", &(MPIR_CVAR_ALLREDUCE_RECURSIVE_MULTIPLYING_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLREDUCE_RECURSIVE_MULTIPLYING_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLREDUCE_RECURSIVE_MULTIPLYING_KVAL", &(MPIR_CVAR_ALLREDUCE_RECURSIVE_MULTIPLYING_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLREDUCE_RECURSIVE_MULTIPLYING_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLREDUCE_RECURSIVE_MULTIPLYING_KVAL", &(MPIR_CVAR_ALLREDUCE_RECURSIVE_MULTIPLYING_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLREDUCE_RECURSIVE_MULTIPLYING_KVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLREDUCE_RECURSIVE_MULTIPLYING_KVAL = %d\n", MPIR_CVAR_ALLREDUCE_RECURSIVE_MULTIPLYING_KVAL);
    }

#if defined MPID_ALLREDUCE_TREE_TYPE
    defaultval.str = MPID_ALLREDUCE_TREE_TYPE;
#else
    defaultval.str = (const char *) "knomial_1";
#endif /* MPID_ALLREDUCE_TREE_TYPE */

    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_CHAR,
        MPIR_CVAR_ALLREDUCE_TREE_TYPE, /* name */
        &MPIR_CVAR_ALLREDUCE_TREE_TYPE, /* address */
        MPIR_CVAR_MAX_STRLEN, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Tree type for tree based allreduce knomial_1 is default as it supports both commutative and non-commutative reduce operations kary      - kary tree type knomial_1 - knomial_1 tree type (tree grows starting from the left of the root) knomial_2 - knomial_2 tree type (tree grows starting from the right of the root) topology_aware - topology_aware tree type topology_aware_k - topology_aware tree type with branching factor k topology_wave - topology_wave tree type");
    tmp_str = defaultval.str;
    got_rc = 0;
    rc = MPL_env2str("MPICH_ALLREDUCE_TREE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLREDUCE_TREE_TYPE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_ALLREDUCE_TREE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLREDUCE_TREE_TYPE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_ALLREDUCE_TREE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLREDUCE_TREE_TYPE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLREDUCE_TREE_TYPE = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        MPIR_CVAR_ALLREDUCE_TREE_TYPE = MPL_strdup(tmp_str);
        MPIR_CVAR_assert(MPIR_CVAR_ALLREDUCE_TREE_TYPE);
        if (MPIR_CVAR_ALLREDUCE_TREE_TYPE == NULL) {
            MPIR_CHKMEM_SETERR(mpi_errno, strlen(tmp_str), "dup of string for MPIR_CVAR_ALLREDUCE_TREE_TYPE");
            goto fn_fail;
        }
    }
    else {
        MPIR_CVAR_ALLREDUCE_TREE_TYPE = NULL;
    }

#if defined MPID_ALLREDUCE_TREE_KVAL
    defaultval.d = MPID_ALLREDUCE_TREE_KVAL;
#else
    defaultval.d = 2;
#endif /* MPID_ALLREDUCE_TREE_KVAL */

    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLREDUCE_TREE_KVAL, /* name */
        &MPIR_CVAR_ALLREDUCE_TREE_KVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Indicates the branching factor for kary or knomial trees.");
    MPIR_CVAR_ALLREDUCE_TREE_KVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLREDUCE_TREE_KVAL", &(MPIR_CVAR_ALLREDUCE_TREE_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLREDUCE_TREE_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLREDUCE_TREE_KVAL", &(MPIR_CVAR_ALLREDUCE_TREE_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLREDUCE_TREE_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLREDUCE_TREE_KVAL", &(MPIR_CVAR_ALLREDUCE_TREE_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLREDUCE_TREE_KVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLREDUCE_TREE_KVAL = %d\n", MPIR_CVAR_ALLREDUCE_TREE_KVAL);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLREDUCE_TOPO_REORDER_ENABLE, /* name */
        &MPIR_CVAR_ALLREDUCE_TOPO_REORDER_ENABLE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This cvar controls if the leaders are reordered based on the number of ranks in each group.");
    MPIR_CVAR_ALLREDUCE_TOPO_REORDER_ENABLE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_ALLREDUCE_TOPO_REORDER_ENABLE", &(MPIR_CVAR_ALLREDUCE_TOPO_REORDER_ENABLE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLREDUCE_TOPO_REORDER_ENABLE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_ALLREDUCE_TOPO_REORDER_ENABLE", &(MPIR_CVAR_ALLREDUCE_TOPO_REORDER_ENABLE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLREDUCE_TOPO_REORDER_ENABLE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_ALLREDUCE_TOPO_REORDER_ENABLE", &(MPIR_CVAR_ALLREDUCE_TOPO_REORDER_ENABLE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLREDUCE_TOPO_REORDER_ENABLE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLREDUCE_TOPO_REORDER_ENABLE = %d\n", MPIR_CVAR_ALLREDUCE_TOPO_REORDER_ENABLE);
    }

    defaultval.d = 200;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLREDUCE_TOPO_OVERHEAD, /* name */
        &MPIR_CVAR_ALLREDUCE_TOPO_OVERHEAD, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This cvar controls the size of the overhead.");
    MPIR_CVAR_ALLREDUCE_TOPO_OVERHEAD = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLREDUCE_TOPO_OVERHEAD", &(MPIR_CVAR_ALLREDUCE_TOPO_OVERHEAD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLREDUCE_TOPO_OVERHEAD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLREDUCE_TOPO_OVERHEAD", &(MPIR_CVAR_ALLREDUCE_TOPO_OVERHEAD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLREDUCE_TOPO_OVERHEAD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLREDUCE_TOPO_OVERHEAD", &(MPIR_CVAR_ALLREDUCE_TOPO_OVERHEAD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLREDUCE_TOPO_OVERHEAD");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLREDUCE_TOPO_OVERHEAD = %d\n", MPIR_CVAR_ALLREDUCE_TOPO_OVERHEAD);
    }

    defaultval.d = 2800;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLREDUCE_TOPO_DIFF_GROUPS, /* name */
        &MPIR_CVAR_ALLREDUCE_TOPO_DIFF_GROUPS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This cvar controls the latency between different groups.");
    MPIR_CVAR_ALLREDUCE_TOPO_DIFF_GROUPS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLREDUCE_TOPO_DIFF_GROUPS", &(MPIR_CVAR_ALLREDUCE_TOPO_DIFF_GROUPS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLREDUCE_TOPO_DIFF_GROUPS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLREDUCE_TOPO_DIFF_GROUPS", &(MPIR_CVAR_ALLREDUCE_TOPO_DIFF_GROUPS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLREDUCE_TOPO_DIFF_GROUPS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLREDUCE_TOPO_DIFF_GROUPS", &(MPIR_CVAR_ALLREDUCE_TOPO_DIFF_GROUPS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLREDUCE_TOPO_DIFF_GROUPS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLREDUCE_TOPO_DIFF_GROUPS = %d\n", MPIR_CVAR_ALLREDUCE_TOPO_DIFF_GROUPS);
    }

    defaultval.d = 1900;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLREDUCE_TOPO_DIFF_SWITCHES, /* name */
        &MPIR_CVAR_ALLREDUCE_TOPO_DIFF_SWITCHES, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This cvar controls the latency between different switches in the same groups.");
    MPIR_CVAR_ALLREDUCE_TOPO_DIFF_SWITCHES = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLREDUCE_TOPO_DIFF_SWITCHES", &(MPIR_CVAR_ALLREDUCE_TOPO_DIFF_SWITCHES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLREDUCE_TOPO_DIFF_SWITCHES");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLREDUCE_TOPO_DIFF_SWITCHES", &(MPIR_CVAR_ALLREDUCE_TOPO_DIFF_SWITCHES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLREDUCE_TOPO_DIFF_SWITCHES");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLREDUCE_TOPO_DIFF_SWITCHES", &(MPIR_CVAR_ALLREDUCE_TOPO_DIFF_SWITCHES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLREDUCE_TOPO_DIFF_SWITCHES");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLREDUCE_TOPO_DIFF_SWITCHES = %d\n", MPIR_CVAR_ALLREDUCE_TOPO_DIFF_SWITCHES);
    }

    defaultval.d = 1600;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLREDUCE_TOPO_SAME_SWITCHES, /* name */
        &MPIR_CVAR_ALLREDUCE_TOPO_SAME_SWITCHES, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This cvar controls the latency in the same switch.");
    MPIR_CVAR_ALLREDUCE_TOPO_SAME_SWITCHES = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLREDUCE_TOPO_SAME_SWITCHES", &(MPIR_CVAR_ALLREDUCE_TOPO_SAME_SWITCHES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLREDUCE_TOPO_SAME_SWITCHES");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLREDUCE_TOPO_SAME_SWITCHES", &(MPIR_CVAR_ALLREDUCE_TOPO_SAME_SWITCHES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLREDUCE_TOPO_SAME_SWITCHES");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLREDUCE_TOPO_SAME_SWITCHES", &(MPIR_CVAR_ALLREDUCE_TOPO_SAME_SWITCHES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLREDUCE_TOPO_SAME_SWITCHES");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLREDUCE_TOPO_SAME_SWITCHES = %d\n", MPIR_CVAR_ALLREDUCE_TOPO_SAME_SWITCHES);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLREDUCE_TREE_PIPELINE_CHUNK_SIZE, /* name */
        &MPIR_CVAR_ALLREDUCE_TREE_PIPELINE_CHUNK_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Maximum chunk size (in bytes) for pipelining in tree based allreduce. Default value is 0, that is, no pipelining by default");
    MPIR_CVAR_ALLREDUCE_TREE_PIPELINE_CHUNK_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLREDUCE_TREE_PIPELINE_CHUNK_SIZE", &(MPIR_CVAR_ALLREDUCE_TREE_PIPELINE_CHUNK_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLREDUCE_TREE_PIPELINE_CHUNK_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLREDUCE_TREE_PIPELINE_CHUNK_SIZE", &(MPIR_CVAR_ALLREDUCE_TREE_PIPELINE_CHUNK_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLREDUCE_TREE_PIPELINE_CHUNK_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLREDUCE_TREE_PIPELINE_CHUNK_SIZE", &(MPIR_CVAR_ALLREDUCE_TREE_PIPELINE_CHUNK_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLREDUCE_TREE_PIPELINE_CHUNK_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLREDUCE_TREE_PIPELINE_CHUNK_SIZE = %d\n", MPIR_CVAR_ALLREDUCE_TREE_PIPELINE_CHUNK_SIZE);
    }

#if defined MPID_ALLREDUCE_TREE_BUFFER_PER_CHILD
    defaultval.d = MPID_ALLREDUCE_TREE_BUFFER_PER_CHILD;
#else
    defaultval.d = 0;
#endif /* MPID_ALLREDUCE_TREE_BUFFER_PER_CHILD */

    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLREDUCE_TREE_BUFFER_PER_CHILD, /* name */
        &MPIR_CVAR_ALLREDUCE_TREE_BUFFER_PER_CHILD, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "If set to true, a rank in tree_kary and tree_knomial algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.");
    MPIR_CVAR_ALLREDUCE_TREE_BUFFER_PER_CHILD = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_ALLREDUCE_TREE_BUFFER_PER_CHILD", &(MPIR_CVAR_ALLREDUCE_TREE_BUFFER_PER_CHILD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLREDUCE_TREE_BUFFER_PER_CHILD");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_ALLREDUCE_TREE_BUFFER_PER_CHILD", &(MPIR_CVAR_ALLREDUCE_TREE_BUFFER_PER_CHILD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLREDUCE_TREE_BUFFER_PER_CHILD");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_ALLREDUCE_TREE_BUFFER_PER_CHILD", &(MPIR_CVAR_ALLREDUCE_TREE_BUFFER_PER_CHILD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLREDUCE_TREE_BUFFER_PER_CHILD");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLREDUCE_TREE_BUFFER_PER_CHILD = %d\n", MPIR_CVAR_ALLREDUCE_TREE_BUFFER_PER_CHILD);
    }

    defaultval.d = 2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLREDUCE_RECEXCH_KVAL, /* name */
        &MPIR_CVAR_ALLREDUCE_RECEXCH_KVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "k value for recursive exchange based allreduce");
    MPIR_CVAR_ALLREDUCE_RECEXCH_KVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLREDUCE_RECEXCH_KVAL", &(MPIR_CVAR_ALLREDUCE_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLREDUCE_RECEXCH_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLREDUCE_RECEXCH_KVAL", &(MPIR_CVAR_ALLREDUCE_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLREDUCE_RECEXCH_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLREDUCE_RECEXCH_KVAL", &(MPIR_CVAR_ALLREDUCE_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLREDUCE_RECEXCH_KVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLREDUCE_RECEXCH_KVAL = %d\n", MPIR_CVAR_ALLREDUCE_RECEXCH_KVAL);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLREDUCE_RECEXCH_SINGLE_PHASE_RECV, /* name */
        &MPIR_CVAR_ALLREDUCE_RECEXCH_SINGLE_PHASE_RECV, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR controls whether the recv is posted for one phase or two phases in recexch algos. By default, we post the recvs for 2 phases.");
    MPIR_CVAR_ALLREDUCE_RECEXCH_SINGLE_PHASE_RECV = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_ALLREDUCE_RECEXCH_SINGLE_PHASE_RECV", &(MPIR_CVAR_ALLREDUCE_RECEXCH_SINGLE_PHASE_RECV));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLREDUCE_RECEXCH_SINGLE_PHASE_RECV");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_ALLREDUCE_RECEXCH_SINGLE_PHASE_RECV", &(MPIR_CVAR_ALLREDUCE_RECEXCH_SINGLE_PHASE_RECV));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLREDUCE_RECEXCH_SINGLE_PHASE_RECV");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_ALLREDUCE_RECEXCH_SINGLE_PHASE_RECV", &(MPIR_CVAR_ALLREDUCE_RECEXCH_SINGLE_PHASE_RECV));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLREDUCE_RECEXCH_SINGLE_PHASE_RECV");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLREDUCE_RECEXCH_SINGLE_PHASE_RECV = %d\n", MPIR_CVAR_ALLREDUCE_RECEXCH_SINGLE_PHASE_RECV);
    }

    defaultval.d = MPIR_CVAR_ALLREDUCE_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLREDUCE_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_ALLREDUCE_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select allreduce algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"nb                    - Force nonblocking algorithm\n"
"reduce_exchange_bcast - Force reduce-exchange-bcast algorithm");
    MPIR_CVAR_ALLREDUCE_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_ALLREDUCE_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLREDUCE_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_ALLREDUCE_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLREDUCE_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_ALLREDUCE_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLREDUCE_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLREDUCE_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_ALLREDUCE_INTER_ALGORITHM = MPIR_CVAR_ALLREDUCE_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_ALLREDUCE_INTER_ALGORITHM = MPIR_CVAR_ALLREDUCE_INTER_ALGORITHM_nb;
        else if (0 == strcmp(tmp_str, "reduce_exchange_bcast"))
            MPIR_CVAR_ALLREDUCE_INTER_ALGORITHM = MPIR_CVAR_ALLREDUCE_INTER_ALGORITHM_reduce_exchange_bcast;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_ALLREDUCE_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLREDUCE_TREE_KVAL, /* name */
        &MPIR_CVAR_IALLREDUCE_TREE_KVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "k value for tree based iallreduce (for tree_kary and tree_knomial)");
    MPIR_CVAR_IALLREDUCE_TREE_KVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IALLREDUCE_TREE_KVAL", &(MPIR_CVAR_IALLREDUCE_TREE_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLREDUCE_TREE_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IALLREDUCE_TREE_KVAL", &(MPIR_CVAR_IALLREDUCE_TREE_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLREDUCE_TREE_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IALLREDUCE_TREE_KVAL", &(MPIR_CVAR_IALLREDUCE_TREE_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLREDUCE_TREE_KVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLREDUCE_TREE_KVAL = %d\n", MPIR_CVAR_IALLREDUCE_TREE_KVAL);
    }

    defaultval.str = (const char *) "kary";
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_CHAR,
        MPIR_CVAR_IALLREDUCE_TREE_TYPE, /* name */
        &MPIR_CVAR_IALLREDUCE_TREE_TYPE, /* address */
        MPIR_CVAR_MAX_STRLEN, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Tree type for tree based ibcast kary      - kary tree type knomial_1 - knomial_1 tree type knomial_2 - knomial_2 tree type");
    tmp_str = defaultval.str;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IALLREDUCE_TREE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLREDUCE_TREE_TYPE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IALLREDUCE_TREE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLREDUCE_TREE_TYPE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IALLREDUCE_TREE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLREDUCE_TREE_TYPE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLREDUCE_TREE_TYPE = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        MPIR_CVAR_IALLREDUCE_TREE_TYPE = MPL_strdup(tmp_str);
        MPIR_CVAR_assert(MPIR_CVAR_IALLREDUCE_TREE_TYPE);
        if (MPIR_CVAR_IALLREDUCE_TREE_TYPE == NULL) {
            MPIR_CHKMEM_SETERR(mpi_errno, strlen(tmp_str), "dup of string for MPIR_CVAR_IALLREDUCE_TREE_TYPE");
            goto fn_fail;
        }
    }
    else {
        MPIR_CVAR_IALLREDUCE_TREE_TYPE = NULL;
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLREDUCE_TREE_PIPELINE_CHUNK_SIZE, /* name */
        &MPIR_CVAR_IALLREDUCE_TREE_PIPELINE_CHUNK_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Maximum chunk size (in bytes) for pipelining in tree based iallreduce. Default value is 0, that is, no pipelining by default");
    MPIR_CVAR_IALLREDUCE_TREE_PIPELINE_CHUNK_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IALLREDUCE_TREE_PIPELINE_CHUNK_SIZE", &(MPIR_CVAR_IALLREDUCE_TREE_PIPELINE_CHUNK_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLREDUCE_TREE_PIPELINE_CHUNK_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IALLREDUCE_TREE_PIPELINE_CHUNK_SIZE", &(MPIR_CVAR_IALLREDUCE_TREE_PIPELINE_CHUNK_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLREDUCE_TREE_PIPELINE_CHUNK_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IALLREDUCE_TREE_PIPELINE_CHUNK_SIZE", &(MPIR_CVAR_IALLREDUCE_TREE_PIPELINE_CHUNK_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLREDUCE_TREE_PIPELINE_CHUNK_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLREDUCE_TREE_PIPELINE_CHUNK_SIZE = %d\n", MPIR_CVAR_IALLREDUCE_TREE_PIPELINE_CHUNK_SIZE);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLREDUCE_TREE_BUFFER_PER_CHILD, /* name */
        &MPIR_CVAR_IALLREDUCE_TREE_BUFFER_PER_CHILD, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "If set to true, a rank in tree_kary and tree_knomial algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.");
    MPIR_CVAR_IALLREDUCE_TREE_BUFFER_PER_CHILD = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_IALLREDUCE_TREE_BUFFER_PER_CHILD", &(MPIR_CVAR_IALLREDUCE_TREE_BUFFER_PER_CHILD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLREDUCE_TREE_BUFFER_PER_CHILD");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_IALLREDUCE_TREE_BUFFER_PER_CHILD", &(MPIR_CVAR_IALLREDUCE_TREE_BUFFER_PER_CHILD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLREDUCE_TREE_BUFFER_PER_CHILD");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_IALLREDUCE_TREE_BUFFER_PER_CHILD", &(MPIR_CVAR_IALLREDUCE_TREE_BUFFER_PER_CHILD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLREDUCE_TREE_BUFFER_PER_CHILD");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLREDUCE_TREE_BUFFER_PER_CHILD = %d\n", MPIR_CVAR_IALLREDUCE_TREE_BUFFER_PER_CHILD);
    }

    defaultval.d = 2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLREDUCE_RECEXCH_KVAL, /* name */
        &MPIR_CVAR_IALLREDUCE_RECEXCH_KVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "k value for recursive exchange based iallreduce");
    MPIR_CVAR_IALLREDUCE_RECEXCH_KVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IALLREDUCE_RECEXCH_KVAL", &(MPIR_CVAR_IALLREDUCE_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLREDUCE_RECEXCH_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IALLREDUCE_RECEXCH_KVAL", &(MPIR_CVAR_IALLREDUCE_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLREDUCE_RECEXCH_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IALLREDUCE_RECEXCH_KVAL", &(MPIR_CVAR_IALLREDUCE_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLREDUCE_RECEXCH_KVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLREDUCE_RECEXCH_KVAL = %d\n", MPIR_CVAR_IALLREDUCE_RECEXCH_KVAL);
    }

    defaultval.d = MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select iallreduce algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_naive                      - Force naive algorithm\n"
"sched_smp                        - Force smp algorithm\n"
"sched_recursive_doubling         - Force recursive doubling algorithm\n"
"sched_reduce_scatter_allgather   - Force reduce scatter allgather algorithm\n"
"tsp_recexch_single_buffer    - Force generic transport recursive exchange with single buffer for receives\n"
"tsp_recexch_multiple_buffer  - Force generic transport recursive exchange with multiple buffers for receives\n"
"tsp_tree                     - Force generic transport tree algorithm\n"
"tsp_ring                     - Force generic transport ring algorithm\n"
"tsp_recexch_reduce_scatter_recexch_allgatherv  - Force generic transport recursive exchange with reduce scatter and allgatherv");
    MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IALLREDUCE_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLREDUCE_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IALLREDUCE_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLREDUCE_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM = MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM = MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_naive"))
            MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM = MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM_sched_naive;
        else if (0 == strcmp(tmp_str, "sched_smp"))
            MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM = MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM_sched_smp;
        else if (0 == strcmp(tmp_str, "sched_recursive_doubling"))
            MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM = MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM_sched_recursive_doubling;
        else if (0 == strcmp(tmp_str, "sched_reduce_scatter_allgather"))
            MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM = MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM_sched_reduce_scatter_allgather;
        else if (0 == strcmp(tmp_str, "tsp_recexch_single_buffer"))
            MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM = MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM_tsp_recexch_single_buffer;
        else if (0 == strcmp(tmp_str, "tsp_recexch_multiple_buffer"))
            MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM = MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM_tsp_recexch_multiple_buffer;
        else if (0 == strcmp(tmp_str, "tsp_tree"))
            MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM = MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM_tsp_tree;
        else if (0 == strcmp(tmp_str, "tsp_ring"))
            MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM = MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM_tsp_ring;
        else if (0 == strcmp(tmp_str, "tsp_recexch_reduce_scatter_recexch_allgatherv"))
            MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM = MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM_tsp_recexch_reduce_scatter_recexch_allgatherv;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_IALLREDUCE_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLREDUCE_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_IALLREDUCE_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select iallreduce algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_remote_reduce_local_bcast - Force remote-reduce-local-bcast algorithm");
    MPIR_CVAR_IALLREDUCE_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IALLREDUCE_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLREDUCE_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IALLREDUCE_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLREDUCE_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IALLREDUCE_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLREDUCE_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLREDUCE_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IALLREDUCE_INTER_ALGORITHM = MPIR_CVAR_IALLREDUCE_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_IALLREDUCE_INTER_ALGORITHM = MPIR_CVAR_IALLREDUCE_INTER_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_remote_reduce_local_bcast"))
            MPIR_CVAR_IALLREDUCE_INTER_ALGORITHM = MPIR_CVAR_IALLREDUCE_INTER_ALGORITHM_sched_remote_reduce_local_bcast;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IALLREDUCE_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 524288;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE, /* name */
        &MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "the long message algorithm will be used if the operation is commutative and the send buffer size is >= this value (in bytes)");
    MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE", &(MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE", &(MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE", &(MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE = %d\n", MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE);
    }

    defaultval.d = MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select reduce_scatter algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"nb                 - Force nonblocking algorithm\n"
"noncommutative     - Force noncommutative algorithm\n"
"pairwise           - Force pairwise algorithm\n"
"recursive_doubling - Force recursive doubling algorithm\n"
"recursive_halving  - Force recursive halving algorithm");
    MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_REDUCE_SCATTER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_REDUCE_SCATTER_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_REDUCE_SCATTER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_REDUCE_SCATTER_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM = MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM = MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM_nb;
        else if (0 == strcmp(tmp_str, "noncommutative"))
            MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM = MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM_noncommutative;
        else if (0 == strcmp(tmp_str, "pairwise"))
            MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM = MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM_pairwise;
        else if (0 == strcmp(tmp_str, "recursive_doubling"))
            MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM = MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM_recursive_doubling;
        else if (0 == strcmp(tmp_str, "recursive_halving"))
            MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM = MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM_recursive_halving;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_REDUCE_SCATTER_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_REDUCE_SCATTER_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_REDUCE_SCATTER_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select reduce_scatter algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"nb                          - Force nonblocking algorithm\n"
"remote_reduce_local_scatter - Force remote-reduce-local-scatter algorithm");
    MPIR_CVAR_REDUCE_SCATTER_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_REDUCE_SCATTER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_REDUCE_SCATTER_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_REDUCE_SCATTER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_REDUCE_SCATTER_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_REDUCE_SCATTER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_REDUCE_SCATTER_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_REDUCE_SCATTER_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_REDUCE_SCATTER_INTER_ALGORITHM = MPIR_CVAR_REDUCE_SCATTER_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_REDUCE_SCATTER_INTER_ALGORITHM = MPIR_CVAR_REDUCE_SCATTER_INTER_ALGORITHM_nb;
        else if (0 == strcmp(tmp_str, "remote_reduce_local_scatter"))
            MPIR_CVAR_REDUCE_SCATTER_INTER_ALGORITHM = MPIR_CVAR_REDUCE_SCATTER_INTER_ALGORITHM_remote_reduce_local_scatter;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_REDUCE_SCATTER_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IREDUCE_SCATTER_RECEXCH_KVAL, /* name */
        &MPIR_CVAR_IREDUCE_SCATTER_RECEXCH_KVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "k value for recursive exchange based ireduce_scatter");
    MPIR_CVAR_IREDUCE_SCATTER_RECEXCH_KVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IREDUCE_SCATTER_RECEXCH_KVAL", &(MPIR_CVAR_IREDUCE_SCATTER_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IREDUCE_SCATTER_RECEXCH_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IREDUCE_SCATTER_RECEXCH_KVAL", &(MPIR_CVAR_IREDUCE_SCATTER_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IREDUCE_SCATTER_RECEXCH_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IREDUCE_SCATTER_RECEXCH_KVAL", &(MPIR_CVAR_IREDUCE_SCATTER_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IREDUCE_SCATTER_RECEXCH_KVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IREDUCE_SCATTER_RECEXCH_KVAL = %d\n", MPIR_CVAR_IREDUCE_SCATTER_RECEXCH_KVAL);
    }

    defaultval.d = MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select ireduce_scatter algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_noncommutative     - Force noncommutative algorithm\n"
"sched_recursive_doubling - Force recursive doubling algorithm\n"
"sched_pairwise           - Force pairwise algorithm\n"
"sched_recursive_halving  - Force recursive halving algorithm\n"
"tsp_recexch          - Force generic transport recursive exchange algorithm");
    MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IREDUCE_SCATTER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IREDUCE_SCATTER_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IREDUCE_SCATTER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IREDUCE_SCATTER_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM = MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM = MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_noncommutative"))
            MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM = MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM_sched_noncommutative;
        else if (0 == strcmp(tmp_str, "sched_recursive_doubling"))
            MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM = MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM_sched_recursive_doubling;
        else if (0 == strcmp(tmp_str, "sched_pairwise"))
            MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM = MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM_sched_pairwise;
        else if (0 == strcmp(tmp_str, "sched_recursive_halving"))
            MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM = MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM_sched_recursive_halving;
        else if (0 == strcmp(tmp_str, "tsp_recexch"))
            MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM = MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM_tsp_recexch;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_IREDUCE_SCATTER_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IREDUCE_SCATTER_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_IREDUCE_SCATTER_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select ireduce_scatter algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_remote_reduce_local_scatterv - Force remote-reduce-local-scatterv algorithm");
    MPIR_CVAR_IREDUCE_SCATTER_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IREDUCE_SCATTER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IREDUCE_SCATTER_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IREDUCE_SCATTER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IREDUCE_SCATTER_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IREDUCE_SCATTER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IREDUCE_SCATTER_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IREDUCE_SCATTER_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IREDUCE_SCATTER_INTER_ALGORITHM = MPIR_CVAR_IREDUCE_SCATTER_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_IREDUCE_SCATTER_INTER_ALGORITHM = MPIR_CVAR_IREDUCE_SCATTER_INTER_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_remote_reduce_local_scatterv"))
            MPIR_CVAR_IREDUCE_SCATTER_INTER_ALGORITHM = MPIR_CVAR_IREDUCE_SCATTER_INTER_ALGORITHM_sched_remote_reduce_local_scatterv;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IREDUCE_SCATTER_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select reduce_scatter_block algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"noncommutative     - Force noncommutative algorithm\n"
"recursive_doubling - Force recursive doubling algorithm\n"
"pairwise           - Force pairwise algorithm\n"
"recursive_halving  - Force recursive halving algorithm\n"
"nb                 - Force nonblocking algorithm");
    MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM = MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "noncommutative"))
            MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM = MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM_noncommutative;
        else if (0 == strcmp(tmp_str, "recursive_doubling"))
            MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM = MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM_recursive_doubling;
        else if (0 == strcmp(tmp_str, "pairwise"))
            MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM = MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM_pairwise;
        else if (0 == strcmp(tmp_str, "recursive_halving"))
            MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM = MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM_recursive_halving;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM = MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM_nb;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select reduce_scatter_block algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"nb                          - Force nonblocking algorithm\n"
"remote_reduce_local_scatter - Force remote-reduce-local-scatter algorithm");
    MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM = MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM = MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM_nb;
        else if (0 == strcmp(tmp_str, "remote_reduce_local_scatter"))
            MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM = MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM_remote_reduce_local_scatter;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IREDUCE_SCATTER_BLOCK_RECEXCH_KVAL, /* name */
        &MPIR_CVAR_IREDUCE_SCATTER_BLOCK_RECEXCH_KVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "k value for recursive exchange based ireduce_scatter_block");
    MPIR_CVAR_IREDUCE_SCATTER_BLOCK_RECEXCH_KVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IREDUCE_SCATTER_BLOCK_RECEXCH_KVAL", &(MPIR_CVAR_IREDUCE_SCATTER_BLOCK_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IREDUCE_SCATTER_BLOCK_RECEXCH_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IREDUCE_SCATTER_BLOCK_RECEXCH_KVAL", &(MPIR_CVAR_IREDUCE_SCATTER_BLOCK_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IREDUCE_SCATTER_BLOCK_RECEXCH_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IREDUCE_SCATTER_BLOCK_RECEXCH_KVAL", &(MPIR_CVAR_IREDUCE_SCATTER_BLOCK_RECEXCH_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IREDUCE_SCATTER_BLOCK_RECEXCH_KVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IREDUCE_SCATTER_BLOCK_RECEXCH_KVAL = %d\n", MPIR_CVAR_IREDUCE_SCATTER_BLOCK_RECEXCH_KVAL);
    }

    defaultval.d = MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select ireduce_scatter_block algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_noncommutative     - Force noncommutative algorithm\n"
"sched_recursive_doubling - Force recursive doubling algorithm\n"
"sched_pairwise           - Force pairwise algorithm\n"
"sched_recursive_halving  - Force recursive halving algorithm\n"
"tsp_recexch          - Force generic transport recursive exchange algorithm");
    MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM = MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM = MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_noncommutative"))
            MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM = MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM_sched_noncommutative;
        else if (0 == strcmp(tmp_str, "sched_recursive_doubling"))
            MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM = MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM_sched_recursive_doubling;
        else if (0 == strcmp(tmp_str, "sched_pairwise"))
            MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM = MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM_sched_pairwise;
        else if (0 == strcmp(tmp_str, "sched_recursive_halving"))
            MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM = MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM_sched_recursive_halving;
        else if (0 == strcmp(tmp_str, "tsp_recexch"))
            MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM = MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM_tsp_recexch;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select ireduce_scatter_block algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_remote_reduce_local_scatterv - Force remote-reduce-local-scatterv algorithm");
    MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM = MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM = MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_remote_reduce_local_scatterv"))
            MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM = MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM_sched_remote_reduce_local_scatterv;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_SCAN_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_SCAN_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_SCAN_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select allgather algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"nb                 - Force nonblocking algorithm\n"
"smp                - Force smp algorithm\n"
"recursive_doubling - Force recursive doubling algorithm");
    MPIR_CVAR_SCAN_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_SCAN_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_SCAN_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_SCAN_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_SCAN_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_SCAN_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_SCAN_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_SCAN_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_SCAN_INTRA_ALGORITHM = MPIR_CVAR_SCAN_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_SCAN_INTRA_ALGORITHM = MPIR_CVAR_SCAN_INTRA_ALGORITHM_nb;
        else if (0 == strcmp(tmp_str, "smp"))
            MPIR_CVAR_SCAN_INTRA_ALGORITHM = MPIR_CVAR_SCAN_INTRA_ALGORITHM_smp;
        else if (0 == strcmp(tmp_str, "recursive_doubling"))
            MPIR_CVAR_SCAN_INTRA_ALGORITHM = MPIR_CVAR_SCAN_INTRA_ALGORITHM_recursive_doubling;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_SCAN_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_ISCAN_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ISCAN_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_ISCAN_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select allgather algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_smp                  - Force smp algorithm\n"
"sched_recursive_doubling   - Force recursive doubling algorithm\n"
"tsp_recursive_doubling - Force generic transport recursive doubling algorithm");
    MPIR_CVAR_ISCAN_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_ISCAN_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ISCAN_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_ISCAN_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ISCAN_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_ISCAN_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ISCAN_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ISCAN_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_ISCAN_INTRA_ALGORITHM = MPIR_CVAR_ISCAN_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_ISCAN_INTRA_ALGORITHM = MPIR_CVAR_ISCAN_INTRA_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_smp"))
            MPIR_CVAR_ISCAN_INTRA_ALGORITHM = MPIR_CVAR_ISCAN_INTRA_ALGORITHM_sched_smp;
        else if (0 == strcmp(tmp_str, "sched_recursive_doubling"))
            MPIR_CVAR_ISCAN_INTRA_ALGORITHM = MPIR_CVAR_ISCAN_INTRA_ALGORITHM_sched_recursive_doubling;
        else if (0 == strcmp(tmp_str, "tsp_recursive_doubling"))
            MPIR_CVAR_ISCAN_INTRA_ALGORITHM = MPIR_CVAR_ISCAN_INTRA_ALGORITHM_tsp_recursive_doubling;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_ISCAN_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_EXSCAN_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_EXSCAN_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_EXSCAN_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select allgather algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"nb                 - Force nonblocking algorithm\n"
"recursive_doubling - Force recursive doubling algorithm");
    MPIR_CVAR_EXSCAN_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_EXSCAN_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_EXSCAN_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_EXSCAN_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_EXSCAN_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_EXSCAN_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_EXSCAN_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_EXSCAN_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_EXSCAN_INTRA_ALGORITHM = MPIR_CVAR_EXSCAN_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_EXSCAN_INTRA_ALGORITHM = MPIR_CVAR_EXSCAN_INTRA_ALGORITHM_nb;
        else if (0 == strcmp(tmp_str, "recursive_doubling"))
            MPIR_CVAR_EXSCAN_INTRA_ALGORITHM = MPIR_CVAR_EXSCAN_INTRA_ALGORITHM_recursive_doubling;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_EXSCAN_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_IEXSCAN_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IEXSCAN_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_IEXSCAN_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select iexscan algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_recursive_doubling - Force recursive doubling algorithm");
    MPIR_CVAR_IEXSCAN_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IEXSCAN_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IEXSCAN_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IEXSCAN_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IEXSCAN_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IEXSCAN_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IEXSCAN_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IEXSCAN_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IEXSCAN_INTRA_ALGORITHM = MPIR_CVAR_IEXSCAN_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_IEXSCAN_INTRA_ALGORITHM = MPIR_CVAR_IEXSCAN_INTRA_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_recursive_doubling"))
            MPIR_CVAR_IEXSCAN_INTRA_ALGORITHM = MPIR_CVAR_IEXSCAN_INTRA_ALGORITHM_sched_recursive_doubling;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IEXSCAN_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select ineighbor_allgather algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"nb   - Force nonblocking algorithm");
    MPIR_CVAR_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM = MPIR_CVAR_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM = MPIR_CVAR_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM_nb;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_NEIGHBOR_ALLGATHER_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_NEIGHBOR_ALLGATHER_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_NEIGHBOR_ALLGATHER_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select ineighbor_allgather algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"nb   - Force nonblocking algorithm");
    MPIR_CVAR_NEIGHBOR_ALLGATHER_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_NEIGHBOR_ALLGATHER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NEIGHBOR_ALLGATHER_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_NEIGHBOR_ALLGATHER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NEIGHBOR_ALLGATHER_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_NEIGHBOR_ALLGATHER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NEIGHBOR_ALLGATHER_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NEIGHBOR_ALLGATHER_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_NEIGHBOR_ALLGATHER_INTER_ALGORITHM = MPIR_CVAR_NEIGHBOR_ALLGATHER_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_NEIGHBOR_ALLGATHER_INTER_ALGORITHM = MPIR_CVAR_NEIGHBOR_ALLGATHER_INTER_ALGORITHM_nb;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_NEIGHBOR_ALLGATHER_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select ineighbor_allgather algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_linear    - Force linear algorithm\n"
"tsp_linear  - Force generic transport based linear algorithm");
    MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_linear"))
            MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM_sched_linear;
        else if (0 == strcmp(tmp_str, "tsp_linear"))
            MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM_tsp_linear;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select ineighbor_allgather algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_linear    - Force linear algorithm\n"
"tsp_linear  - Force generic transport based linear algorithm");
    MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_INEIGHBOR_ALLGATHER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_INEIGHBOR_ALLGATHER_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_INEIGHBOR_ALLGATHER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_INEIGHBOR_ALLGATHER_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_linear"))
            MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM_sched_linear;
        else if (0 == strcmp(tmp_str, "tsp_linear"))
            MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM_tsp_linear;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select neighbor_allgatherv algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"nb   - Force nb algorithm");
    MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM = MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM = MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM_nb;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select neighbor_allgatherv algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"nb   - Force nb algorithm");
    MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM = MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM = MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM_nb;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select ineighbor_allgatherv algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_linear          - Force linear algorithm\n"
"tsp_linear        - Force generic transport based linear algorithm");
    MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_linear"))
            MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM_sched_linear;
        else if (0 == strcmp(tmp_str, "tsp_linear"))
            MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM_tsp_linear;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select ineighbor_allgatherv algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_linear          - Force linear algorithm\n"
"tsp_linear        - Force generic transport based linear algorithm");
    MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_linear"))
            MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM_sched_linear;
        else if (0 == strcmp(tmp_str, "tsp_linear"))
            MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM_tsp_linear;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select neighbor_alltoall algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"nb   - Force nb algorithm");
    MPIR_CVAR_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM = MPIR_CVAR_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM = MPIR_CVAR_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM_nb;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_NEIGHBOR_ALLTOALL_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_NEIGHBOR_ALLTOALL_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_NEIGHBOR_ALLTOALL_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select neighbor_alltoall algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"nb   - Force nb algorithm");
    MPIR_CVAR_NEIGHBOR_ALLTOALL_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_NEIGHBOR_ALLTOALL_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NEIGHBOR_ALLTOALL_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_NEIGHBOR_ALLTOALL_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NEIGHBOR_ALLTOALL_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_NEIGHBOR_ALLTOALL_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NEIGHBOR_ALLTOALL_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NEIGHBOR_ALLTOALL_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_NEIGHBOR_ALLTOALL_INTER_ALGORITHM = MPIR_CVAR_NEIGHBOR_ALLTOALL_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_NEIGHBOR_ALLTOALL_INTER_ALGORITHM = MPIR_CVAR_NEIGHBOR_ALLTOALL_INTER_ALGORITHM_nb;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_NEIGHBOR_ALLTOALL_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select ineighbor_alltoall algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_linear          - Force linear algorithm\n"
"tsp_linear        - Force generic transport based linear algorithm");
    MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_linear"))
            MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM_sched_linear;
        else if (0 == strcmp(tmp_str, "tsp_linear"))
            MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM_tsp_linear;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select ineighbor_alltoall algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_linear          - Force linear algorithm\n"
"tsp_linear        - Force generic transport based linear algorithm");
    MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_INEIGHBOR_ALLTOALL_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_INEIGHBOR_ALLTOALL_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_INEIGHBOR_ALLTOALL_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_INEIGHBOR_ALLTOALL_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_linear"))
            MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM_sched_linear;
        else if (0 == strcmp(tmp_str, "tsp_linear"))
            MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM_tsp_linear;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select neighbor_alltoallv algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"nb   - Force nb algorithm");
    MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM = MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM = MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM_nb;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select neighbor_alltoallv algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"nb   - Force nb algorithm");
    MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM = MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM = MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM_nb;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select ineighbor_alltoallv algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_linear          - Force linear algorithm\n"
"tsp_linear  - Force generic transport based linear algorithm");
    MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_linear"))
            MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM_sched_linear;
        else if (0 == strcmp(tmp_str, "tsp_linear"))
            MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM_tsp_linear;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select ineighbor_alltoallv algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_linear          - Force linear algorithm\n"
"tsp_linear  - Force generic transport based linear algorithm");
    MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_linear"))
            MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM_sched_linear;
        else if (0 == strcmp(tmp_str, "tsp_linear"))
            MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM_tsp_linear;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select neighbor_alltoallw algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"nb   - Force nb algorithm");
    MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM = MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM = MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM_nb;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select neighbor_alltoallw algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"nb   - Force nb algorithm");
    MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM = MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "nb"))
            MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM = MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM_nb;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select ineighbor_alltoallw algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_linear          - Force linear algorithm\n"
"tsp_linear        - Force generic transport based linear algorithm");
    MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_linear"))
            MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM_sched_linear;
        else if (0 == strcmp(tmp_str, "tsp_linear"))
            MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM_tsp_linear;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM, /* name */
        &MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select ineighbor_alltoallw algorithm\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE)\n"
"sched_auto - Internal algorithm selection for sched-based algorithms\n"
"sched_linear          - Force linear algorithm\n"
"tsp_linear        - Force generic transport based linear algorithm");
    MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "sched_auto"))
            MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM_sched_auto;
        else if (0 == strcmp(tmp_str, "sched_linear"))
            MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM_sched_linear;
        else if (0 == strcmp(tmp_str, "tsp_linear"))
            MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM = MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM_tsp_linear;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BARRIER_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_BARRIER_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Barrier will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_BARRIER_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_BARRIER_DEVICE_COLLECTIVE", &(MPIR_CVAR_BARRIER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BARRIER_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_BARRIER_DEVICE_COLLECTIVE", &(MPIR_CVAR_BARRIER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BARRIER_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_BARRIER_DEVICE_COLLECTIVE", &(MPIR_CVAR_BARRIER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BARRIER_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BARRIER_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_BARRIER_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IBARRIER_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_IBARRIER_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Ibarrier will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_IBARRIER_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_IBARRIER_DEVICE_COLLECTIVE", &(MPIR_CVAR_IBARRIER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IBARRIER_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_IBARRIER_DEVICE_COLLECTIVE", &(MPIR_CVAR_IBARRIER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IBARRIER_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_IBARRIER_DEVICE_COLLECTIVE", &(MPIR_CVAR_IBARRIER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IBARRIER_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IBARRIER_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_IBARRIER_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BARRIER_INIT_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_BARRIER_INIT_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Barrier will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_BARRIER_INIT_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_BARRIER_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_BARRIER_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BARRIER_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_BARRIER_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_BARRIER_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BARRIER_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_BARRIER_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_BARRIER_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BARRIER_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BARRIER_INIT_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_BARRIER_INIT_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BCAST_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_BCAST_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Bcast will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_BCAST_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_BCAST_DEVICE_COLLECTIVE", &(MPIR_CVAR_BCAST_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BCAST_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_BCAST_DEVICE_COLLECTIVE", &(MPIR_CVAR_BCAST_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BCAST_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_BCAST_DEVICE_COLLECTIVE", &(MPIR_CVAR_BCAST_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BCAST_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BCAST_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_BCAST_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IBCAST_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_IBCAST_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Ibcast will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_IBCAST_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_IBCAST_DEVICE_COLLECTIVE", &(MPIR_CVAR_IBCAST_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IBCAST_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_IBCAST_DEVICE_COLLECTIVE", &(MPIR_CVAR_IBCAST_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IBCAST_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_IBCAST_DEVICE_COLLECTIVE", &(MPIR_CVAR_IBCAST_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IBCAST_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IBCAST_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_IBCAST_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BCAST_INIT_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_BCAST_INIT_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Bcast_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_BCAST_INIT_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_BCAST_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_BCAST_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BCAST_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_BCAST_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_BCAST_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BCAST_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_BCAST_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_BCAST_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BCAST_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BCAST_INIT_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_BCAST_INIT_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_GATHER_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_GATHER_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Gather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_GATHER_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_GATHER_DEVICE_COLLECTIVE", &(MPIR_CVAR_GATHER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_GATHER_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_GATHER_DEVICE_COLLECTIVE", &(MPIR_CVAR_GATHER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_GATHER_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_GATHER_DEVICE_COLLECTIVE", &(MPIR_CVAR_GATHER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_GATHER_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_GATHER_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_GATHER_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IGATHER_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_IGATHER_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Igather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_IGATHER_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_IGATHER_DEVICE_COLLECTIVE", &(MPIR_CVAR_IGATHER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IGATHER_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_IGATHER_DEVICE_COLLECTIVE", &(MPIR_CVAR_IGATHER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IGATHER_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_IGATHER_DEVICE_COLLECTIVE", &(MPIR_CVAR_IGATHER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IGATHER_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IGATHER_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_IGATHER_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_GATHER_INIT_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_GATHER_INIT_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Gather_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_GATHER_INIT_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_GATHER_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_GATHER_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_GATHER_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_GATHER_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_GATHER_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_GATHER_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_GATHER_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_GATHER_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_GATHER_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_GATHER_INIT_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_GATHER_INIT_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_GATHERV_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_GATHERV_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Gatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_GATHERV_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_GATHERV_DEVICE_COLLECTIVE", &(MPIR_CVAR_GATHERV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_GATHERV_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_GATHERV_DEVICE_COLLECTIVE", &(MPIR_CVAR_GATHERV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_GATHERV_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_GATHERV_DEVICE_COLLECTIVE", &(MPIR_CVAR_GATHERV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_GATHERV_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_GATHERV_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_GATHERV_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IGATHERV_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_IGATHERV_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Igatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_IGATHERV_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_IGATHERV_DEVICE_COLLECTIVE", &(MPIR_CVAR_IGATHERV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IGATHERV_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_IGATHERV_DEVICE_COLLECTIVE", &(MPIR_CVAR_IGATHERV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IGATHERV_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_IGATHERV_DEVICE_COLLECTIVE", &(MPIR_CVAR_IGATHERV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IGATHERV_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IGATHERV_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_IGATHERV_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_GATHERV_INIT_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_GATHERV_INIT_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Gatherv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_GATHERV_INIT_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_GATHERV_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_GATHERV_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_GATHERV_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_GATHERV_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_GATHERV_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_GATHERV_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_GATHERV_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_GATHERV_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_GATHERV_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_GATHERV_INIT_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_GATHERV_INIT_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_SCATTER_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_SCATTER_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Scatter will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_SCATTER_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_SCATTER_DEVICE_COLLECTIVE", &(MPIR_CVAR_SCATTER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_SCATTER_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_SCATTER_DEVICE_COLLECTIVE", &(MPIR_CVAR_SCATTER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_SCATTER_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_SCATTER_DEVICE_COLLECTIVE", &(MPIR_CVAR_SCATTER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_SCATTER_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_SCATTER_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_SCATTER_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ISCATTER_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_ISCATTER_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Iscatter will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_ISCATTER_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_ISCATTER_DEVICE_COLLECTIVE", &(MPIR_CVAR_ISCATTER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ISCATTER_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_ISCATTER_DEVICE_COLLECTIVE", &(MPIR_CVAR_ISCATTER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ISCATTER_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_ISCATTER_DEVICE_COLLECTIVE", &(MPIR_CVAR_ISCATTER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ISCATTER_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ISCATTER_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_ISCATTER_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_SCATTER_INIT_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_SCATTER_INIT_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Scatter_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_SCATTER_INIT_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_SCATTER_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_SCATTER_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_SCATTER_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_SCATTER_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_SCATTER_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_SCATTER_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_SCATTER_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_SCATTER_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_SCATTER_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_SCATTER_INIT_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_SCATTER_INIT_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_SCATTERV_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_SCATTERV_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Scatterv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_SCATTERV_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_SCATTERV_DEVICE_COLLECTIVE", &(MPIR_CVAR_SCATTERV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_SCATTERV_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_SCATTERV_DEVICE_COLLECTIVE", &(MPIR_CVAR_SCATTERV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_SCATTERV_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_SCATTERV_DEVICE_COLLECTIVE", &(MPIR_CVAR_SCATTERV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_SCATTERV_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_SCATTERV_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_SCATTERV_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ISCATTERV_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_ISCATTERV_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Iscatterv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_ISCATTERV_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_ISCATTERV_DEVICE_COLLECTIVE", &(MPIR_CVAR_ISCATTERV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ISCATTERV_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_ISCATTERV_DEVICE_COLLECTIVE", &(MPIR_CVAR_ISCATTERV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ISCATTERV_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_ISCATTERV_DEVICE_COLLECTIVE", &(MPIR_CVAR_ISCATTERV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ISCATTERV_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ISCATTERV_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_ISCATTERV_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_SCATTERV_INIT_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_SCATTERV_INIT_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Scatterv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_SCATTERV_INIT_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_SCATTERV_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_SCATTERV_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_SCATTERV_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_SCATTERV_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_SCATTERV_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_SCATTERV_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_SCATTERV_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_SCATTERV_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_SCATTERV_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_SCATTERV_INIT_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_SCATTERV_INIT_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLGATHER_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_ALLGATHER_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Allgather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_ALLGATHER_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_ALLGATHER_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLGATHER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLGATHER_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_ALLGATHER_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLGATHER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLGATHER_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_ALLGATHER_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLGATHER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLGATHER_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLGATHER_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_ALLGATHER_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLGATHER_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_IALLGATHER_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Iallgather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_IALLGATHER_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_IALLGATHER_DEVICE_COLLECTIVE", &(MPIR_CVAR_IALLGATHER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLGATHER_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_IALLGATHER_DEVICE_COLLECTIVE", &(MPIR_CVAR_IALLGATHER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLGATHER_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_IALLGATHER_DEVICE_COLLECTIVE", &(MPIR_CVAR_IALLGATHER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLGATHER_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLGATHER_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_IALLGATHER_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLGATHER_INIT_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_ALLGATHER_INIT_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Allgather_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_ALLGATHER_INIT_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_ALLGATHER_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLGATHER_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLGATHER_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_ALLGATHER_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLGATHER_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLGATHER_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_ALLGATHER_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLGATHER_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLGATHER_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLGATHER_INIT_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_ALLGATHER_INIT_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLGATHERV_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_ALLGATHERV_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Allgatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_ALLGATHERV_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_ALLGATHERV_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLGATHERV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLGATHERV_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_ALLGATHERV_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLGATHERV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLGATHERV_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_ALLGATHERV_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLGATHERV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLGATHERV_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLGATHERV_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_ALLGATHERV_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLGATHERV_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_IALLGATHERV_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Iallgatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_IALLGATHERV_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_IALLGATHERV_DEVICE_COLLECTIVE", &(MPIR_CVAR_IALLGATHERV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLGATHERV_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_IALLGATHERV_DEVICE_COLLECTIVE", &(MPIR_CVAR_IALLGATHERV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLGATHERV_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_IALLGATHERV_DEVICE_COLLECTIVE", &(MPIR_CVAR_IALLGATHERV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLGATHERV_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLGATHERV_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_IALLGATHERV_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLGATHERV_INIT_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_ALLGATHERV_INIT_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Allgatherv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_ALLGATHERV_INIT_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_ALLGATHERV_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLGATHERV_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLGATHERV_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_ALLGATHERV_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLGATHERV_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLGATHERV_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_ALLGATHERV_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLGATHERV_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLGATHERV_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLGATHERV_INIT_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_ALLGATHERV_INIT_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLTOALL_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_ALLTOALL_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Alltoall will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_ALLTOALL_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_ALLTOALL_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLTOALL_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLTOALL_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_ALLTOALL_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLTOALL_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLTOALL_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_ALLTOALL_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLTOALL_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLTOALL_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLTOALL_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_ALLTOALL_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLTOALL_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_IALLTOALL_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Ialltoall will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_IALLTOALL_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_IALLTOALL_DEVICE_COLLECTIVE", &(MPIR_CVAR_IALLTOALL_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLTOALL_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_IALLTOALL_DEVICE_COLLECTIVE", &(MPIR_CVAR_IALLTOALL_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLTOALL_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_IALLTOALL_DEVICE_COLLECTIVE", &(MPIR_CVAR_IALLTOALL_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLTOALL_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLTOALL_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_IALLTOALL_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLTOALL_INIT_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_ALLTOALL_INIT_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Alltoall_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_ALLTOALL_INIT_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_ALLTOALL_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLTOALL_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLTOALL_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_ALLTOALL_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLTOALL_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLTOALL_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_ALLTOALL_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLTOALL_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLTOALL_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLTOALL_INIT_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_ALLTOALL_INIT_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLTOALLV_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_ALLTOALLV_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Alltoallv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_ALLTOALLV_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_ALLTOALLV_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLTOALLV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLTOALLV_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_ALLTOALLV_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLTOALLV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLTOALLV_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_ALLTOALLV_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLTOALLV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLTOALLV_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLTOALLV_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_ALLTOALLV_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLTOALLV_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_IALLTOALLV_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Ialltoallv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_IALLTOALLV_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_IALLTOALLV_DEVICE_COLLECTIVE", &(MPIR_CVAR_IALLTOALLV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLTOALLV_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_IALLTOALLV_DEVICE_COLLECTIVE", &(MPIR_CVAR_IALLTOALLV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLTOALLV_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_IALLTOALLV_DEVICE_COLLECTIVE", &(MPIR_CVAR_IALLTOALLV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLTOALLV_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLTOALLV_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_IALLTOALLV_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLTOALLV_INIT_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_ALLTOALLV_INIT_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Alltoallv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_ALLTOALLV_INIT_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_ALLTOALLV_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLTOALLV_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLTOALLV_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_ALLTOALLV_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLTOALLV_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLTOALLV_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_ALLTOALLV_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLTOALLV_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLTOALLV_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLTOALLV_INIT_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_ALLTOALLV_INIT_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLTOALLW_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_ALLTOALLW_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Alltoallw will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_ALLTOALLW_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_ALLTOALLW_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLTOALLW_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLTOALLW_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_ALLTOALLW_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLTOALLW_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLTOALLW_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_ALLTOALLW_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLTOALLW_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLTOALLW_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLTOALLW_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_ALLTOALLW_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLTOALLW_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_IALLTOALLW_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Ialltoallw will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_IALLTOALLW_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_IALLTOALLW_DEVICE_COLLECTIVE", &(MPIR_CVAR_IALLTOALLW_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLTOALLW_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_IALLTOALLW_DEVICE_COLLECTIVE", &(MPIR_CVAR_IALLTOALLW_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLTOALLW_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_IALLTOALLW_DEVICE_COLLECTIVE", &(MPIR_CVAR_IALLTOALLW_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLTOALLW_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLTOALLW_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_IALLTOALLW_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLTOALLW_INIT_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_ALLTOALLW_INIT_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Alltoallw_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_ALLTOALLW_INIT_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_ALLTOALLW_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLTOALLW_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLTOALLW_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_ALLTOALLW_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLTOALLW_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLTOALLW_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_ALLTOALLW_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLTOALLW_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLTOALLW_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLTOALLW_INIT_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_ALLTOALLW_INIT_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_REDUCE_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_REDUCE_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Reduce will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_REDUCE_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_REDUCE_DEVICE_COLLECTIVE", &(MPIR_CVAR_REDUCE_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_REDUCE_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_REDUCE_DEVICE_COLLECTIVE", &(MPIR_CVAR_REDUCE_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_REDUCE_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_REDUCE_DEVICE_COLLECTIVE", &(MPIR_CVAR_REDUCE_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_REDUCE_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_REDUCE_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_REDUCE_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IREDUCE_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_IREDUCE_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Ireduce will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_IREDUCE_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_IREDUCE_DEVICE_COLLECTIVE", &(MPIR_CVAR_IREDUCE_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IREDUCE_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_IREDUCE_DEVICE_COLLECTIVE", &(MPIR_CVAR_IREDUCE_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IREDUCE_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_IREDUCE_DEVICE_COLLECTIVE", &(MPIR_CVAR_IREDUCE_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IREDUCE_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IREDUCE_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_IREDUCE_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_REDUCE_INIT_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_REDUCE_INIT_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Reduce_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_REDUCE_INIT_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_REDUCE_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_REDUCE_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_REDUCE_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_REDUCE_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_REDUCE_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_REDUCE_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_REDUCE_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_REDUCE_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_REDUCE_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_REDUCE_INIT_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_REDUCE_INIT_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLREDUCE_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_ALLREDUCE_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Allreduce will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_ALLREDUCE_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_ALLREDUCE_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLREDUCE_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLREDUCE_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_ALLREDUCE_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLREDUCE_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLREDUCE_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_ALLREDUCE_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLREDUCE_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLREDUCE_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLREDUCE_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_ALLREDUCE_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLREDUCE_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_IALLREDUCE_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Iallreduce will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_IALLREDUCE_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_IALLREDUCE_DEVICE_COLLECTIVE", &(MPIR_CVAR_IALLREDUCE_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLREDUCE_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_IALLREDUCE_DEVICE_COLLECTIVE", &(MPIR_CVAR_IALLREDUCE_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLREDUCE_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_IALLREDUCE_DEVICE_COLLECTIVE", &(MPIR_CVAR_IALLREDUCE_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLREDUCE_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLREDUCE_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_IALLREDUCE_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLREDUCE_INIT_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_ALLREDUCE_INIT_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Allreduce_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_ALLREDUCE_INIT_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_ALLREDUCE_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLREDUCE_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLREDUCE_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_ALLREDUCE_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLREDUCE_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLREDUCE_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_ALLREDUCE_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_ALLREDUCE_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLREDUCE_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLREDUCE_INIT_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_ALLREDUCE_INIT_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_REDUCE_SCATTER_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_REDUCE_SCATTER_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Reduce_scatter will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_REDUCE_SCATTER_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_REDUCE_SCATTER_DEVICE_COLLECTIVE", &(MPIR_CVAR_REDUCE_SCATTER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_REDUCE_SCATTER_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_REDUCE_SCATTER_DEVICE_COLLECTIVE", &(MPIR_CVAR_REDUCE_SCATTER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_REDUCE_SCATTER_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_REDUCE_SCATTER_DEVICE_COLLECTIVE", &(MPIR_CVAR_REDUCE_SCATTER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_REDUCE_SCATTER_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_REDUCE_SCATTER_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_REDUCE_SCATTER_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IREDUCE_SCATTER_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_IREDUCE_SCATTER_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Ireduce_scatter will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_IREDUCE_SCATTER_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_IREDUCE_SCATTER_DEVICE_COLLECTIVE", &(MPIR_CVAR_IREDUCE_SCATTER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IREDUCE_SCATTER_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_IREDUCE_SCATTER_DEVICE_COLLECTIVE", &(MPIR_CVAR_IREDUCE_SCATTER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IREDUCE_SCATTER_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_IREDUCE_SCATTER_DEVICE_COLLECTIVE", &(MPIR_CVAR_IREDUCE_SCATTER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IREDUCE_SCATTER_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IREDUCE_SCATTER_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_IREDUCE_SCATTER_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_REDUCE_SCATTER_INIT_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_REDUCE_SCATTER_INIT_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Reduce_scatter_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_REDUCE_SCATTER_INIT_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_REDUCE_SCATTER_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_REDUCE_SCATTER_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_REDUCE_SCATTER_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_REDUCE_SCATTER_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_REDUCE_SCATTER_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_REDUCE_SCATTER_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_REDUCE_SCATTER_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_REDUCE_SCATTER_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_REDUCE_SCATTER_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_REDUCE_SCATTER_INIT_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_REDUCE_SCATTER_INIT_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_REDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_REDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Reduce_scatter_block will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_REDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_REDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE", &(MPIR_CVAR_REDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_REDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_REDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE", &(MPIR_CVAR_REDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_REDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_REDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE", &(MPIR_CVAR_REDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_REDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_REDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_REDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IREDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_IREDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Ireduce_scatter_block will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_IREDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_IREDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE", &(MPIR_CVAR_IREDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IREDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_IREDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE", &(MPIR_CVAR_IREDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IREDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_IREDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE", &(MPIR_CVAR_IREDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IREDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IREDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_IREDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_REDUCE_SCATTER_BLOCK_INIT_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_REDUCE_SCATTER_BLOCK_INIT_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Reduce_scatter_block_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_REDUCE_SCATTER_BLOCK_INIT_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_REDUCE_SCATTER_BLOCK_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_REDUCE_SCATTER_BLOCK_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_REDUCE_SCATTER_BLOCK_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_REDUCE_SCATTER_BLOCK_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_REDUCE_SCATTER_BLOCK_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_REDUCE_SCATTER_BLOCK_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_REDUCE_SCATTER_BLOCK_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_REDUCE_SCATTER_BLOCK_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_REDUCE_SCATTER_BLOCK_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_REDUCE_SCATTER_BLOCK_INIT_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_REDUCE_SCATTER_BLOCK_INIT_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_SCAN_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_SCAN_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Scan will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_SCAN_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_SCAN_DEVICE_COLLECTIVE", &(MPIR_CVAR_SCAN_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_SCAN_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_SCAN_DEVICE_COLLECTIVE", &(MPIR_CVAR_SCAN_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_SCAN_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_SCAN_DEVICE_COLLECTIVE", &(MPIR_CVAR_SCAN_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_SCAN_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_SCAN_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_SCAN_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ISCAN_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_ISCAN_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Iscan will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_ISCAN_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_ISCAN_DEVICE_COLLECTIVE", &(MPIR_CVAR_ISCAN_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ISCAN_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_ISCAN_DEVICE_COLLECTIVE", &(MPIR_CVAR_ISCAN_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ISCAN_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_ISCAN_DEVICE_COLLECTIVE", &(MPIR_CVAR_ISCAN_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ISCAN_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ISCAN_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_ISCAN_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_SCAN_INIT_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_SCAN_INIT_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Scan_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_SCAN_INIT_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_SCAN_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_SCAN_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_SCAN_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_SCAN_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_SCAN_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_SCAN_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_SCAN_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_SCAN_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_SCAN_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_SCAN_INIT_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_SCAN_INIT_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_EXSCAN_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_EXSCAN_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Exscan will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_EXSCAN_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_EXSCAN_DEVICE_COLLECTIVE", &(MPIR_CVAR_EXSCAN_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_EXSCAN_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_EXSCAN_DEVICE_COLLECTIVE", &(MPIR_CVAR_EXSCAN_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_EXSCAN_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_EXSCAN_DEVICE_COLLECTIVE", &(MPIR_CVAR_EXSCAN_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_EXSCAN_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_EXSCAN_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_EXSCAN_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IEXSCAN_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_IEXSCAN_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Iexscan will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_IEXSCAN_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_IEXSCAN_DEVICE_COLLECTIVE", &(MPIR_CVAR_IEXSCAN_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IEXSCAN_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_IEXSCAN_DEVICE_COLLECTIVE", &(MPIR_CVAR_IEXSCAN_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IEXSCAN_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_IEXSCAN_DEVICE_COLLECTIVE", &(MPIR_CVAR_IEXSCAN_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IEXSCAN_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IEXSCAN_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_IEXSCAN_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_EXSCAN_INIT_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_EXSCAN_INIT_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Exscan_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_EXSCAN_INIT_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_EXSCAN_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_EXSCAN_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_EXSCAN_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_EXSCAN_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_EXSCAN_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_EXSCAN_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_EXSCAN_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_EXSCAN_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_EXSCAN_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_EXSCAN_INIT_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_EXSCAN_INIT_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_NEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_NEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Neighbor_allgather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_NEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_NEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_NEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_NEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_NEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_INEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_INEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Ineighbor_allgather will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_INEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_INEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE", &(MPIR_CVAR_INEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_INEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_INEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE", &(MPIR_CVAR_INEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_INEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_INEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE", &(MPIR_CVAR_INEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_INEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_INEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_INEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_NEIGHBOR_ALLGATHER_INIT_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_NEIGHBOR_ALLGATHER_INIT_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Neighbor_allgather_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_NEIGHBOR_ALLGATHER_INIT_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_NEIGHBOR_ALLGATHER_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLGATHER_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NEIGHBOR_ALLGATHER_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_NEIGHBOR_ALLGATHER_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLGATHER_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NEIGHBOR_ALLGATHER_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_NEIGHBOR_ALLGATHER_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLGATHER_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NEIGHBOR_ALLGATHER_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NEIGHBOR_ALLGATHER_INIT_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_NEIGHBOR_ALLGATHER_INIT_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_NEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_NEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Neighbor_allgatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_NEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_NEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_NEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_NEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_NEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_INEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_INEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Ineighbor_allgatherv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_INEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_INEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE", &(MPIR_CVAR_INEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_INEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_INEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE", &(MPIR_CVAR_INEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_INEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_INEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE", &(MPIR_CVAR_INEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_INEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_INEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_INEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_NEIGHBOR_ALLGATHERV_INIT_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_NEIGHBOR_ALLGATHERV_INIT_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Neighbor_allgatherv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_NEIGHBOR_ALLGATHERV_INIT_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_NEIGHBOR_ALLGATHERV_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLGATHERV_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NEIGHBOR_ALLGATHERV_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_NEIGHBOR_ALLGATHERV_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLGATHERV_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NEIGHBOR_ALLGATHERV_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_NEIGHBOR_ALLGATHERV_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLGATHERV_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NEIGHBOR_ALLGATHERV_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NEIGHBOR_ALLGATHERV_INIT_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_NEIGHBOR_ALLGATHERV_INIT_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_NEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_NEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Neighbor_alltoall will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_NEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_NEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_NEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_NEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_NEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_INEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_INEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Ineighbor_alltoall will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_INEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_INEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE", &(MPIR_CVAR_INEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_INEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_INEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE", &(MPIR_CVAR_INEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_INEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_INEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE", &(MPIR_CVAR_INEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_INEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_INEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_INEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_NEIGHBOR_ALLTOALL_INIT_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_NEIGHBOR_ALLTOALL_INIT_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Neighbor_alltoall_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_NEIGHBOR_ALLTOALL_INIT_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_NEIGHBOR_ALLTOALL_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLTOALL_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NEIGHBOR_ALLTOALL_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_NEIGHBOR_ALLTOALL_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLTOALL_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NEIGHBOR_ALLTOALL_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_NEIGHBOR_ALLTOALL_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLTOALL_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NEIGHBOR_ALLTOALL_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NEIGHBOR_ALLTOALL_INIT_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_NEIGHBOR_ALLTOALL_INIT_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_NEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_NEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Neighbor_alltoallv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_NEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_NEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_NEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_NEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_NEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_INEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_INEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Ineighbor_alltoallv will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_INEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_INEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE", &(MPIR_CVAR_INEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_INEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_INEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE", &(MPIR_CVAR_INEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_INEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_INEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE", &(MPIR_CVAR_INEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_INEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_INEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_INEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_NEIGHBOR_ALLTOALLV_INIT_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_NEIGHBOR_ALLTOALLV_INIT_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Neighbor_alltoallv_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_NEIGHBOR_ALLTOALLV_INIT_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_NEIGHBOR_ALLTOALLV_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLTOALLV_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NEIGHBOR_ALLTOALLV_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_NEIGHBOR_ALLTOALLV_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLTOALLV_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NEIGHBOR_ALLTOALLV_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_NEIGHBOR_ALLTOALLV_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLTOALLV_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NEIGHBOR_ALLTOALLV_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NEIGHBOR_ALLTOALLV_INIT_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_NEIGHBOR_ALLTOALLV_INIT_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_NEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_NEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Neighbor_alltoallw will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_NEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_NEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_NEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_NEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_NEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_INEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_INEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Ineighbor_alltoallw will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_INEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_INEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE", &(MPIR_CVAR_INEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_INEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_INEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE", &(MPIR_CVAR_INEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_INEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_INEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE", &(MPIR_CVAR_INEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_INEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_INEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_INEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_NEIGHBOR_ALLTOALLW_INIT_DEVICE_COLLECTIVE, /* name */
        &MPIR_CVAR_NEIGHBOR_ALLTOALLW_INIT_DEVICE_COLLECTIVE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR is only used when MPIR_CVAR_DEVICE_COLLECTIVES is set to \"percoll\".  If set to true, MPI_Neighbor_alltoallw_init will allow the device to override the MPIR-level collective algorithms.  The device might still call the MPIR-level algorithms manually.  If set to false, the device-override will be disabled.");
    MPIR_CVAR_NEIGHBOR_ALLTOALLW_INIT_DEVICE_COLLECTIVE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_NEIGHBOR_ALLTOALLW_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLTOALLW_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NEIGHBOR_ALLTOALLW_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_NEIGHBOR_ALLTOALLW_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLTOALLW_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NEIGHBOR_ALLTOALLW_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_NEIGHBOR_ALLTOALLW_INIT_DEVICE_COLLECTIVE", &(MPIR_CVAR_NEIGHBOR_ALLTOALLW_INIT_DEVICE_COLLECTIVE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NEIGHBOR_ALLTOALLW_INIT_DEVICE_COLLECTIVE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NEIGHBOR_ALLTOALLW_INIT_DEVICE_COLLECTIVE = %d\n", MPIR_CVAR_NEIGHBOR_ALLTOALLW_INIT_DEVICE_COLLECTIVE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_COLL_HYBRID_MEMORY, /* name */
        &MPIR_CVAR_COLL_HYBRID_MEMORY, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This cvar indicates if the memory used in the collective operations are the same type. It set to true, it means in a collective operation, some buffers could be on the CPU and some buffers could be on the GPU. If set to false, it means all the data in a collective operation are on the same type of memory.");
    MPIR_CVAR_COLL_HYBRID_MEMORY = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_COLL_HYBRID_MEMORY", &(MPIR_CVAR_COLL_HYBRID_MEMORY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_COLL_HYBRID_MEMORY");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_COLL_HYBRID_MEMORY", &(MPIR_CVAR_COLL_HYBRID_MEMORY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_COLL_HYBRID_MEMORY");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_COLL_HYBRID_MEMORY", &(MPIR_CVAR_COLL_HYBRID_MEMORY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_COLL_HYBRID_MEMORY");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_COLL_HYBRID_MEMORY = %d\n", MPIR_CVAR_COLL_HYBRID_MEMORY);
    }

    defaultval.d = 1024;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_GATHER_VSMALL_MSG_SIZE, /* name */
        &MPIR_CVAR_GATHER_VSMALL_MSG_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "use a temporary buffer for intracommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE)");
    MPIR_CVAR_GATHER_VSMALL_MSG_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_GATHER_VSMALL_MSG_SIZE", &(MPIR_CVAR_GATHER_VSMALL_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_GATHER_VSMALL_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_GATHER_VSMALL_MSG_SIZE", &(MPIR_CVAR_GATHER_VSMALL_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_GATHER_VSMALL_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_GATHER_VSMALL_MSG_SIZE", &(MPIR_CVAR_GATHER_VSMALL_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_GATHER_VSMALL_MSG_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_GATHER_VSMALL_MSG_SIZE = %d\n", MPIR_CVAR_GATHER_VSMALL_MSG_SIZE);
    }

    defaultval.d = 2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLTOALL_BRUCKS_KVAL, /* name */
        &MPIR_CVAR_IALLTOALL_BRUCKS_KVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "radix (k) value for generic transport brucks based ialltoall");
    MPIR_CVAR_IALLTOALL_BRUCKS_KVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IALLTOALL_BRUCKS_KVAL", &(MPIR_CVAR_IALLTOALL_BRUCKS_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLTOALL_BRUCKS_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IALLTOALL_BRUCKS_KVAL", &(MPIR_CVAR_IALLTOALL_BRUCKS_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLTOALL_BRUCKS_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IALLTOALL_BRUCKS_KVAL", &(MPIR_CVAR_IALLTOALL_BRUCKS_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLTOALL_BRUCKS_KVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLTOALL_BRUCKS_KVAL = %d\n", MPIR_CVAR_IALLTOALL_BRUCKS_KVAL);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLTOALL_BRUCKS_BUFFER_PER_NBR, /* name */
        &MPIR_CVAR_IALLTOALL_BRUCKS_BUFFER_PER_NBR, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "If set to true, the tsp based brucks algorithm will allocate dedicated send and receive buffers for every neighbor in the brucks algorithm. Otherwise, it would reuse a single buffer for sending and receiving data to/from neighbors");
    MPIR_CVAR_IALLTOALL_BRUCKS_BUFFER_PER_NBR = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_IALLTOALL_BRUCKS_BUFFER_PER_NBR", &(MPIR_CVAR_IALLTOALL_BRUCKS_BUFFER_PER_NBR));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLTOALL_BRUCKS_BUFFER_PER_NBR");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_IALLTOALL_BRUCKS_BUFFER_PER_NBR", &(MPIR_CVAR_IALLTOALL_BRUCKS_BUFFER_PER_NBR));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLTOALL_BRUCKS_BUFFER_PER_NBR");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_IALLTOALL_BRUCKS_BUFFER_PER_NBR", &(MPIR_CVAR_IALLTOALL_BRUCKS_BUFFER_PER_NBR));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLTOALL_BRUCKS_BUFFER_PER_NBR");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLTOALL_BRUCKS_BUFFER_PER_NBR = %d\n", MPIR_CVAR_IALLTOALL_BRUCKS_BUFFER_PER_NBR);
    }

    defaultval.d = 64;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLTOALL_SCATTERED_OUTSTANDING_TASKS, /* name */
        &MPIR_CVAR_IALLTOALL_SCATTERED_OUTSTANDING_TASKS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Maximum number of outstanding sends and recvs posted at a time");
    MPIR_CVAR_IALLTOALL_SCATTERED_OUTSTANDING_TASKS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IALLTOALL_SCATTERED_OUTSTANDING_TASKS", &(MPIR_CVAR_IALLTOALL_SCATTERED_OUTSTANDING_TASKS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLTOALL_SCATTERED_OUTSTANDING_TASKS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IALLTOALL_SCATTERED_OUTSTANDING_TASKS", &(MPIR_CVAR_IALLTOALL_SCATTERED_OUTSTANDING_TASKS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLTOALL_SCATTERED_OUTSTANDING_TASKS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IALLTOALL_SCATTERED_OUTSTANDING_TASKS", &(MPIR_CVAR_IALLTOALL_SCATTERED_OUTSTANDING_TASKS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLTOALL_SCATTERED_OUTSTANDING_TASKS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLTOALL_SCATTERED_OUTSTANDING_TASKS = %d\n", MPIR_CVAR_IALLTOALL_SCATTERED_OUTSTANDING_TASKS);
    }

    defaultval.d = 4;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IALLTOALL_SCATTERED_BATCH_SIZE, /* name */
        &MPIR_CVAR_IALLTOALL_SCATTERED_BATCH_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Number of send/receive tasks that scattered algorithm waits for completion before posting another batch of send/receives of that size");
    MPIR_CVAR_IALLTOALL_SCATTERED_BATCH_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_IALLTOALL_SCATTERED_BATCH_SIZE", &(MPIR_CVAR_IALLTOALL_SCATTERED_BATCH_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IALLTOALL_SCATTERED_BATCH_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_IALLTOALL_SCATTERED_BATCH_SIZE", &(MPIR_CVAR_IALLTOALL_SCATTERED_BATCH_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IALLTOALL_SCATTERED_BATCH_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_IALLTOALL_SCATTERED_BATCH_SIZE", &(MPIR_CVAR_IALLTOALL_SCATTERED_BATCH_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IALLTOALL_SCATTERED_BATCH_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IALLTOALL_SCATTERED_BATCH_SIZE = %d\n", MPIR_CVAR_IALLTOALL_SCATTERED_BATCH_SIZE);
    }

    defaultval.d = MPIR_CVAR_DEVICE_COLLECTIVES_percoll;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_DEVICE_COLLECTIVES, /* name */
        &MPIR_CVAR_DEVICE_COLLECTIVES, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select whether the device can override the\n"
"MPIR-level collective algorithms.\n"
"all     - Always prefer the device collectives\n"
"none    - Never pick the device collectives\n"
"percoll - Use the per-collective CVARs to decide");
    MPIR_CVAR_DEVICE_COLLECTIVES = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_DEVICE_COLLECTIVES", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_DEVICE_COLLECTIVES");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_DEVICE_COLLECTIVES", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_DEVICE_COLLECTIVES");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_DEVICE_COLLECTIVES", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_DEVICE_COLLECTIVES");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_DEVICE_COLLECTIVES = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "all"))
            MPIR_CVAR_DEVICE_COLLECTIVES = MPIR_CVAR_DEVICE_COLLECTIVES_all;
        else if (0 == strcmp(tmp_str, "none"))
            MPIR_CVAR_DEVICE_COLLECTIVES = MPIR_CVAR_DEVICE_COLLECTIVES_none;
        else if (0 == strcmp(tmp_str, "percoll"))
            MPIR_CVAR_DEVICE_COLLECTIVES = MPIR_CVAR_DEVICE_COLLECTIVES_percoll;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_DEVICE_COLLECTIVES", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_COLLECTIVE_FALLBACK_silent;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_COLLECTIVE_FALLBACK, /* name */
        &MPIR_CVAR_COLLECTIVE_FALLBACK, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to control what the MPI library should do if the\n"
"user-specified collective algorithm does not work for the\n"
"arguments passed in by the user.\n"
"error   - throw an error\n"
"print   - print an error message and fallback to the internally selected algorithm\n"
"silent  - silently fallback to the internally selected algorithm");
    MPIR_CVAR_COLLECTIVE_FALLBACK = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_COLLECTIVE_FALLBACK", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_COLLECTIVE_FALLBACK");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_COLLECTIVE_FALLBACK", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_COLLECTIVE_FALLBACK");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_COLLECTIVE_FALLBACK", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_COLLECTIVE_FALLBACK");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_COLLECTIVE_FALLBACK = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "error"))
            MPIR_CVAR_COLLECTIVE_FALLBACK = MPIR_CVAR_COLLECTIVE_FALLBACK_error;
        else if (0 == strcmp(tmp_str, "print"))
            MPIR_CVAR_COLLECTIVE_FALLBACK = MPIR_CVAR_COLLECTIVE_FALLBACK_print;
        else if (0 == strcmp(tmp_str, "silent"))
            MPIR_CVAR_COLLECTIVE_FALLBACK = MPIR_CVAR_COLLECTIVE_FALLBACK_silent;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_COLLECTIVE_FALLBACK", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.str = (const char *) "";
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_CHAR,
        MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE, /* name */
        &MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE, /* address */
        MPIR_CVAR_MAX_STRLEN, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Defines the location of tuning file.");
    tmp_str = defaultval.str;
    got_rc = 0;
    rc = MPL_env2str("MPICH_COLL_SELECTION_TUNING_JSON_FILE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_COLL_SELECTION_TUNING_JSON_FILE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_COLL_SELECTION_TUNING_JSON_FILE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_COLL_SELECTION_TUNING_JSON_FILE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE = MPL_strdup(tmp_str);
        MPIR_CVAR_assert(MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE);
        if (MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE == NULL) {
            MPIR_CHKMEM_SETERR(mpi_errno, strlen(tmp_str), "dup of string for MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE");
            goto fn_fail;
        }
    }
    else {
        MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE = NULL;
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_HIERARCHY_DUMP, /* name */
        &MPIR_CVAR_HIERARCHY_DUMP, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "If set to true, each rank will dump the hierarchy data structure to a file named \"hierarchy[rank]\" in the current folder. If set to false, the hierarchy data structure will not be dumped.");
    MPIR_CVAR_HIERARCHY_DUMP = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_HIERARCHY_DUMP", &(MPIR_CVAR_HIERARCHY_DUMP));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_HIERARCHY_DUMP");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_HIERARCHY_DUMP", &(MPIR_CVAR_HIERARCHY_DUMP));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_HIERARCHY_DUMP");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_HIERARCHY_DUMP", &(MPIR_CVAR_HIERARCHY_DUMP));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_HIERARCHY_DUMP");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_HIERARCHY_DUMP = %d\n", MPIR_CVAR_HIERARCHY_DUMP);
    }

#if defined MPID_COORDINATES_FILE
    defaultval.str = MPID_COORDINATES_FILE;
#else
    defaultval.str = (const char *) "";
#endif /* MPID_COORDINATES_FILE */

    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_CHAR,
        MPIR_CVAR_COORDINATES_FILE, /* name */
        &MPIR_CVAR_COORDINATES_FILE, /* address */
        MPIR_CVAR_MAX_STRLEN, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Defines the location of the input coordinates file.");
    tmp_str = defaultval.str;
    got_rc = 0;
    rc = MPL_env2str("MPICH_COORDINATES_FILE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_COORDINATES_FILE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_COORDINATES_FILE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_COORDINATES_FILE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_COORDINATES_FILE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_COORDINATES_FILE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_COORDINATES_FILE = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        MPIR_CVAR_COORDINATES_FILE = MPL_strdup(tmp_str);
        MPIR_CVAR_assert(MPIR_CVAR_COORDINATES_FILE);
        if (MPIR_CVAR_COORDINATES_FILE == NULL) {
            MPIR_CHKMEM_SETERR(mpi_errno, strlen(tmp_str), "dup of string for MPIR_CVAR_COORDINATES_FILE");
            goto fn_fail;
        }
    }
    else {
        MPIR_CVAR_COORDINATES_FILE = NULL;
    }

#if defined MPID_COLL_TREE_DUMP
    defaultval.d = MPID_COLL_TREE_DUMP;
#else
    defaultval.d = 0;
#endif /* MPID_COLL_TREE_DUMP */

    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_COLL_TREE_DUMP, /* name */
        &MPIR_CVAR_COLL_TREE_DUMP, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "If set to true, each rank will dump the tree to a file named \"colltree[rank].json\" in the current folder. If set to false, the tree will not be dumped.");
    MPIR_CVAR_COLL_TREE_DUMP = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_COLL_TREE_DUMP", &(MPIR_CVAR_COLL_TREE_DUMP));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_COLL_TREE_DUMP");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_COLL_TREE_DUMP", &(MPIR_CVAR_COLL_TREE_DUMP));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_COLL_TREE_DUMP");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_COLL_TREE_DUMP", &(MPIR_CVAR_COLL_TREE_DUMP));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_COLL_TREE_DUMP");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_COLL_TREE_DUMP = %d\n", MPIR_CVAR_COLL_TREE_DUMP);
    }

#if defined MPID_COORDINATES_DUMP
    defaultval.d = MPID_COORDINATES_DUMP;
#else
    defaultval.d = 0;
#endif /* MPID_COORDINATES_DUMP */

    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_COORDINATES_DUMP, /* name */
        &MPIR_CVAR_COORDINATES_DUMP, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "If set to true, rank 0 will dump the network coordinates to a file named \"coords\" in the current folder. If set to false, the network coordinates will not be dumped.");
    MPIR_CVAR_COORDINATES_DUMP = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_COORDINATES_DUMP", &(MPIR_CVAR_COORDINATES_DUMP));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_COORDINATES_DUMP");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_COORDINATES_DUMP", &(MPIR_CVAR_COORDINATES_DUMP));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_COORDINATES_DUMP");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_COORDINATES_DUMP", &(MPIR_CVAR_COORDINATES_DUMP));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_COORDINATES_DUMP");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_COORDINATES_DUMP = %d\n", MPIR_CVAR_COORDINATES_DUMP);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_PROGRESS_MAX_COLLS, /* name */
        &MPIR_CVAR_PROGRESS_MAX_COLLS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Maximum number of collective operations at a time that the progress engine should make progress on");
    MPIR_CVAR_PROGRESS_MAX_COLLS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_PROGRESS_MAX_COLLS", &(MPIR_CVAR_PROGRESS_MAX_COLLS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_PROGRESS_MAX_COLLS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_PROGRESS_MAX_COLLS", &(MPIR_CVAR_PROGRESS_MAX_COLLS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_PROGRESS_MAX_COLLS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_PROGRESS_MAX_COLLS", &(MPIR_CVAR_PROGRESS_MAX_COLLS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_PROGRESS_MAX_COLLS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_PROGRESS_MAX_COLLS = %d\n", MPIR_CVAR_PROGRESS_MAX_COLLS);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_COMM_SPLIT_USE_QSORT, /* name */
        &MPIR_CVAR_COMM_SPLIT_USE_QSORT, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COMMUNICATOR", /* category */
        "Use qsort(3) in the implementation of MPI_Comm_split instead of bubble sort.");
    MPIR_CVAR_COMM_SPLIT_USE_QSORT = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_COMM_SPLIT_USE_QSORT", &(MPIR_CVAR_COMM_SPLIT_USE_QSORT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_COMM_SPLIT_USE_QSORT");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_COMM_SPLIT_USE_QSORT", &(MPIR_CVAR_COMM_SPLIT_USE_QSORT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_COMM_SPLIT_USE_QSORT");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_COMM_SPLIT_USE_QSORT", &(MPIR_CVAR_COMM_SPLIT_USE_QSORT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_COMM_SPLIT_USE_QSORT");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_COMM_SPLIT_USE_QSORT = %d\n", MPIR_CVAR_COMM_SPLIT_USE_QSORT);
    }

    defaultval.d = 2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CTXID_EAGER_SIZE, /* name */
        &MPIR_CVAR_CTXID_EAGER_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "THREADS", /* category */
        "The MPIR_CVAR_CTXID_EAGER_SIZE environment variable allows you to specify how many words in the context ID mask will be set aside for the eager allocation protocol.  If the application is running out of context IDs, reducing this value may help.");
    MPIR_CVAR_CTXID_EAGER_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CTXID_EAGER_SIZE", &(MPIR_CVAR_CTXID_EAGER_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CTXID_EAGER_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CTXID_EAGER_SIZE", &(MPIR_CVAR_CTXID_EAGER_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CTXID_EAGER_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CTXID_EAGER_SIZE", &(MPIR_CVAR_CTXID_EAGER_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CTXID_EAGER_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CTXID_EAGER_SIZE = %d\n", MPIR_CVAR_CTXID_EAGER_SIZE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_DATALOOP_FAST_SEEK, /* name */
        &MPIR_CVAR_DATALOOP_FAST_SEEK, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "DATALOOP", /* category */
        "use a datatype-specialized algorithm to shortcut seeking to the correct location in a noncontiguous buffer");
    MPIR_CVAR_DATALOOP_FAST_SEEK = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_DATALOOP_FAST_SEEK", &(MPIR_CVAR_DATALOOP_FAST_SEEK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_DATALOOP_FAST_SEEK");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_DATALOOP_FAST_SEEK", &(MPIR_CVAR_DATALOOP_FAST_SEEK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_DATALOOP_FAST_SEEK");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_DATALOOP_FAST_SEEK", &(MPIR_CVAR_DATALOOP_FAST_SEEK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_DATALOOP_FAST_SEEK");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_DATALOOP_FAST_SEEK = %d\n", MPIR_CVAR_DATALOOP_FAST_SEEK);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_YAKSA_COMPLEX_SUPPORT, /* name */
        &MPIR_CVAR_YAKSA_COMPLEX_SUPPORT, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR indicates that complex type reduction is not supported in yaksa.");
    MPIR_CVAR_YAKSA_COMPLEX_SUPPORT = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_YAKSA_COMPLEX_SUPPORT", &(MPIR_CVAR_YAKSA_COMPLEX_SUPPORT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_YAKSA_COMPLEX_SUPPORT");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_YAKSA_COMPLEX_SUPPORT", &(MPIR_CVAR_YAKSA_COMPLEX_SUPPORT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_YAKSA_COMPLEX_SUPPORT");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_YAKSA_COMPLEX_SUPPORT", &(MPIR_CVAR_YAKSA_COMPLEX_SUPPORT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_YAKSA_COMPLEX_SUPPORT");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_YAKSA_COMPLEX_SUPPORT = %d\n", MPIR_CVAR_YAKSA_COMPLEX_SUPPORT);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_GPU_DOUBLE_SUPPORT, /* name */
        &MPIR_CVAR_GPU_DOUBLE_SUPPORT, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR indicates that double type is not supported on the GPU.");
    MPIR_CVAR_GPU_DOUBLE_SUPPORT = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_GPU_DOUBLE_SUPPORT", &(MPIR_CVAR_GPU_DOUBLE_SUPPORT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_GPU_DOUBLE_SUPPORT");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_GPU_DOUBLE_SUPPORT", &(MPIR_CVAR_GPU_DOUBLE_SUPPORT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_GPU_DOUBLE_SUPPORT");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_GPU_DOUBLE_SUPPORT", &(MPIR_CVAR_GPU_DOUBLE_SUPPORT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_GPU_DOUBLE_SUPPORT");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_GPU_DOUBLE_SUPPORT = %d\n", MPIR_CVAR_GPU_DOUBLE_SUPPORT);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_GPU_LONG_DOUBLE_SUPPORT, /* name */
        &MPIR_CVAR_GPU_LONG_DOUBLE_SUPPORT, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This CVAR indicates that double type is not supported on the GPU.");
    MPIR_CVAR_GPU_LONG_DOUBLE_SUPPORT = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_GPU_LONG_DOUBLE_SUPPORT", &(MPIR_CVAR_GPU_LONG_DOUBLE_SUPPORT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_GPU_LONG_DOUBLE_SUPPORT");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_GPU_LONG_DOUBLE_SUPPORT", &(MPIR_CVAR_GPU_LONG_DOUBLE_SUPPORT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_GPU_LONG_DOUBLE_SUPPORT");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_GPU_LONG_DOUBLE_SUPPORT", &(MPIR_CVAR_GPU_LONG_DOUBLE_SUPPORT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_GPU_LONG_DOUBLE_SUPPORT");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_GPU_LONG_DOUBLE_SUPPORT = %d\n", MPIR_CVAR_GPU_LONG_DOUBLE_SUPPORT);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ENABLE_YAKSA_REDUCTION, /* name */
        &MPIR_CVAR_ENABLE_YAKSA_REDUCTION, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This cvar enables yaksa based reduction for local reduce.");
    MPIR_CVAR_ENABLE_YAKSA_REDUCTION = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ENABLE_YAKSA_REDUCTION", &(MPIR_CVAR_ENABLE_YAKSA_REDUCTION));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ENABLE_YAKSA_REDUCTION");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ENABLE_YAKSA_REDUCTION", &(MPIR_CVAR_ENABLE_YAKSA_REDUCTION));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ENABLE_YAKSA_REDUCTION");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ENABLE_YAKSA_REDUCTION", &(MPIR_CVAR_ENABLE_YAKSA_REDUCTION));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ENABLE_YAKSA_REDUCTION");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ENABLE_YAKSA_REDUCTION = %d\n", MPIR_CVAR_ENABLE_YAKSA_REDUCTION);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_YAKSA_REDUCTION_THRESHOLD, /* name */
        &MPIR_CVAR_YAKSA_REDUCTION_THRESHOLD, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This cvar disables yaksa based reduction for messages above the threshold. The default is no limit.");
    MPIR_CVAR_YAKSA_REDUCTION_THRESHOLD = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_YAKSA_REDUCTION_THRESHOLD", &(MPIR_CVAR_YAKSA_REDUCTION_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_YAKSA_REDUCTION_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_YAKSA_REDUCTION_THRESHOLD", &(MPIR_CVAR_YAKSA_REDUCTION_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_YAKSA_REDUCTION_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_YAKSA_REDUCTION_THRESHOLD", &(MPIR_CVAR_YAKSA_REDUCTION_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_YAKSA_REDUCTION_THRESHOLD");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_YAKSA_REDUCTION_THRESHOLD = %d\n", MPIR_CVAR_YAKSA_REDUCTION_THRESHOLD);
    }

    defaultval.d = 64;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_PROCTABLE_SIZE, /* name */
        &MPIR_CVAR_PROCTABLE_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "DEBUGGER", /* category */
        "Size of the \"MPIR\" debugger interface proctable (process table).");
    MPIR_CVAR_PROCTABLE_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_PROCTABLE_SIZE", &(MPIR_CVAR_PROCTABLE_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_PROCTABLE_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_PROCTABLE_SIZE", &(MPIR_CVAR_PROCTABLE_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_PROCTABLE_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_PROCTABLE_SIZE", &(MPIR_CVAR_PROCTABLE_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_PROCTABLE_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_PROCTABLE_SIZE = %d\n", MPIR_CVAR_PROCTABLE_SIZE);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_PROCTABLE_PRINT, /* name */
        &MPIR_CVAR_PROCTABLE_PRINT, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "DEBUGGER", /* category */
        "If true, dump the proctable entries at MPII_Wait_for_debugger-time.");
    MPIR_CVAR_PROCTABLE_PRINT = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_PROCTABLE_PRINT", &(MPIR_CVAR_PROCTABLE_PRINT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_PROCTABLE_PRINT");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_PROCTABLE_PRINT", &(MPIR_CVAR_PROCTABLE_PRINT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_PROCTABLE_PRINT");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_PROCTABLE_PRINT", &(MPIR_CVAR_PROCTABLE_PRINT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_PROCTABLE_PRINT");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_PROCTABLE_PRINT = %d\n", MPIR_CVAR_PROCTABLE_PRINT);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_PRINT_ERROR_STACK, /* name */
        &MPIR_CVAR_PRINT_ERROR_STACK, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "ERROR_HANDLING", /* category */
        "If true, print an error stack trace at error handling time.");
    MPIR_CVAR_PRINT_ERROR_STACK = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_PRINT_ERROR_STACK", &(MPIR_CVAR_PRINT_ERROR_STACK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_PRINT_ERROR_STACK");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_PRINT_ERROR_STACK", &(MPIR_CVAR_PRINT_ERROR_STACK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_PRINT_ERROR_STACK");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_PRINT_ERROR_STACK", &(MPIR_CVAR_PRINT_ERROR_STACK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_PRINT_ERROR_STACK");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_PRINT_ERROR_STACK = %d\n", MPIR_CVAR_PRINT_ERROR_STACK);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CHOP_ERROR_STACK, /* name */
        &MPIR_CVAR_CHOP_ERROR_STACK, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "ERROR_HANDLING", /* category */
        "If >0, truncate error stack output lines this many characters wide.  If 0, do not truncate, and if <0 use a sensible default.");
    MPIR_CVAR_CHOP_ERROR_STACK = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CHOP_ERROR_STACK", &(MPIR_CVAR_CHOP_ERROR_STACK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CHOP_ERROR_STACK");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CHOP_ERROR_STACK", &(MPIR_CVAR_CHOP_ERROR_STACK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CHOP_ERROR_STACK");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CHOP_ERROR_STACK", &(MPIR_CVAR_CHOP_ERROR_STACK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CHOP_ERROR_STACK");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CHOP_ERROR_STACK = %d\n", MPIR_CVAR_CHOP_ERROR_STACK);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ASYNC_PROGRESS, /* name */
        &MPIR_CVAR_ASYNC_PROGRESS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "THREADS", /* category */
        "If set to true, MPICH will initiate an additional thread to make asynchronous progress on all communication operations including point-to-point, collective, one-sided operations and I/O.  Setting this variable will automatically increase the thread-safety level to MPI_THREAD_MULTIPLE.  While this improves the progress semantics, it might cause a small amount of performance overhead for regular MPI operations.  The user is encouraged to leave one or more hardware threads vacant in order to prevent contention between the application threads and the progress thread(s).  The impact of oversubscription is highly system dependent but may be substantial in some cases, hence this recommendation.");
    MPIR_CVAR_ASYNC_PROGRESS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_ASYNC_PROGRESS", &(MPIR_CVAR_ASYNC_PROGRESS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ASYNC_PROGRESS");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_ASYNC_PROGRESS", &(MPIR_CVAR_ASYNC_PROGRESS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ASYNC_PROGRESS");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_ASYNC_PROGRESS", &(MPIR_CVAR_ASYNC_PROGRESS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ASYNC_PROGRESS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ASYNC_PROGRESS = %d\n", MPIR_CVAR_ASYNC_PROGRESS);
    }

#if defined MPID_PROGRESS_THREAD_AFFINITY
    defaultval.str = MPID_PROGRESS_THREAD_AFFINITY;
#else
    defaultval.str = (const char *) "";
#endif /* MPID_PROGRESS_THREAD_AFFINITY */

    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_CHAR,
        MPIR_CVAR_PROGRESS_THREAD_AFFINITY, /* name */
        &MPIR_CVAR_PROGRESS_THREAD_AFFINITY, /* address */
        MPIR_CVAR_MAX_STRLEN, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "THREADS", /* category */
        "Specifies affinity for all progress threads of local processes. Can be set to auto or comma-separated list of logical processors. When set to auto - MPICH will automatically select logical CPU cores to decide affinity of the progress threads. When set to comma-separated list of logical processors - In case of N progress threads per process, the first N logical processors from list will be assigned to threads of first local process, the next N logical processors from list - to second local process and so on. For example, thread affinity is \"0,1,2,3\", 2 progress threads per process and 2 processes per node. Progress threads of first local process will be pinned on logical processors \"0,1\", progress threads of second local process - on \"2,3\". Cannot work together with MPIR_CVAR_NUM_CLIQUES or MPIR_CVAR_ODD_EVEN_CLIQUES.");
    tmp_str = defaultval.str;
    got_rc = 0;
    rc = MPL_env2str("MPICH_PROGRESS_THREAD_AFFINITY", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_PROGRESS_THREAD_AFFINITY");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_PROGRESS_THREAD_AFFINITY", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_PROGRESS_THREAD_AFFINITY");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_PROGRESS_THREAD_AFFINITY", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_PROGRESS_THREAD_AFFINITY");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_PROGRESS_THREAD_AFFINITY = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        MPIR_CVAR_PROGRESS_THREAD_AFFINITY = MPL_strdup(tmp_str);
        MPIR_CVAR_assert(MPIR_CVAR_PROGRESS_THREAD_AFFINITY);
        if (MPIR_CVAR_PROGRESS_THREAD_AFFINITY == NULL) {
            MPIR_CHKMEM_SETERR(mpi_errno, strlen(tmp_str), "dup of string for MPIR_CVAR_PROGRESS_THREAD_AFFINITY");
            goto fn_fail;
        }
    }
    else {
        MPIR_CVAR_PROGRESS_THREAD_AFFINITY = NULL;
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_SUPPRESS_ABORT_MESSAGE, /* name */
        &MPIR_CVAR_SUPPRESS_ABORT_MESSAGE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "ERROR_HANDLING", /* category */
        "Disable printing of abort error message.");
    MPIR_CVAR_SUPPRESS_ABORT_MESSAGE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_SUPPRESS_ABORT_MESSAGE", &(MPIR_CVAR_SUPPRESS_ABORT_MESSAGE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_SUPPRESS_ABORT_MESSAGE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_SUPPRESS_ABORT_MESSAGE", &(MPIR_CVAR_SUPPRESS_ABORT_MESSAGE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_SUPPRESS_ABORT_MESSAGE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_SUPPRESS_ABORT_MESSAGE", &(MPIR_CVAR_SUPPRESS_ABORT_MESSAGE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_SUPPRESS_ABORT_MESSAGE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_SUPPRESS_ABORT_MESSAGE = %d\n", MPIR_CVAR_SUPPRESS_ABORT_MESSAGE);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_COREDUMP_ON_ABORT, /* name */
        &MPIR_CVAR_COREDUMP_ON_ABORT, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "ERROR_HANDLING", /* category */
        "Call libc abort() to generate a corefile");
    MPIR_CVAR_COREDUMP_ON_ABORT = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_COREDUMP_ON_ABORT", &(MPIR_CVAR_COREDUMP_ON_ABORT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_COREDUMP_ON_ABORT");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_COREDUMP_ON_ABORT", &(MPIR_CVAR_COREDUMP_ON_ABORT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_COREDUMP_ON_ABORT");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_COREDUMP_ON_ABORT", &(MPIR_CVAR_COREDUMP_ON_ABORT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_COREDUMP_ON_ABORT");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_COREDUMP_ON_ABORT = %d\n", MPIR_CVAR_COREDUMP_ON_ABORT);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ERROR_CHECKING, /* name */
        &MPIR_CVAR_ERROR_CHECKING, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "ERROR_HANDLING", /* category */
        "If true, perform checks for errors, typically to verify valid inputs to MPI routines.  Only effective when MPICH is configured with --enable-error-checking=runtime .");
    MPIR_CVAR_ERROR_CHECKING = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_ERROR_CHECKING", &(MPIR_CVAR_ERROR_CHECKING));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ERROR_CHECKING");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_ERROR_CHECKING", &(MPIR_CVAR_ERROR_CHECKING));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ERROR_CHECKING");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_ERROR_CHECKING", &(MPIR_CVAR_ERROR_CHECKING));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ERROR_CHECKING");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ERROR_CHECKING = %d\n", MPIR_CVAR_ERROR_CHECKING);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_MEMDUMP, /* name */
        &MPIR_CVAR_MEMDUMP, /* address */
        1, /* count */
        MPI_T_VERBOSITY_MPIDEV_DETAIL,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "DEVELOPER", /* category */
        "If true, list any memory that was allocated by MPICH and that remains allocated when MPI_Finalize completes.");
    MPIR_CVAR_MEMDUMP = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_MEMDUMP", &(MPIR_CVAR_MEMDUMP));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_MEMDUMP");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_MEMDUMP", &(MPIR_CVAR_MEMDUMP));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_MEMDUMP");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_MEMDUMP", &(MPIR_CVAR_MEMDUMP));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_MEMDUMP");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_MEMDUMP = %d\n", MPIR_CVAR_MEMDUMP);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_DEBUG_SUMMARY, /* name */
        &MPIR_CVAR_DEBUG_SUMMARY, /* address */
        1, /* count */
        MPI_T_VERBOSITY_MPIDEV_DETAIL,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "DEVELOPER", /* category */
        "1: Print internal summary of various debug information, such as memory allocation by category. Each layer may print their own summary information. For example, ch4-ofi may print its provider capability settings. 2: Also print the preferred NIC for each rank");
    MPIR_CVAR_DEBUG_SUMMARY = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_MEM_CATEGORY_INFORMATION", &(MPIR_CVAR_DEBUG_SUMMARY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_MEM_CATEGORY_INFORMATION");
    got_rc += rc;
    rc = MPL_env2int("MPICH_CH4_OFI_CAPABILITY_SETS_DEBUG", &(MPIR_CVAR_DEBUG_SUMMARY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_CAPABILITY_SETS_DEBUG");
    got_rc += rc;
    rc = MPL_env2int("MPICH_CH4_UCX_CAPABILITY_DEBUG", &(MPIR_CVAR_DEBUG_SUMMARY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_UCX_CAPABILITY_DEBUG");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_MEM_CATEGORY_INFORMATION", &(MPIR_CVAR_DEBUG_SUMMARY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_MEM_CATEGORY_INFORMATION");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_CAPABILITY_SETS_DEBUG", &(MPIR_CVAR_DEBUG_SUMMARY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_CAPABILITY_SETS_DEBUG");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_UCX_CAPABILITY_DEBUG", &(MPIR_CVAR_DEBUG_SUMMARY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_UCX_CAPABILITY_DEBUG");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_MEM_CATEGORY_INFORMATION", &(MPIR_CVAR_DEBUG_SUMMARY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_MEM_CATEGORY_INFORMATION");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_CAPABILITY_SETS_DEBUG", &(MPIR_CVAR_DEBUG_SUMMARY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_CAPABILITY_SETS_DEBUG");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_UCX_CAPABILITY_DEBUG", &(MPIR_CVAR_DEBUG_SUMMARY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_UCX_CAPABILITY_DEBUG");
    got_rc += rc;
    rc = MPL_env2int("MPICH_DEBUG_SUMMARY", &(MPIR_CVAR_DEBUG_SUMMARY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_DEBUG_SUMMARY");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_DEBUG_SUMMARY", &(MPIR_CVAR_DEBUG_SUMMARY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_DEBUG_SUMMARY");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_DEBUG_SUMMARY", &(MPIR_CVAR_DEBUG_SUMMARY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_DEBUG_SUMMARY");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_DEBUG_SUMMARY = %d\n", MPIR_CVAR_DEBUG_SUMMARY);
    }

    defaultval.str = (const char *) "MPI_THREAD_SINGLE";
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_CHAR,
        MPIR_CVAR_DEFAULT_THREAD_LEVEL, /* name */
        &MPIR_CVAR_DEFAULT_THREAD_LEVEL, /* address */
        MPIR_CVAR_MAX_STRLEN, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "THREADS", /* category */
        "Sets the default thread level to use when using MPI_INIT. This variable is case-insensitive.");
    tmp_str = defaultval.str;
    got_rc = 0;
    rc = MPL_env2str("MPICH_DEFAULT_THREAD_LEVEL", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_DEFAULT_THREAD_LEVEL");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_DEFAULT_THREAD_LEVEL", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_DEFAULT_THREAD_LEVEL");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_DEFAULT_THREAD_LEVEL", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_DEFAULT_THREAD_LEVEL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_DEFAULT_THREAD_LEVEL = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        MPIR_CVAR_DEFAULT_THREAD_LEVEL = MPL_strdup(tmp_str);
        MPIR_CVAR_assert(MPIR_CVAR_DEFAULT_THREAD_LEVEL);
        if (MPIR_CVAR_DEFAULT_THREAD_LEVEL == NULL) {
            MPIR_CHKMEM_SETERR(mpi_errno, strlen(tmp_str), "dup of string for MPIR_CVAR_DEFAULT_THREAD_LEVEL");
            goto fn_fail;
        }
    }
    else {
        MPIR_CVAR_DEFAULT_THREAD_LEVEL = NULL;
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_DEBUG_HOLD, /* name */
        &MPIR_CVAR_DEBUG_HOLD, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "DEBUGGER", /* category */
        "If true, causes processes to wait in MPI_Init and MPI_Initthread for a debugger to be attached.  Once the debugger has attached, the variable 'hold' should be set to 0 in order to allow the process to continue (e.g., in gdb, \"set hold=0\").");
    MPIR_CVAR_DEBUG_HOLD = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_DEBUG_HOLD", &(MPIR_CVAR_DEBUG_HOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_DEBUG_HOLD");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_DEBUG_HOLD", &(MPIR_CVAR_DEBUG_HOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_DEBUG_HOLD");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_DEBUG_HOLD", &(MPIR_CVAR_DEBUG_HOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_DEBUG_HOLD");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_DEBUG_HOLD = %d\n", MPIR_CVAR_DEBUG_HOLD);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_GPU_USE_IMMEDIATE_COMMAND_LIST, /* name */
        &MPIR_CVAR_GPU_USE_IMMEDIATE_COMMAND_LIST, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "GPU", /* category */
        "If true, mpl/ze will use immediate command list for copying");
    MPIR_CVAR_GPU_USE_IMMEDIATE_COMMAND_LIST = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_GPU_USE_IMMEDIATE_COMMAND_LIST", &(MPIR_CVAR_GPU_USE_IMMEDIATE_COMMAND_LIST));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_GPU_USE_IMMEDIATE_COMMAND_LIST");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_GPU_USE_IMMEDIATE_COMMAND_LIST", &(MPIR_CVAR_GPU_USE_IMMEDIATE_COMMAND_LIST));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_GPU_USE_IMMEDIATE_COMMAND_LIST");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_GPU_USE_IMMEDIATE_COMMAND_LIST", &(MPIR_CVAR_GPU_USE_IMMEDIATE_COMMAND_LIST));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_GPU_USE_IMMEDIATE_COMMAND_LIST");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_GPU_USE_IMMEDIATE_COMMAND_LIST = %d\n", MPIR_CVAR_GPU_USE_IMMEDIATE_COMMAND_LIST);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_GPU_ROUND_ROBIN_COMMAND_QUEUES, /* name */
        &MPIR_CVAR_GPU_ROUND_ROBIN_COMMAND_QUEUES, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "GPU", /* category */
        "If true, mpl/ze will use command queues in a round-robin fashion. If false, only command queues of index 0 will be used.");
    MPIR_CVAR_GPU_ROUND_ROBIN_COMMAND_QUEUES = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_GPU_ROUND_ROBIN_COMMAND_QUEUES", &(MPIR_CVAR_GPU_ROUND_ROBIN_COMMAND_QUEUES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_GPU_ROUND_ROBIN_COMMAND_QUEUES");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_GPU_ROUND_ROBIN_COMMAND_QUEUES", &(MPIR_CVAR_GPU_ROUND_ROBIN_COMMAND_QUEUES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_GPU_ROUND_ROBIN_COMMAND_QUEUES");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_GPU_ROUND_ROBIN_COMMAND_QUEUES", &(MPIR_CVAR_GPU_ROUND_ROBIN_COMMAND_QUEUES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_GPU_ROUND_ROBIN_COMMAND_QUEUES");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_GPU_ROUND_ROBIN_COMMAND_QUEUES = %d\n", MPIR_CVAR_GPU_ROUND_ROBIN_COMMAND_QUEUES);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_NO_COLLECTIVE_FINALIZE, /* name */
        &MPIR_CVAR_NO_COLLECTIVE_FINALIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "If true, prevent MPI_Finalize to invoke collective behavior such as barrier or communicating to other processes. Consequently, it may result in leaking memory or losing messages due to pre-mature exiting. The default is false, which may invoke collective behaviors at finalize.");
    MPIR_CVAR_NO_COLLECTIVE_FINALIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_NO_COLLECTIVE_FINALIZE", &(MPIR_CVAR_NO_COLLECTIVE_FINALIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NO_COLLECTIVE_FINALIZE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_NO_COLLECTIVE_FINALIZE", &(MPIR_CVAR_NO_COLLECTIVE_FINALIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NO_COLLECTIVE_FINALIZE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_NO_COLLECTIVE_FINALIZE", &(MPIR_CVAR_NO_COLLECTIVE_FINALIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NO_COLLECTIVE_FINALIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NO_COLLECTIVE_FINALIZE = %d\n", MPIR_CVAR_NO_COLLECTIVE_FINALIZE);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_FINALIZE_WAIT, /* name */
        &MPIR_CVAR_FINALIZE_WAIT, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "If true, poll progress at MPI_Finalize until reference count on MPI_COMM_WORLD and MPI_COMM_SELF reaches zero. This may be necessary to prevent remote processes hanging if it has pending communication protocols, e.g. a rendezvous send.");
    MPIR_CVAR_FINALIZE_WAIT = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_FINALIZE_WAIT", &(MPIR_CVAR_FINALIZE_WAIT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_FINALIZE_WAIT");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_FINALIZE_WAIT", &(MPIR_CVAR_FINALIZE_WAIT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_FINALIZE_WAIT");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_FINALIZE_WAIT", &(MPIR_CVAR_FINALIZE_WAIT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_FINALIZE_WAIT");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_FINALIZE_WAIT = %d\n", MPIR_CVAR_FINALIZE_WAIT);
    }

#if defined MPID_INIT_SKIP_PMI_BARRIER
    defaultval.d = MPID_INIT_SKIP_PMI_BARRIER;
#else
    defaultval.d = 1;
#endif /* MPID_INIT_SKIP_PMI_BARRIER */

    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_INIT_SKIP_PMI_BARRIER, /* name */
        &MPIR_CVAR_INIT_SKIP_PMI_BARRIER, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "DEBUGGER", /* category */
        "Skip MPIR_pmi_barrier() in MPI_Init");
    MPIR_CVAR_INIT_SKIP_PMI_BARRIER = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_INIT_SKIP_PMI_BARRIER", &(MPIR_CVAR_INIT_SKIP_PMI_BARRIER));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_INIT_SKIP_PMI_BARRIER");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_INIT_SKIP_PMI_BARRIER", &(MPIR_CVAR_INIT_SKIP_PMI_BARRIER));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_INIT_SKIP_PMI_BARRIER");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_INIT_SKIP_PMI_BARRIER", &(MPIR_CVAR_INIT_SKIP_PMI_BARRIER));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_INIT_SKIP_PMI_BARRIER");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_INIT_SKIP_PMI_BARRIER = %d\n", MPIR_CVAR_INIT_SKIP_PMI_BARRIER);
    }

    defaultval.d = 4096;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE, /* name */
        &MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH4", /* category */
        "If a send message size is less than or equal to MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE (in bytes), then enable GPU-based fast memcpy. The environment variable is valid only when then GPU IPC shmmod is enabled.");
    MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_GPU_FAST_COPY_MAX_SIZE", &(MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_GPU_FAST_COPY_MAX_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_GPU_FAST_COPY_MAX_SIZE", &(MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_GPU_FAST_COPY_MAX_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE", &(MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE = %d\n", MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE);
    }

    defaultval.d = 1048576;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE_H2D, /* name */
        &MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE_H2D, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH4", /* category */
        "If a receive message size is less than or equal to MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE_H2D (in bytes), then enable GPU-based fast memcpy.");
    MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE_H2D = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_GPU_FAST_COPY_MAX_SIZE_H2D", &(MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE_H2D));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_GPU_FAST_COPY_MAX_SIZE_H2D");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_GPU_FAST_COPY_MAX_SIZE_H2D", &(MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE_H2D));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_GPU_FAST_COPY_MAX_SIZE_H2D");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE_H2D", &(MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE_H2D));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE_H2D");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE_H2D = %d\n", MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE_H2D);
    }

    defaultval.d = 32768;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE_D2H, /* name */
        &MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE_D2H, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH4", /* category */
        "If a send message size is less than or equal to MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE_D2H (in bytes), then enable GPU-based fast memcpy.");
    MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE_D2H = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_GPU_FAST_COPY_MAX_SIZE_D2H", &(MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE_D2H));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_GPU_FAST_COPY_MAX_SIZE_D2H");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_GPU_FAST_COPY_MAX_SIZE_D2H", &(MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE_D2H));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_GPU_FAST_COPY_MAX_SIZE_D2H");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE_D2H", &(MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE_D2H));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE_D2H");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE_D2H = %d\n", MPIR_CVAR_GPU_FAST_COPY_MAX_SIZE_D2H);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_REQUEST_ERR_FATAL, /* name */
        &MPIR_CVAR_REQUEST_ERR_FATAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "REQUEST", /* category */
        "By default, MPI_Waitall, MPI_Testall, MPI_Waitsome, and MPI_Testsome return MPI_ERR_IN_STATUS when one of the request fails. If MPIR_CVAR_REQUEST_ERR_FATAL is set to true, these routines will return the error code of the request immediately. The default MPI_ERRS_ARE_FATAL error handler will dump a error stack in this case, which maybe more convenient for debugging. This cvar will also make nonblocking shched return error right away as it issues operations.");
    MPIR_CVAR_REQUEST_ERR_FATAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_REQUEST_ERR_FATAL", &(MPIR_CVAR_REQUEST_ERR_FATAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_REQUEST_ERR_FATAL");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_REQUEST_ERR_FATAL", &(MPIR_CVAR_REQUEST_ERR_FATAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_REQUEST_ERR_FATAL");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_REQUEST_ERR_FATAL", &(MPIR_CVAR_REQUEST_ERR_FATAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_REQUEST_ERR_FATAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_REQUEST_ERR_FATAL = %d\n", MPIR_CVAR_REQUEST_ERR_FATAL);
    }

    defaultval.d = 8;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_REQUEST_POLL_FREQ, /* name */
        &MPIR_CVAR_REQUEST_POLL_FREQ, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "REQUEST", /* category */
        "How frequent to poll during MPI_{Waitany,Waitsome} in terms of number of processed requests before polling.");
    MPIR_CVAR_REQUEST_POLL_FREQ = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_REQUEST_POLL_FREQ", &(MPIR_CVAR_REQUEST_POLL_FREQ));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_REQUEST_POLL_FREQ");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_REQUEST_POLL_FREQ", &(MPIR_CVAR_REQUEST_POLL_FREQ));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_REQUEST_POLL_FREQ");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_REQUEST_POLL_FREQ", &(MPIR_CVAR_REQUEST_POLL_FREQ));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_REQUEST_POLL_FREQ");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_REQUEST_POLL_FREQ = %d\n", MPIR_CVAR_REQUEST_POLL_FREQ);
    }

    defaultval.d = 64;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_REQUEST_BATCH_SIZE, /* name */
        &MPIR_CVAR_REQUEST_BATCH_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "REQUEST", /* category */
        "The number of requests to make completion as a batch in MPI_Waitall and MPI_Testall implementation. A large number is likely to cause more cache misses.");
    MPIR_CVAR_REQUEST_BATCH_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_REQUEST_BATCH_SIZE", &(MPIR_CVAR_REQUEST_BATCH_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_REQUEST_BATCH_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_REQUEST_BATCH_SIZE", &(MPIR_CVAR_REQUEST_BATCH_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_REQUEST_BATCH_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_REQUEST_BATCH_SIZE", &(MPIR_CVAR_REQUEST_BATCH_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_REQUEST_BATCH_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_REQUEST_BATCH_SIZE = %d\n", MPIR_CVAR_REQUEST_BATCH_SIZE);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_DEBUG_PROGRESS_TIMEOUT, /* name */
        &MPIR_CVAR_DEBUG_PROGRESS_TIMEOUT, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4", /* category */
        "Sets the timeout in seconds to dump outstanding requests when progress wait is not making progress for some time.");
    MPIR_CVAR_DEBUG_PROGRESS_TIMEOUT = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_DEBUG_PROGRESS_TIMEOUT", &(MPIR_CVAR_DEBUG_PROGRESS_TIMEOUT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_DEBUG_PROGRESS_TIMEOUT");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_DEBUG_PROGRESS_TIMEOUT", &(MPIR_CVAR_DEBUG_PROGRESS_TIMEOUT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_DEBUG_PROGRESS_TIMEOUT");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_DEBUG_PROGRESS_TIMEOUT", &(MPIR_CVAR_DEBUG_PROGRESS_TIMEOUT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_DEBUG_PROGRESS_TIMEOUT");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_DEBUG_PROGRESS_TIMEOUT = %d\n", MPIR_CVAR_DEBUG_PROGRESS_TIMEOUT);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_DIMS_VERBOSE, /* name */
        &MPIR_CVAR_DIMS_VERBOSE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_MPIDEV_DETAIL,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "DIMS", /* category */
        "If true, enable verbose output about the actions of the implementation of MPI_Dims_create.");
    MPIR_CVAR_DIMS_VERBOSE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_DIMS_VERBOSE", &(MPIR_CVAR_DIMS_VERBOSE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_DIMS_VERBOSE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_DIMS_VERBOSE", &(MPIR_CVAR_DIMS_VERBOSE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_DIMS_VERBOSE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_DIMS_VERBOSE", &(MPIR_CVAR_DIMS_VERBOSE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_DIMS_VERBOSE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_DIMS_VERBOSE = %d\n", MPIR_CVAR_DIMS_VERBOSE);
    }

    defaultval.str = (const char *) NULL;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_CHAR,
        MPIR_CVAR_QMPI_TOOL_LIST, /* name */
        &MPIR_CVAR_QMPI_TOOL_LIST, /* address */
        MPIR_CVAR_MAX_STRLEN, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "TOOLS", /* category */
        "Set the number and order of QMPI tools to be loaded by the MPI library when it is initialized.");
    tmp_str = defaultval.str;
    got_rc = 0;
    rc = MPL_env2str("MPICH_QMPI_TOOL_LIST", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_QMPI_TOOL_LIST");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_QMPI_TOOL_LIST", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_QMPI_TOOL_LIST");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_QMPI_TOOL_LIST", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_QMPI_TOOL_LIST");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_QMPI_TOOL_LIST = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        MPIR_CVAR_QMPI_TOOL_LIST = MPL_strdup(tmp_str);
        MPIR_CVAR_assert(MPIR_CVAR_QMPI_TOOL_LIST);
        if (MPIR_CVAR_QMPI_TOOL_LIST == NULL) {
            MPIR_CHKMEM_SETERR(mpi_errno, strlen(tmp_str), "dup of string for MPIR_CVAR_QMPI_TOOL_LIST");
            goto fn_fail;
        }
    }
    else {
        MPIR_CVAR_QMPI_TOOL_LIST = NULL;
    }

    defaultval.str = (const char *) NULL;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_CHAR,
        MPIR_CVAR_NAMESERV_FILE_PUBDIR, /* name */
        &MPIR_CVAR_NAMESERV_FILE_PUBDIR, /* address */
        MPIR_CVAR_MAX_STRLEN, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "PROCESS_MANAGER", /* category */
        "Sets the directory to use for MPI service publishing in the file nameserv implementation.  Allows the user to override where the publish and lookup information is placed for connect/accept based applications.");
    tmp_str = defaultval.str;
    got_rc = 0;
    rc = MPL_env2str("MPICH_NAMEPUB_DIR", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NAMEPUB_DIR");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_NAMEPUB_DIR", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NAMEPUB_DIR");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_NAMEPUB_DIR", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NAMEPUB_DIR");
    got_rc += rc;
    rc = MPL_env2str("MPICH_NAMESERV_FILE_PUBDIR", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NAMESERV_FILE_PUBDIR");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_NAMESERV_FILE_PUBDIR", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NAMESERV_FILE_PUBDIR");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_NAMESERV_FILE_PUBDIR", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NAMESERV_FILE_PUBDIR");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NAMESERV_FILE_PUBDIR = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        MPIR_CVAR_NAMESERV_FILE_PUBDIR = MPL_strdup(tmp_str);
        MPIR_CVAR_assert(MPIR_CVAR_NAMESERV_FILE_PUBDIR);
        if (MPIR_CVAR_NAMESERV_FILE_PUBDIR == NULL) {
            MPIR_CHKMEM_SETERR(mpi_errno, strlen(tmp_str), "dup of string for MPIR_CVAR_NAMESERV_FILE_PUBDIR");
            goto fn_fail;
        }
    }
    else {
        MPIR_CVAR_NAMESERV_FILE_PUBDIR = NULL;
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ABORT_ON_LEAKED_HANDLES, /* name */
        &MPIR_CVAR_ABORT_ON_LEAKED_HANDLES, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "MEMORY", /* category */
        "If true, MPI will call MPI_Abort at MPI_Finalize if any MPI object handles have been leaked.  For example, if MPI_Comm_dup is called without calling a corresponding MPI_Comm_free.  For uninteresting reasons, enabling this option may prevent all known object leaks from being reported.  MPICH must have been configure with \"--enable-g=handlealloc\" or better in order for this functionality to work.");
    MPIR_CVAR_ABORT_ON_LEAKED_HANDLES = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_ABORT_ON_LEAKED_HANDLES", &(MPIR_CVAR_ABORT_ON_LEAKED_HANDLES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ABORT_ON_LEAKED_HANDLES");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_ABORT_ON_LEAKED_HANDLES", &(MPIR_CVAR_ABORT_ON_LEAKED_HANDLES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ABORT_ON_LEAKED_HANDLES");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_ABORT_ON_LEAKED_HANDLES", &(MPIR_CVAR_ABORT_ON_LEAKED_HANDLES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ABORT_ON_LEAKED_HANDLES");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ABORT_ON_LEAKED_HANDLES = %d\n", MPIR_CVAR_ABORT_ON_LEAKED_HANDLES);
    }

    defaultval.str = (const char *) "auto";
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_CHAR,
        MPIR_CVAR_NETLOC_NODE_FILE, /* name */
        &MPIR_CVAR_NETLOC_NODE_FILE, /* address */
        MPIR_CVAR_MAX_STRLEN, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "DEBUGGER", /* category */
        "Subnet json file");
    tmp_str = defaultval.str;
    got_rc = 0;
    rc = MPL_env2str("MPICH_NETLOC_NODE_FILE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NETLOC_NODE_FILE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_NETLOC_NODE_FILE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NETLOC_NODE_FILE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_NETLOC_NODE_FILE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NETLOC_NODE_FILE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NETLOC_NODE_FILE = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        MPIR_CVAR_NETLOC_NODE_FILE = MPL_strdup(tmp_str);
        MPIR_CVAR_assert(MPIR_CVAR_NETLOC_NODE_FILE);
        if (MPIR_CVAR_NETLOC_NODE_FILE == NULL) {
            MPIR_CHKMEM_SETERR(mpi_errno, strlen(tmp_str), "dup of string for MPIR_CVAR_NETLOC_NODE_FILE");
            goto fn_fail;
        }
    }
    else {
        MPIR_CVAR_NETLOC_NODE_FILE = NULL;
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_NOLOCAL, /* name */
        &MPIR_CVAR_NOLOCAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "NODEMAP", /* category */
        "If true, force all processes to operate as though all processes are located on another node.  For example, this disables shared memory communication hierarchical collectives.");
    MPIR_CVAR_NOLOCAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_NO_LOCAL", &(MPIR_CVAR_NOLOCAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NO_LOCAL");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_NO_LOCAL", &(MPIR_CVAR_NOLOCAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NO_LOCAL");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_NO_LOCAL", &(MPIR_CVAR_NOLOCAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NO_LOCAL");
    got_rc += rc;
    rc = MPL_env2bool("MPICH_NOLOCAL", &(MPIR_CVAR_NOLOCAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NOLOCAL");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_NOLOCAL", &(MPIR_CVAR_NOLOCAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NOLOCAL");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_NOLOCAL", &(MPIR_CVAR_NOLOCAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NOLOCAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NOLOCAL = %d\n", MPIR_CVAR_NOLOCAL);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ODD_EVEN_CLIQUES, /* name */
        &MPIR_CVAR_ODD_EVEN_CLIQUES, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "NODEMAP", /* category */
        "If true, odd procs on a node are seen as local to each other, and even procs on a node are seen as local to each other.  Used for debugging on a single machine. Deprecated in favor of MPIR_CVAR_NUM_CLIQUES.");
    MPIR_CVAR_ODD_EVEN_CLIQUES = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_EVEN_ODD_CLIQUES", &(MPIR_CVAR_ODD_EVEN_CLIQUES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_EVEN_ODD_CLIQUES");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_EVEN_ODD_CLIQUES", &(MPIR_CVAR_ODD_EVEN_CLIQUES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_EVEN_ODD_CLIQUES");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_EVEN_ODD_CLIQUES", &(MPIR_CVAR_ODD_EVEN_CLIQUES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_EVEN_ODD_CLIQUES");
    got_rc += rc;
    rc = MPL_env2bool("MPICH_ODD_EVEN_CLIQUES", &(MPIR_CVAR_ODD_EVEN_CLIQUES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ODD_EVEN_CLIQUES");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_ODD_EVEN_CLIQUES", &(MPIR_CVAR_ODD_EVEN_CLIQUES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ODD_EVEN_CLIQUES");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_ODD_EVEN_CLIQUES", &(MPIR_CVAR_ODD_EVEN_CLIQUES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ODD_EVEN_CLIQUES");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ODD_EVEN_CLIQUES = %d\n", MPIR_CVAR_ODD_EVEN_CLIQUES);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_NUM_CLIQUES, /* name */
        &MPIR_CVAR_NUM_CLIQUES, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "NODEMAP", /* category */
        "Specify the number of cliques that should be used to partition procs on a local node. Procs with the same clique number are seen as local to each other. Used for debugging on a single machine.");
    MPIR_CVAR_NUM_CLIQUES = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_NUM_CLIQUES", &(MPIR_CVAR_NUM_CLIQUES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NUM_CLIQUES");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_NUM_CLIQUES", &(MPIR_CVAR_NUM_CLIQUES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NUM_CLIQUES");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_NUM_CLIQUES", &(MPIR_CVAR_NUM_CLIQUES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NUM_CLIQUES");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NUM_CLIQUES = %d\n", MPIR_CVAR_NUM_CLIQUES);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CLIQUES_BY_BLOCK, /* name */
        &MPIR_CVAR_CLIQUES_BY_BLOCK, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "NODEMAP", /* category */
        "Specify to divide processes into cliques by uniform blocks. The default is to divide in round-robin fashion. Used for debugging on a single machine.");
    MPIR_CVAR_CLIQUES_BY_BLOCK = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_CLIQUES_BY_BLOCK", &(MPIR_CVAR_CLIQUES_BY_BLOCK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CLIQUES_BY_BLOCK");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_CLIQUES_BY_BLOCK", &(MPIR_CVAR_CLIQUES_BY_BLOCK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CLIQUES_BY_BLOCK");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_CLIQUES_BY_BLOCK", &(MPIR_CVAR_CLIQUES_BY_BLOCK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CLIQUES_BY_BLOCK");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CLIQUES_BY_BLOCK = %d\n", MPIR_CVAR_CLIQUES_BY_BLOCK);
    }

    defaultval.d = MPIR_CVAR_PMI_VERSION_1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_PMI_VERSION, /* name */
        &MPIR_CVAR_PMI_VERSION, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "NODEMAP", /* category */
        "Variable to select runtime PMI version.\n"
"1        - PMI (default)\n"
"2        - PMI2\n"
"x        - PMIx");
    MPIR_CVAR_PMI_VERSION = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_PMI_VERSION", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_PMI_VERSION");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_PMI_VERSION", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_PMI_VERSION");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_PMI_VERSION", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_PMI_VERSION");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_PMI_VERSION = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "1"))
            MPIR_CVAR_PMI_VERSION = MPIR_CVAR_PMI_VERSION_1;
        else if (0 == strcmp(tmp_str, "2"))
            MPIR_CVAR_PMI_VERSION = MPIR_CVAR_PMI_VERSION_2;
        else if (0 == strcmp(tmp_str, "x"))
            MPIR_CVAR_PMI_VERSION = MPIR_CVAR_PMI_VERSION_x;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_PMI_VERSION", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_COLL_ALIAS_CHECK, /* name */
        &MPIR_CVAR_COLL_ALIAS_CHECK, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Enable checking of aliasing in collective operations");
    MPIR_CVAR_COLL_ALIAS_CHECK = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_COLL_ALIAS_CHECK", &(MPIR_CVAR_COLL_ALIAS_CHECK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_COLL_ALIAS_CHECK");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_COLL_ALIAS_CHECK", &(MPIR_CVAR_COLL_ALIAS_CHECK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_COLL_ALIAS_CHECK");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_COLL_ALIAS_CHECK", &(MPIR_CVAR_COLL_ALIAS_CHECK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_COLL_ALIAS_CHECK");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_COLL_ALIAS_CHECK = %d\n", MPIR_CVAR_COLL_ALIAS_CHECK);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ENABLE_GPU, /* name */
        &MPIR_CVAR_ENABLE_GPU, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "GPU", /* category */
        "Control MPICH GPU support. If set to 0, all GPU support is disabled and we do not query the buffer type internally because we assume no GPU buffer is use.");
    MPIR_CVAR_ENABLE_GPU = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ENABLE_GPU", &(MPIR_CVAR_ENABLE_GPU));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ENABLE_GPU");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ENABLE_GPU", &(MPIR_CVAR_ENABLE_GPU));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ENABLE_GPU");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ENABLE_GPU", &(MPIR_CVAR_ENABLE_GPU));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ENABLE_GPU");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ENABLE_GPU = %d\n", MPIR_CVAR_ENABLE_GPU);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_GPU_HAS_WAIT_KERNEL, /* name */
        &MPIR_CVAR_GPU_HAS_WAIT_KERNEL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "GPU", /* category */
        "If set to 1, avoid allocate allocating GPU registered host buffers for temporary buffers. When stream workq and GPU wait kernels are in use, access APIs for GPU registered memory may cause deadlock.");
    MPIR_CVAR_GPU_HAS_WAIT_KERNEL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_GPU_HAS_WAIT_KERNEL", &(MPIR_CVAR_GPU_HAS_WAIT_KERNEL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_GPU_HAS_WAIT_KERNEL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_GPU_HAS_WAIT_KERNEL", &(MPIR_CVAR_GPU_HAS_WAIT_KERNEL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_GPU_HAS_WAIT_KERNEL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_GPU_HAS_WAIT_KERNEL", &(MPIR_CVAR_GPU_HAS_WAIT_KERNEL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_GPU_HAS_WAIT_KERNEL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_GPU_HAS_WAIT_KERNEL = %d\n", MPIR_CVAR_GPU_HAS_WAIT_KERNEL);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ENABLE_GPU_REGISTER, /* name */
        &MPIR_CVAR_ENABLE_GPU_REGISTER, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "GPU", /* category */
        "Control whether to actually register buffers with the GPU runtime in MPIR_gpu_register_host. This could lower the latency of certain GPU communication at the cost of some amount of GPU memory consumed by the MPI library. By default, registration is enabled.");
    MPIR_CVAR_ENABLE_GPU_REGISTER = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_ENABLE_GPU_REGISTER", &(MPIR_CVAR_ENABLE_GPU_REGISTER));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ENABLE_GPU_REGISTER");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_ENABLE_GPU_REGISTER", &(MPIR_CVAR_ENABLE_GPU_REGISTER));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ENABLE_GPU_REGISTER");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_ENABLE_GPU_REGISTER", &(MPIR_CVAR_ENABLE_GPU_REGISTER));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ENABLE_GPU_REGISTER");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ENABLE_GPU_REGISTER = %d\n", MPIR_CVAR_ENABLE_GPU_REGISTER);
    }

    defaultval.d = 1000;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_POLLS_BEFORE_YIELD, /* name */
        &MPIR_CVAR_POLLS_BEFORE_YIELD, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "NEMESIS", /* category */
        "When MPICH is in a busy waiting loop, it will periodically call a function to yield the processor.  This cvar sets the number of loops before the yield function is called.  A value of 0 disables yielding.");
    MPIR_CVAR_POLLS_BEFORE_YIELD = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_POLLS_BEFORE_YIELD", &(MPIR_CVAR_POLLS_BEFORE_YIELD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_POLLS_BEFORE_YIELD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_POLLS_BEFORE_YIELD", &(MPIR_CVAR_POLLS_BEFORE_YIELD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_POLLS_BEFORE_YIELD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_POLLS_BEFORE_YIELD", &(MPIR_CVAR_POLLS_BEFORE_YIELD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_POLLS_BEFORE_YIELD");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_POLLS_BEFORE_YIELD = %d\n", MPIR_CVAR_POLLS_BEFORE_YIELD);
    }

    defaultval.str = (const char *) NULL;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_CHAR,
        MPIR_CVAR_CH3_INTERFACE_HOSTNAME, /* name */
        &MPIR_CVAR_CH3_INTERFACE_HOSTNAME, /* address */
        MPIR_CVAR_MAX_STRLEN, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH3", /* category */
        "If non-NULL, this cvar specifies the IP address that other processes should use when connecting to this process. This cvar is mutually exclusive with the MPIR_CVAR_CH3_NETWORK_IFACE cvar and it is an error to set them both.");
    tmp_str = defaultval.str;
    got_rc = 0;
    rc = MPL_env2str("MPICH_INTERFACE_HOSTNAME", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_INTERFACE_HOSTNAME");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_INTERFACE_HOSTNAME", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_INTERFACE_HOSTNAME");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_INTERFACE_HOSTNAME", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_INTERFACE_HOSTNAME");
    got_rc += rc;
    rc = MPL_env2str("MPICH_CH3_INTERFACE_HOSTNAME", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH3_INTERFACE_HOSTNAME");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_CH3_INTERFACE_HOSTNAME", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH3_INTERFACE_HOSTNAME");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_CH3_INTERFACE_HOSTNAME", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH3_INTERFACE_HOSTNAME");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH3_INTERFACE_HOSTNAME = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        MPIR_CVAR_CH3_INTERFACE_HOSTNAME = MPL_strdup(tmp_str);
        MPIR_CVAR_assert(MPIR_CVAR_CH3_INTERFACE_HOSTNAME);
        if (MPIR_CVAR_CH3_INTERFACE_HOSTNAME == NULL) {
            MPIR_CHKMEM_SETERR(mpi_errno, strlen(tmp_str), "dup of string for MPIR_CVAR_CH3_INTERFACE_HOSTNAME");
            goto fn_fail;
        }
    }
    else {
        MPIR_CVAR_CH3_INTERFACE_HOSTNAME = NULL;
    }

    defaultval.range = (MPIR_T_cvar_range_value_t) {0,0};
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH3_PORT_RANGE, /* name */
        &MPIR_CVAR_CH3_PORT_RANGE, /* address */
        2, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH3", /* category */
        "The MPIR_CVAR_CH3_PORT_RANGE environment variable allows you to specify the range of TCP ports to be used by the process manager and the MPICH library. The format of this variable is <low>:<high>.  To specify any available port, use 0:0.");
    MPIR_CVAR_CH3_PORT_RANGE = defaultval.range;
    got_rc = 0;
    rc = MPL_env2range("MPICH_PORTRANGE", &(MPIR_CVAR_CH3_PORT_RANGE.low), &(MPIR_CVAR_CH3_PORT_RANGE.high));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_PORTRANGE");
    got_rc += rc;
    rc = MPL_env2range("MPICH_PORT_RANGE", &(MPIR_CVAR_CH3_PORT_RANGE.low), &(MPIR_CVAR_CH3_PORT_RANGE.high));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_PORT_RANGE");
    got_rc += rc;
    rc = MPL_env2range("MPIR_PARAM_PORTRANGE", &(MPIR_CVAR_CH3_PORT_RANGE.low), &(MPIR_CVAR_CH3_PORT_RANGE.high));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_PORTRANGE");
    got_rc += rc;
    rc = MPL_env2range("MPIR_PARAM_PORT_RANGE", &(MPIR_CVAR_CH3_PORT_RANGE.low), &(MPIR_CVAR_CH3_PORT_RANGE.high));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_PORT_RANGE");
    got_rc += rc;
    rc = MPL_env2range("MPIR_CVAR_PORTRANGE", &(MPIR_CVAR_CH3_PORT_RANGE.low), &(MPIR_CVAR_CH3_PORT_RANGE.high));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_PORTRANGE");
    got_rc += rc;
    rc = MPL_env2range("MPIR_CVAR_PORT_RANGE", &(MPIR_CVAR_CH3_PORT_RANGE.low), &(MPIR_CVAR_CH3_PORT_RANGE.high));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_PORT_RANGE");
    got_rc += rc;
    rc = MPL_env2range("MPICH_CH3_PORT_RANGE", &(MPIR_CVAR_CH3_PORT_RANGE.low), &(MPIR_CVAR_CH3_PORT_RANGE.high));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH3_PORT_RANGE");
    got_rc += rc;
    rc = MPL_env2range("MPIR_PARAM_CH3_PORT_RANGE", &(MPIR_CVAR_CH3_PORT_RANGE.low), &(MPIR_CVAR_CH3_PORT_RANGE.high));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH3_PORT_RANGE");
    got_rc += rc;
    rc = MPL_env2range("MPIR_CVAR_CH3_PORT_RANGE", &(MPIR_CVAR_CH3_PORT_RANGE.low), &(MPIR_CVAR_CH3_PORT_RANGE.high));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH3_PORT_RANGE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH3_PORT_RANGE = %d-%d\n", MPIR_CVAR_CH3_PORT_RANGE.low, MPIR_CVAR_CH3_PORT_RANGE.high);
    }

    defaultval.str = (const char *) NULL;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_CHAR,
        MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE, /* name */
        &MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE, /* address */
        MPIR_CVAR_MAX_STRLEN, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "NEMESIS", /* category */
        "If non-NULL, this cvar specifies which pseudo-ethernet interface the tcp netmod should use (e.g., \"eth1\", \"ib0\"). Note, this is a Linux-specific cvar. This cvar is mutually exclusive with the MPIR_CVAR_CH3_INTERFACE_HOSTNAME cvar and it is an error to set them both.");
    tmp_str = defaultval.str;
    got_rc = 0;
    rc = MPL_env2str("MPICH_NETWORK_IFACE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NETWORK_IFACE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_NETWORK_IFACE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NETWORK_IFACE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_NETWORK_IFACE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NETWORK_IFACE");
    got_rc += rc;
    rc = MPL_env2str("MPICH_NEMESIS_TCP_NETWORK_IFACE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NEMESIS_TCP_NETWORK_IFACE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_NEMESIS_TCP_NETWORK_IFACE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NEMESIS_TCP_NETWORK_IFACE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE = MPL_strdup(tmp_str);
        MPIR_CVAR_assert(MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE);
        if (MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE == NULL) {
            MPIR_CHKMEM_SETERR(mpi_errno, strlen(tmp_str), "dup of string for MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE");
            goto fn_fail;
        }
    }
    else {
        MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE = NULL;
    }

    defaultval.d = 10;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES, /* name */
        &MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "NEMESIS", /* category */
        "This cvar controls the number of times to retry the gethostbyname() function before giving up.");
    MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_NEMESIS_TCP_HOST_LOOKUP_RETRIES", &(MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NEMESIS_TCP_HOST_LOOKUP_RETRIES");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_NEMESIS_TCP_HOST_LOOKUP_RETRIES", &(MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NEMESIS_TCP_HOST_LOOKUP_RETRIES");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES", &(MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES = %d\n", MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_NEMESIS_ENABLE_CKPOINT, /* name */
        &MPIR_CVAR_NEMESIS_ENABLE_CKPOINT, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "NEMESIS", /* category */
        "If true, enables checkpointing support and returns an error if checkpointing library cannot be initialized.");
    MPIR_CVAR_NEMESIS_ENABLE_CKPOINT = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_NEMESIS_ENABLE_CKPOINT", &(MPIR_CVAR_NEMESIS_ENABLE_CKPOINT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NEMESIS_ENABLE_CKPOINT");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_NEMESIS_ENABLE_CKPOINT", &(MPIR_CVAR_NEMESIS_ENABLE_CKPOINT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NEMESIS_ENABLE_CKPOINT");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_NEMESIS_ENABLE_CKPOINT", &(MPIR_CVAR_NEMESIS_ENABLE_CKPOINT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NEMESIS_ENABLE_CKPOINT");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NEMESIS_ENABLE_CKPOINT = %d\n", MPIR_CVAR_NEMESIS_ENABLE_CKPOINT);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ, /* name */
        &MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "NEMESIS", /* category */
        "This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for shared memory. If this cvar is set to -1, then Nemesis will choose an appropriate value.");
    MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_NEMESIS_SHM_EAGER_MAX_SZ", &(MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NEMESIS_SHM_EAGER_MAX_SZ");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_NEMESIS_SHM_EAGER_MAX_SZ", &(MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NEMESIS_SHM_EAGER_MAX_SZ");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ", &(MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ = %d\n", MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ);
    }

    defaultval.d = -2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ, /* name */
        &MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "NEMESIS", /* category */
        "This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for ready-send messages.  If this cvar is set to -1, then ready messages will always be sent eagerly.  If this cvar is set to -2, then Nemesis will choose an appropriate value.");
    MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_NEMESIS_SHM_READY_EAGER_MAX_SZ", &(MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NEMESIS_SHM_READY_EAGER_MAX_SZ");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_NEMESIS_SHM_READY_EAGER_MAX_SZ", &(MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NEMESIS_SHM_READY_EAGER_MAX_SZ");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ", &(MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ = %d\n", MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ENABLE_FT, /* name */
        &MPIR_CVAR_ENABLE_FT, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "FT", /* category */
        "Enable fault tolerance functions");
    MPIR_CVAR_ENABLE_FT = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_ENABLE_FT", &(MPIR_CVAR_ENABLE_FT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ENABLE_FT");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_ENABLE_FT", &(MPIR_CVAR_ENABLE_FT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ENABLE_FT");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_ENABLE_FT", &(MPIR_CVAR_ENABLE_FT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ENABLE_FT");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ENABLE_FT = %d\n", MPIR_CVAR_ENABLE_FT);
    }

    defaultval.str = (const char *) "";
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_CHAR,
        MPIR_CVAR_NEMESIS_NETMOD, /* name */
        &MPIR_CVAR_NEMESIS_NETMOD, /* address */
        MPIR_CVAR_MAX_STRLEN, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "NEMESIS", /* category */
        "If non-empty, this cvar specifies which network module should be used for communication. This variable is case-insensitive.");
    tmp_str = defaultval.str;
    got_rc = 0;
    rc = MPL_env2str("MPICH_NEMESIS_NETMOD", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NEMESIS_NETMOD");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_NEMESIS_NETMOD", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NEMESIS_NETMOD");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_NEMESIS_NETMOD", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NEMESIS_NETMOD");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NEMESIS_NETMOD = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        MPIR_CVAR_NEMESIS_NETMOD = MPL_strdup(tmp_str);
        MPIR_CVAR_assert(MPIR_CVAR_NEMESIS_NETMOD);
        if (MPIR_CVAR_NEMESIS_NETMOD == NULL) {
            MPIR_CHKMEM_SETERR(mpi_errno, strlen(tmp_str), "dup of string for MPIR_CVAR_NEMESIS_NETMOD");
            goto fn_fail;
        }
    }
    else {
        MPIR_CVAR_NEMESIS_NETMOD = NULL;
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH3_ENABLE_HCOLL, /* name */
        &MPIR_CVAR_CH3_ENABLE_HCOLL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH3", /* category */
        "If true, enable HCOLL collectives.");
    MPIR_CVAR_CH3_ENABLE_HCOLL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_CH3_ENABLE_HCOLL", &(MPIR_CVAR_CH3_ENABLE_HCOLL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH3_ENABLE_HCOLL");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_CH3_ENABLE_HCOLL", &(MPIR_CVAR_CH3_ENABLE_HCOLL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH3_ENABLE_HCOLL");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_CH3_ENABLE_HCOLL", &(MPIR_CVAR_CH3_ENABLE_HCOLL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH3_ENABLE_HCOLL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH3_ENABLE_HCOLL = %d\n", MPIR_CVAR_CH3_ENABLE_HCOLL);
    }

    defaultval.d = 180;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH3_COMM_CONNECT_TIMEOUT, /* name */
        &MPIR_CVAR_CH3_COMM_CONNECT_TIMEOUT, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_GROUP_EQ,
        defaultval,
        "CH3", /* category */
        "The default time out period in seconds for a connection attempt to the server communicator where the named port exists but no pending accept. User can change the value for a specified connection through its info argument.");
    MPIR_CVAR_CH3_COMM_CONNECT_TIMEOUT = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH3_COMM_CONNECT_TIMEOUT", &(MPIR_CVAR_CH3_COMM_CONNECT_TIMEOUT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH3_COMM_CONNECT_TIMEOUT");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH3_COMM_CONNECT_TIMEOUT", &(MPIR_CVAR_CH3_COMM_CONNECT_TIMEOUT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH3_COMM_CONNECT_TIMEOUT");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH3_COMM_CONNECT_TIMEOUT", &(MPIR_CVAR_CH3_COMM_CONNECT_TIMEOUT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH3_COMM_CONNECT_TIMEOUT");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH3_COMM_CONNECT_TIMEOUT = %d\n", MPIR_CVAR_CH3_COMM_CONNECT_TIMEOUT);
    }

    defaultval.d = 65536;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE, /* name */
        &MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH3", /* category */
        "Specify the threshold of data size of a RMA operation which can be piggybacked with a LOCK message. It is always a positive value and should not be smaller than MPIDI_RMA_IMMED_BYTES. If user sets it as a small value, for middle and large data size, we will lose performance because of always waiting for round-trip of LOCK synchronization; if user sets it as a large value, we need to consume more memory on target side to buffer this lock request when lock is not satisfied.");
    MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE", &(MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE", &(MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE", &(MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE = %d\n", MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE);
    }

    defaultval.d = 65536;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD, /* name */
        &MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH3", /* category */
        "Threshold of number of active requests to trigger blocking waiting in operation routines. When the value is negative, we never blockingly wait in operation routines. When the value is zero, we always trigger blocking waiting in operation routines to wait until no. of active requests becomes zero. When the value is positive, we do blocking waiting in operation routines to wait until no. of active requests being reduced to this value.");
    MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH3_RMA_ACTIVE_REQ_THRESHOLD", &(MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH3_RMA_ACTIVE_REQ_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH3_RMA_ACTIVE_REQ_THRESHOLD", &(MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH3_RMA_ACTIVE_REQ_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD", &(MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD = %d\n", MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD);
    }

    defaultval.d = 128;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD, /* name */
        &MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH3", /* category */
        "Threshold at which the RMA implementation attempts to complete requests while completing RMA operations and while using the lazy synchronization approach.  Change this value if programs fail because they run out of requests or other internal resources");
    MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD", &(MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD", &(MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD", &(MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD = %d\n", MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD);
    }

    defaultval.d = 1024;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM, /* name */
        &MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH3", /* category */
        "Specify the threshold of switching the algorithm used in FENCE from the basic algorithm to the scalable algorithm. The value can be negative, zero or positive. When the number of processes is larger than or equal to this value, FENCE will use a scalable algorithm which do not use O(P) data structure; when the number of processes is smaller than the value, FENCE will use a basic but fast algorithm which requires an O(P) data structure.");
    MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM", &(MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM", &(MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM", &(MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM = %d\n", MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING, /* name */
        &MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH3", /* category */
        "Specify if delay issuing of RMA operations for piggybacking LOCK/UNLOCK/FLUSH is enabled. It can be either 0 or 1. When it is set to 1, the issuing of LOCK message is delayed until origin process see the first RMA operation and piggyback LOCK with that operation, and the origin process always keeps the current last operation until the ending synchronization call in order to piggyback UNLOCK/FLUSH with that operation. When it is set to 0, in WIN_LOCK/UNLOCK case, the LOCK message is sent out as early as possible, in WIN_LOCK_ALL/UNLOCK_ALL case, the origin process still tries to piggyback LOCK message with the first operation; for UNLOCK/FLUSH message, the origin process no longer keeps the current last operation but only piggyback UNLOCK/FLUSH if there is an operation available in the ending synchronization call.");
    MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING", &(MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING", &(MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING", &(MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING = %d\n", MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING);
    }

    defaultval.d = 262144;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH3_RMA_SLOTS_SIZE, /* name */
        &MPIR_CVAR_CH3_RMA_SLOTS_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH3", /* category */
        "Number of RMA slots during window creation. Each slot contains a linked list of target elements. The distribution of ranks among slots follows a round-robin pattern. Requires a positive value.");
    MPIR_CVAR_CH3_RMA_SLOTS_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH3_RMA_SLOTS_SIZE", &(MPIR_CVAR_CH3_RMA_SLOTS_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH3_RMA_SLOTS_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH3_RMA_SLOTS_SIZE", &(MPIR_CVAR_CH3_RMA_SLOTS_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH3_RMA_SLOTS_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH3_RMA_SLOTS_SIZE", &(MPIR_CVAR_CH3_RMA_SLOTS_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH3_RMA_SLOTS_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH3_RMA_SLOTS_SIZE = %d\n", MPIR_CVAR_CH3_RMA_SLOTS_SIZE);
    }

    defaultval.d = 655360;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES, /* name */
        &MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH3", /* category */
        "Size (in bytes) of available lock data this window can provided. If current buffered lock data is more than this value, the process will drop the upcoming operation data. Requires a positive value.");
    MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH3_RMA_TARGET_LOCK_DATA_BYTES", &(MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH3_RMA_TARGET_LOCK_DATA_BYTES");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH3_RMA_TARGET_LOCK_DATA_BYTES", &(MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH3_RMA_TARGET_LOCK_DATA_BYTES");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES", &(MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES = %d\n", MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES);
    }

    defaultval.d = 131072;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE, /* name */
        &MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH3", /* category */
        "This cvar controls the message size at which CH3 switches from eager to rendezvous mode.");
    MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH3_EAGER_MAX_MSG_SIZE", &(MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH3_EAGER_MAX_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH3_EAGER_MAX_MSG_SIZE", &(MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH3_EAGER_MAX_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE", &(MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE = %d\n", MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH3_PG_VERBOSE, /* name */
        &MPIR_CVAR_CH3_PG_VERBOSE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_GROUP_EQ,
        defaultval,
        "CH3", /* category */
        "If set, print the PG state on finalize.");
    MPIR_CVAR_CH3_PG_VERBOSE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_CH3_PG_VERBOSE", &(MPIR_CVAR_CH3_PG_VERBOSE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH3_PG_VERBOSE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_CH3_PG_VERBOSE", &(MPIR_CVAR_CH3_PG_VERBOSE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH3_PG_VERBOSE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_CH3_PG_VERBOSE", &(MPIR_CVAR_CH3_PG_VERBOSE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH3_PG_VERBOSE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH3_PG_VERBOSE = %d\n", MPIR_CVAR_CH3_PG_VERBOSE);
    }

    defaultval.d = 256;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE, /* name */
        &MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH3", /* category */
        "Size of the window-private RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediately.  Requires a positive value.");
    MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH3_RMA_OP_WIN_POOL_SIZE", &(MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH3_RMA_OP_WIN_POOL_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH3_RMA_OP_WIN_POOL_SIZE", &(MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH3_RMA_OP_WIN_POOL_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE", &(MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE = %d\n", MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE);
    }

    defaultval.d = 16384;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE, /* name */
        &MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH3", /* category */
        "Size of the Global RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediately.  Requires a positive value.");
    MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH3_RMA_OP_GLOBAL_POOL_SIZE", &(MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH3_RMA_OP_GLOBAL_POOL_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH3_RMA_OP_GLOBAL_POOL_SIZE", &(MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH3_RMA_OP_GLOBAL_POOL_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE", &(MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE = %d\n", MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE);
    }

    defaultval.d = 256;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE, /* name */
        &MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH3", /* category */
        "Size of the window-private RMA target pool (in number of targets) that stores information about RMA targets that could not be issued immediately.  Requires a positive value.");
    MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH3_RMA_TARGET_WIN_POOL_SIZE", &(MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH3_RMA_TARGET_WIN_POOL_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH3_RMA_TARGET_WIN_POOL_SIZE", &(MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH3_RMA_TARGET_WIN_POOL_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE", &(MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE = %d\n", MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE);
    }

    defaultval.d = 16384;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE, /* name */
        &MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH3", /* category */
        "Size of the Global RMA targets pool (in number of targets) that stores information about RMA targets that could not be issued immediately.  Requires a positive value.");
    MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH3_RMA_TARGET_GLOBAL_POOL_SIZE", &(MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH3_RMA_TARGET_GLOBAL_POOL_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH3_RMA_TARGET_GLOBAL_POOL_SIZE", &(MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH3_RMA_TARGET_GLOBAL_POOL_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE", &(MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE = %d\n", MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE);
    }

    defaultval.d = 256;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE, /* name */
        &MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH3", /* category */
        "Size of the window-private RMA lock entries pool (in number of lock entries) that stores information about RMA lock requests that could not be satisfied immediately.  Requires a positive value.");
    MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE", &(MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE", &(MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE", &(MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE = %d\n", MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE);
    }

    defaultval.str = (const char *) NULL;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_CHAR,
        MPIR_CVAR_OFI_USE_PROVIDER, /* name */
        &MPIR_CVAR_OFI_USE_PROVIDER, /* address */
        MPIR_CVAR_MAX_STRLEN, /* count */
        MPI_T_VERBOSITY_MPIDEV_DETAIL,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "DEVELOPER", /* category */
        "This variable is no longer supported. Use FI_PROVIDER instead to select libfabric providers.");
    tmp_str = defaultval.str;
    got_rc = 0;
    rc = MPL_env2str("MPICH_OFI_USE_PROVIDER", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_OFI_USE_PROVIDER");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_OFI_USE_PROVIDER", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_OFI_USE_PROVIDER");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_OFI_USE_PROVIDER", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_OFI_USE_PROVIDER");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_OFI_USE_PROVIDER = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        MPIR_CVAR_OFI_USE_PROVIDER = MPL_strdup(tmp_str);
        MPIR_CVAR_assert(MPIR_CVAR_OFI_USE_PROVIDER);
        if (MPIR_CVAR_OFI_USE_PROVIDER == NULL) {
            MPIR_CHKMEM_SETERR(mpi_errno, strlen(tmp_str), "dup of string for MPIR_CVAR_OFI_USE_PROVIDER");
            goto fn_fail;
        }
    }
    else {
        MPIR_CVAR_OFI_USE_PROVIDER = NULL;
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_SINGLE_HOST_ENABLED, /* name */
        &MPIR_CVAR_SINGLE_HOST_ENABLED, /* address */
        1, /* count */
        MPI_T_VERBOSITY_MPIDEV_DETAIL,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "DEVELOPER", /* category */
        "Set this variable to true to indicate that processes are launched on a single host. The current implication is to avoid the cxi provider to prevent the use of scarce hardware resources.");
    MPIR_CVAR_SINGLE_HOST_ENABLED = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_SINGLE_HOST_ENABLED", &(MPIR_CVAR_SINGLE_HOST_ENABLED));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_SINGLE_HOST_ENABLED");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_SINGLE_HOST_ENABLED", &(MPIR_CVAR_SINGLE_HOST_ENABLED));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_SINGLE_HOST_ENABLED");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_SINGLE_HOST_ENABLED", &(MPIR_CVAR_SINGLE_HOST_ENABLED));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_SINGLE_HOST_ENABLED");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_SINGLE_HOST_ENABLED = %d\n", MPIR_CVAR_SINGLE_HOST_ENABLED);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_AM_LONG_FORCE_PIPELINE, /* name */
        &MPIR_CVAR_CH4_OFI_AM_LONG_FORCE_PIPELINE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "DEVELOPER", /* category */
        "For long message to be sent using pipeline rather than default RDMA read.");
    MPIR_CVAR_CH4_OFI_AM_LONG_FORCE_PIPELINE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_CH4_OFI_AM_LONG_FORCE_PIPELINE", &(MPIR_CVAR_CH4_OFI_AM_LONG_FORCE_PIPELINE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_AM_LONG_FORCE_PIPELINE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_CH4_OFI_AM_LONG_FORCE_PIPELINE", &(MPIR_CVAR_CH4_OFI_AM_LONG_FORCE_PIPELINE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_AM_LONG_FORCE_PIPELINE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_CH4_OFI_AM_LONG_FORCE_PIPELINE", &(MPIR_CVAR_CH4_OFI_AM_LONG_FORCE_PIPELINE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_AM_LONG_FORCE_PIPELINE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_AM_LONG_FORCE_PIPELINE = %d\n", MPIR_CVAR_CH4_OFI_AM_LONG_FORCE_PIPELINE);
    }

    defaultval.d = MPIR_CVAR_BCAST_OFI_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BCAST_OFI_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_BCAST_OFI_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select algorithm for intra-node bcast\n"
"mpir                        - Fallback to MPIR collectives\n"
"trigger_tree_tagged         - Force triggered ops based Tagged Tree\n"
"trigger_tree_rma            - Force triggered ops based RMA Tree\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_OFI_COLL_SELECTION_TUNING_JSON_FILE)");
    MPIR_CVAR_BCAST_OFI_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_BCAST_OFI_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BCAST_OFI_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_BCAST_OFI_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BCAST_OFI_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_BCAST_OFI_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BCAST_OFI_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BCAST_OFI_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "mpir"))
            MPIR_CVAR_BCAST_OFI_INTRA_ALGORITHM = MPIR_CVAR_BCAST_OFI_INTRA_ALGORITHM_mpir;
        else if (0 == strcmp(tmp_str, "trigger_tree_tagged"))
            MPIR_CVAR_BCAST_OFI_INTRA_ALGORITHM = MPIR_CVAR_BCAST_OFI_INTRA_ALGORITHM_trigger_tree_tagged;
        else if (0 == strcmp(tmp_str, "trigger_tree_rma"))
            MPIR_CVAR_BCAST_OFI_INTRA_ALGORITHM = MPIR_CVAR_BCAST_OFI_INTRA_ALGORITHM_trigger_tree_rma;
        else if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_BCAST_OFI_INTRA_ALGORITHM = MPIR_CVAR_BCAST_OFI_INTRA_ALGORITHM_auto;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_BCAST_OFI_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_CH4_OFI_GPU_RECEIVE_ENGINE_TYPE_copy_low_latency;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_GPU_RECEIVE_ENGINE_TYPE, /* name */
        &MPIR_CVAR_CH4_OFI_GPU_RECEIVE_ENGINE_TYPE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "Specifies GPU engine type for GPU pt2pt on the receiver side.\n"
"compute - use a compute engine\n"
"copy_high_bandwidth - use a high-bandwidth copy engine\n"
"copy_low_latency - use a low-latency copy engine\n"
"yaksa - use Yaksa");
    MPIR_CVAR_CH4_OFI_GPU_RECEIVE_ENGINE_TYPE = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_CH4_OFI_GPU_RECEIVE_ENGINE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_GPU_RECEIVE_ENGINE_TYPE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_CH4_OFI_GPU_RECEIVE_ENGINE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_GPU_RECEIVE_ENGINE_TYPE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_CH4_OFI_GPU_RECEIVE_ENGINE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_GPU_RECEIVE_ENGINE_TYPE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_GPU_RECEIVE_ENGINE_TYPE = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "compute"))
            MPIR_CVAR_CH4_OFI_GPU_RECEIVE_ENGINE_TYPE = MPIR_CVAR_CH4_OFI_GPU_RECEIVE_ENGINE_TYPE_compute;
        else if (0 == strcmp(tmp_str, "copy_high_bandwidth"))
            MPIR_CVAR_CH4_OFI_GPU_RECEIVE_ENGINE_TYPE = MPIR_CVAR_CH4_OFI_GPU_RECEIVE_ENGINE_TYPE_copy_high_bandwidth;
        else if (0 == strcmp(tmp_str, "copy_low_latency"))
            MPIR_CVAR_CH4_OFI_GPU_RECEIVE_ENGINE_TYPE = MPIR_CVAR_CH4_OFI_GPU_RECEIVE_ENGINE_TYPE_copy_low_latency;
        else if (0 == strcmp(tmp_str, "yaksa"))
            MPIR_CVAR_CH4_OFI_GPU_RECEIVE_ENGINE_TYPE = MPIR_CVAR_CH4_OFI_GPU_RECEIVE_ENGINE_TYPE_yaksa;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_CH4_OFI_GPU_RECEIVE_ENGINE_TYPE", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_OFI_SKIP_IPV6, /* name */
        &MPIR_CVAR_OFI_SKIP_IPV6, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "DEVELOPER", /* category */
        "Skip IPv6 providers.");
    MPIR_CVAR_OFI_SKIP_IPV6 = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_OFI_SKIP_IPV6", &(MPIR_CVAR_OFI_SKIP_IPV6));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_OFI_SKIP_IPV6");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_OFI_SKIP_IPV6", &(MPIR_CVAR_OFI_SKIP_IPV6));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_OFI_SKIP_IPV6");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_OFI_SKIP_IPV6", &(MPIR_CVAR_OFI_SKIP_IPV6));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_OFI_SKIP_IPV6");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_OFI_SKIP_IPV6 = %d\n", MPIR_CVAR_OFI_SKIP_IPV6);
    }

#if defined MPID_CH4_OFI_ENABLE_DATA
    defaultval.d = MPID_CH4_OFI_ENABLE_DATA;
#else
    defaultval.d = -1;
#endif /* MPID_CH4_OFI_ENABLE_DATA */

    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_ENABLE_DATA, /* name */
        &MPIR_CVAR_CH4_OFI_ENABLE_DATA, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "Enable immediate data fields in OFI to transmit source rank outside of the match bits");
    MPIR_CVAR_CH4_OFI_ENABLE_DATA = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_ENABLE_DATA", &(MPIR_CVAR_CH4_OFI_ENABLE_DATA));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_ENABLE_DATA");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_ENABLE_DATA", &(MPIR_CVAR_CH4_OFI_ENABLE_DATA));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_ENABLE_DATA");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_ENABLE_DATA", &(MPIR_CVAR_CH4_OFI_ENABLE_DATA));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_ENABLE_DATA");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_ENABLE_DATA = %d\n", MPIR_CVAR_CH4_OFI_ENABLE_DATA);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE, /* name */
        &MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "If true, the OFI addressing information will be stored with an FI_AV_TABLE. If false, an FI_AV_MAP will be used.");
    MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_ENABLE_AV_TABLE", &(MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_ENABLE_AV_TABLE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_ENABLE_AV_TABLE", &(MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_ENABLE_AV_TABLE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE", &(MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE = %d\n", MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_ENABLE_SHARED_AV, /* name */
        &MPIR_CVAR_CH4_OFI_ENABLE_SHARED_AV, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "If true, it will try open shared av at initialization.");
    MPIR_CVAR_CH4_OFI_ENABLE_SHARED_AV = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_CH4_OFI_ENABLE_SHARED_AV", &(MPIR_CVAR_CH4_OFI_ENABLE_SHARED_AV));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_ENABLE_SHARED_AV");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_CH4_OFI_ENABLE_SHARED_AV", &(MPIR_CVAR_CH4_OFI_ENABLE_SHARED_AV));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_ENABLE_SHARED_AV");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_CH4_OFI_ENABLE_SHARED_AV", &(MPIR_CVAR_CH4_OFI_ENABLE_SHARED_AV));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_ENABLE_SHARED_AV");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_ENABLE_SHARED_AV = %d\n", MPIR_CVAR_CH4_OFI_ENABLE_SHARED_AV);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS, /* name */
        &MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "If true, use OFI scalable endpoints.");
    MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS", &(MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS", &(MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS", &(MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS = %d\n", MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_ENABLE_SHARED_CONTEXTS, /* name */
        &MPIR_CVAR_CH4_OFI_ENABLE_SHARED_CONTEXTS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "If set to false (zero), MPICH does not use OFI shared contexts. If set to -1, it is determined by the OFI capability sets based on the provider. Otherwise, MPICH tries to use OFI shared contexts. If they are unavailable, it'll fall back to the mode without shared contexts.");
    MPIR_CVAR_CH4_OFI_ENABLE_SHARED_CONTEXTS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_ENABLE_SHARED_CONTEXTS", &(MPIR_CVAR_CH4_OFI_ENABLE_SHARED_CONTEXTS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_ENABLE_SHARED_CONTEXTS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_ENABLE_SHARED_CONTEXTS", &(MPIR_CVAR_CH4_OFI_ENABLE_SHARED_CONTEXTS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_ENABLE_SHARED_CONTEXTS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_ENABLE_SHARED_CONTEXTS", &(MPIR_CVAR_CH4_OFI_ENABLE_SHARED_CONTEXTS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_ENABLE_SHARED_CONTEXTS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_ENABLE_SHARED_CONTEXTS = %d\n", MPIR_CVAR_CH4_OFI_ENABLE_SHARED_CONTEXTS);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_ENABLE_MR_VIRT_ADDRESS, /* name */
        &MPIR_CVAR_CH4_OFI_ENABLE_MR_VIRT_ADDRESS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "If true, enable virtual addressing for OFI memory regions. This variable is only meaningful for OFI versions 1.5+. It is equivalent to using FI_MR_BASIC in versions of OFI older than 1.5.");
    MPIR_CVAR_CH4_OFI_ENABLE_MR_VIRT_ADDRESS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_ENABLE_MR_VIRT_ADDRESS", &(MPIR_CVAR_CH4_OFI_ENABLE_MR_VIRT_ADDRESS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_ENABLE_MR_VIRT_ADDRESS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_ENABLE_MR_VIRT_ADDRESS", &(MPIR_CVAR_CH4_OFI_ENABLE_MR_VIRT_ADDRESS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_ENABLE_MR_VIRT_ADDRESS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_ENABLE_MR_VIRT_ADDRESS", &(MPIR_CVAR_CH4_OFI_ENABLE_MR_VIRT_ADDRESS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_ENABLE_MR_VIRT_ADDRESS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_ENABLE_MR_VIRT_ADDRESS = %d\n", MPIR_CVAR_CH4_OFI_ENABLE_MR_VIRT_ADDRESS);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_ENABLE_MR_ALLOCATED, /* name */
        &MPIR_CVAR_CH4_OFI_ENABLE_MR_ALLOCATED, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "If true, require all OFI memory regions must be backed by physical memory pages at the time the registration call is made. This variable is only meaningful for OFI versions 1.5+. It is equivalent to using FI_MR_BASIC in versions of OFI older than 1.5.");
    MPIR_CVAR_CH4_OFI_ENABLE_MR_ALLOCATED = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_ENABLE_MR_ALLOCATED", &(MPIR_CVAR_CH4_OFI_ENABLE_MR_ALLOCATED));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_ENABLE_MR_ALLOCATED");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_ENABLE_MR_ALLOCATED", &(MPIR_CVAR_CH4_OFI_ENABLE_MR_ALLOCATED));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_ENABLE_MR_ALLOCATED");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_ENABLE_MR_ALLOCATED", &(MPIR_CVAR_CH4_OFI_ENABLE_MR_ALLOCATED));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_ENABLE_MR_ALLOCATED");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_ENABLE_MR_ALLOCATED = %d\n", MPIR_CVAR_CH4_OFI_ENABLE_MR_ALLOCATED);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_ENABLE_MR_REGISTER_NULL, /* name */
        &MPIR_CVAR_CH4_OFI_ENABLE_MR_REGISTER_NULL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "If true, memory registration call supports registering with NULL addresses.");
    MPIR_CVAR_CH4_OFI_ENABLE_MR_REGISTER_NULL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_ENABLE_MR_REGISTER_NULL", &(MPIR_CVAR_CH4_OFI_ENABLE_MR_REGISTER_NULL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_ENABLE_MR_REGISTER_NULL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_ENABLE_MR_REGISTER_NULL", &(MPIR_CVAR_CH4_OFI_ENABLE_MR_REGISTER_NULL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_ENABLE_MR_REGISTER_NULL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_ENABLE_MR_REGISTER_NULL", &(MPIR_CVAR_CH4_OFI_ENABLE_MR_REGISTER_NULL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_ENABLE_MR_REGISTER_NULL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_ENABLE_MR_REGISTER_NULL = %d\n", MPIR_CVAR_CH4_OFI_ENABLE_MR_REGISTER_NULL);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_ENABLE_MR_PROV_KEY, /* name */
        &MPIR_CVAR_CH4_OFI_ENABLE_MR_PROV_KEY, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "If true, enable provider supplied key for OFI memory regions. This variable is only meaningful for OFI versions 1.5+. It is equivalent to using FI_MR_BASIC in versions of OFI older than 1.5.");
    MPIR_CVAR_CH4_OFI_ENABLE_MR_PROV_KEY = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_ENABLE_MR_PROV_KEY", &(MPIR_CVAR_CH4_OFI_ENABLE_MR_PROV_KEY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_ENABLE_MR_PROV_KEY");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_ENABLE_MR_PROV_KEY", &(MPIR_CVAR_CH4_OFI_ENABLE_MR_PROV_KEY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_ENABLE_MR_PROV_KEY");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_ENABLE_MR_PROV_KEY", &(MPIR_CVAR_CH4_OFI_ENABLE_MR_PROV_KEY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_ENABLE_MR_PROV_KEY");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_ENABLE_MR_PROV_KEY = %d\n", MPIR_CVAR_CH4_OFI_ENABLE_MR_PROV_KEY);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_ENABLE_TAGGED, /* name */
        &MPIR_CVAR_CH4_OFI_ENABLE_TAGGED, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "If true, use tagged message transmission functions in OFI.");
    MPIR_CVAR_CH4_OFI_ENABLE_TAGGED = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_ENABLE_TAGGED", &(MPIR_CVAR_CH4_OFI_ENABLE_TAGGED));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_ENABLE_TAGGED");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_ENABLE_TAGGED", &(MPIR_CVAR_CH4_OFI_ENABLE_TAGGED));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_ENABLE_TAGGED");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_ENABLE_TAGGED", &(MPIR_CVAR_CH4_OFI_ENABLE_TAGGED));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_ENABLE_TAGGED");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_ENABLE_TAGGED = %d\n", MPIR_CVAR_CH4_OFI_ENABLE_TAGGED);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_ENABLE_AM, /* name */
        &MPIR_CVAR_CH4_OFI_ENABLE_AM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "If true, enable OFI active message support.");
    MPIR_CVAR_CH4_OFI_ENABLE_AM = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_ENABLE_AM", &(MPIR_CVAR_CH4_OFI_ENABLE_AM));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_ENABLE_AM");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_ENABLE_AM", &(MPIR_CVAR_CH4_OFI_ENABLE_AM));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_ENABLE_AM");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_ENABLE_AM", &(MPIR_CVAR_CH4_OFI_ENABLE_AM));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_ENABLE_AM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_ENABLE_AM = %d\n", MPIR_CVAR_CH4_OFI_ENABLE_AM);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_ENABLE_RMA, /* name */
        &MPIR_CVAR_CH4_OFI_ENABLE_RMA, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "If true, enable OFI RMA support for MPI RMA operations. OFI support for basic RMA is always required to implement large messgage transfers in the active message code path.");
    MPIR_CVAR_CH4_OFI_ENABLE_RMA = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_ENABLE_RMA", &(MPIR_CVAR_CH4_OFI_ENABLE_RMA));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_ENABLE_RMA");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_ENABLE_RMA", &(MPIR_CVAR_CH4_OFI_ENABLE_RMA));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_ENABLE_RMA");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_ENABLE_RMA", &(MPIR_CVAR_CH4_OFI_ENABLE_RMA));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_ENABLE_RMA");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_ENABLE_RMA = %d\n", MPIR_CVAR_CH4_OFI_ENABLE_RMA);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS, /* name */
        &MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "If true, enable OFI Atomics support.");
    MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_ENABLE_ATOMICS", &(MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_ENABLE_ATOMICS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_ENABLE_ATOMICS", &(MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_ENABLE_ATOMICS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS", &(MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS = %d\n", MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS, /* name */
        &MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "Specifies the maximum number of iovecs that can be used by the OFI provider for fetch_atomic operations. The default value is -1, indicating that no value is set.");
    MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_FETCH_ATOMIC_IOVECS", &(MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_FETCH_ATOMIC_IOVECS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_FETCH_ATOMIC_IOVECS", &(MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_FETCH_ATOMIC_IOVECS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS", &(MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS = %d\n", MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS, /* name */
        &MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "If true, enable MPI data auto progress.");
    MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS", &(MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS", &(MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS", &(MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS = %d\n", MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS, /* name */
        &MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "If true, enable MPI control auto progress.");
    MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS", &(MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS", &(MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS", &(MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS = %d\n", MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK, /* name */
        &MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "If true, enable iovec for pt2pt.");
    MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_ENABLE_PT2PT_NOPACK", &(MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_ENABLE_PT2PT_NOPACK");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_ENABLE_PT2PT_NOPACK", &(MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_ENABLE_PT2PT_NOPACK");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK", &(MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK = %d\n", MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK);
    }

#if defined MPID_CH4_OFI_ENABLE_HMEM
    defaultval.d = MPID_CH4_OFI_ENABLE_HMEM;
#else
    defaultval.d = 0;
#endif /* MPID_CH4_OFI_ENABLE_HMEM */

    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_ENABLE_HMEM, /* name */
        &MPIR_CVAR_CH4_OFI_ENABLE_HMEM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "If true, uses GPU direct RDMA support in the provider.");
    MPIR_CVAR_CH4_OFI_ENABLE_HMEM = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_ENABLE_HMEM", &(MPIR_CVAR_CH4_OFI_ENABLE_HMEM));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_ENABLE_HMEM");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_ENABLE_HMEM", &(MPIR_CVAR_CH4_OFI_ENABLE_HMEM));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_ENABLE_HMEM");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_ENABLE_HMEM", &(MPIR_CVAR_CH4_OFI_ENABLE_HMEM));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_ENABLE_HMEM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_ENABLE_HMEM = %d\n", MPIR_CVAR_CH4_OFI_ENABLE_HMEM);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_ENABLE_MR_HMEM, /* name */
        &MPIR_CVAR_CH4_OFI_ENABLE_MR_HMEM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "If true, need to register the buffer to use GPU direct RDMA.");
    MPIR_CVAR_CH4_OFI_ENABLE_MR_HMEM = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_ENABLE_MR_HMEM", &(MPIR_CVAR_CH4_OFI_ENABLE_MR_HMEM));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_ENABLE_MR_HMEM");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_ENABLE_MR_HMEM", &(MPIR_CVAR_CH4_OFI_ENABLE_MR_HMEM));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_ENABLE_MR_HMEM");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_ENABLE_MR_HMEM", &(MPIR_CVAR_CH4_OFI_ENABLE_MR_HMEM));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_ENABLE_MR_HMEM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_ENABLE_MR_HMEM = %d\n", MPIR_CVAR_CH4_OFI_ENABLE_MR_HMEM);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_GPU_RDMA_THRESHOLD, /* name */
        &MPIR_CVAR_CH4_OFI_GPU_RDMA_THRESHOLD, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "The threshold to start using GPU direct RDMA.");
    MPIR_CVAR_CH4_OFI_GPU_RDMA_THRESHOLD = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_GPU_RDMA_THRESHOLD", &(MPIR_CVAR_CH4_OFI_GPU_RDMA_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_GPU_RDMA_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_GPU_RDMA_THRESHOLD", &(MPIR_CVAR_CH4_OFI_GPU_RDMA_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_GPU_RDMA_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_GPU_RDMA_THRESHOLD", &(MPIR_CVAR_CH4_OFI_GPU_RDMA_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_GPU_RDMA_THRESHOLD");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_GPU_RDMA_THRESHOLD = %d\n", MPIR_CVAR_CH4_OFI_GPU_RDMA_THRESHOLD);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_CONTEXT_ID_BITS, /* name */
        &MPIR_CVAR_CH4_OFI_CONTEXT_ID_BITS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "Specifies the number of bits that will be used for matching the context ID. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.");
    MPIR_CVAR_CH4_OFI_CONTEXT_ID_BITS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_CONTEXT_ID_BITS", &(MPIR_CVAR_CH4_OFI_CONTEXT_ID_BITS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_CONTEXT_ID_BITS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_CONTEXT_ID_BITS", &(MPIR_CVAR_CH4_OFI_CONTEXT_ID_BITS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_CONTEXT_ID_BITS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_CONTEXT_ID_BITS", &(MPIR_CVAR_CH4_OFI_CONTEXT_ID_BITS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_CONTEXT_ID_BITS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_CONTEXT_ID_BITS = %d\n", MPIR_CVAR_CH4_OFI_CONTEXT_ID_BITS);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_RANK_BITS, /* name */
        &MPIR_CVAR_CH4_OFI_RANK_BITS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "Specifies the number of bits that will be used for matching the MPI rank. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.");
    MPIR_CVAR_CH4_OFI_RANK_BITS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_RANK_BITS", &(MPIR_CVAR_CH4_OFI_RANK_BITS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_RANK_BITS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_RANK_BITS", &(MPIR_CVAR_CH4_OFI_RANK_BITS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_RANK_BITS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_RANK_BITS", &(MPIR_CVAR_CH4_OFI_RANK_BITS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_RANK_BITS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_RANK_BITS = %d\n", MPIR_CVAR_CH4_OFI_RANK_BITS);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_TAG_BITS, /* name */
        &MPIR_CVAR_CH4_OFI_TAG_BITS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "Specifies the number of bits that will be used for matching the user tag. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.");
    MPIR_CVAR_CH4_OFI_TAG_BITS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_TAG_BITS", &(MPIR_CVAR_CH4_OFI_TAG_BITS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_TAG_BITS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_TAG_BITS", &(MPIR_CVAR_CH4_OFI_TAG_BITS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_TAG_BITS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_TAG_BITS", &(MPIR_CVAR_CH4_OFI_TAG_BITS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_TAG_BITS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_TAG_BITS = %d\n", MPIR_CVAR_CH4_OFI_TAG_BITS);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_MAJOR_VERSION, /* name */
        &MPIR_CVAR_CH4_OFI_MAJOR_VERSION, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "Specifies the major version of the OFI library. The default is the major version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.");
    MPIR_CVAR_CH4_OFI_MAJOR_VERSION = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_MAJOR_VERSION", &(MPIR_CVAR_CH4_OFI_MAJOR_VERSION));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_MAJOR_VERSION");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_MAJOR_VERSION", &(MPIR_CVAR_CH4_OFI_MAJOR_VERSION));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_MAJOR_VERSION");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_MAJOR_VERSION", &(MPIR_CVAR_CH4_OFI_MAJOR_VERSION));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_MAJOR_VERSION");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_MAJOR_VERSION = %d\n", MPIR_CVAR_CH4_OFI_MAJOR_VERSION);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_MINOR_VERSION, /* name */
        &MPIR_CVAR_CH4_OFI_MINOR_VERSION, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "Specifies the major version of the OFI library. The default is the minor version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.");
    MPIR_CVAR_CH4_OFI_MINOR_VERSION = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_MINOR_VERSION", &(MPIR_CVAR_CH4_OFI_MINOR_VERSION));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_MINOR_VERSION");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_MINOR_VERSION", &(MPIR_CVAR_CH4_OFI_MINOR_VERSION));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_MINOR_VERSION");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_MINOR_VERSION", &(MPIR_CVAR_CH4_OFI_MINOR_VERSION));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_MINOR_VERSION");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_MINOR_VERSION = %d\n", MPIR_CVAR_CH4_OFI_MINOR_VERSION);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_MAX_RMA_SEP_CTX, /* name */
        &MPIR_CVAR_CH4_OFI_MAX_RMA_SEP_CTX, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "If set to positive, this CVAR specifies the maximum number of transmit contexts RMA can utilize in a scalable endpoint. This value is effective only when scalable endpoint is available, otherwise it will be ignored.");
    MPIR_CVAR_CH4_OFI_MAX_RMA_SEP_CTX = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_MAX_RMA_SEP_CTX", &(MPIR_CVAR_CH4_OFI_MAX_RMA_SEP_CTX));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_MAX_RMA_SEP_CTX");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_MAX_RMA_SEP_CTX", &(MPIR_CVAR_CH4_OFI_MAX_RMA_SEP_CTX));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_MAX_RMA_SEP_CTX");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_MAX_RMA_SEP_CTX", &(MPIR_CVAR_CH4_OFI_MAX_RMA_SEP_CTX));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_MAX_RMA_SEP_CTX");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_MAX_RMA_SEP_CTX = %d\n", MPIR_CVAR_CH4_OFI_MAX_RMA_SEP_CTX);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_MAX_EAGAIN_RETRY, /* name */
        &MPIR_CVAR_CH4_OFI_MAX_EAGAIN_RETRY, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "If set to positive, this CVAR specifies the maximum number of retries of an ofi operations before returning MPIX_ERR_EAGAIN. This value is effective only when the communicator has the MPI_OFI_set_eagain info hint set to true.");
    MPIR_CVAR_CH4_OFI_MAX_EAGAIN_RETRY = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_MAX_EAGAIN_RETRY", &(MPIR_CVAR_CH4_OFI_MAX_EAGAIN_RETRY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_MAX_EAGAIN_RETRY");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_MAX_EAGAIN_RETRY", &(MPIR_CVAR_CH4_OFI_MAX_EAGAIN_RETRY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_MAX_EAGAIN_RETRY");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_MAX_EAGAIN_RETRY", &(MPIR_CVAR_CH4_OFI_MAX_EAGAIN_RETRY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_MAX_EAGAIN_RETRY");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_MAX_EAGAIN_RETRY = %d\n", MPIR_CVAR_CH4_OFI_MAX_EAGAIN_RETRY);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS, /* name */
        &MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "Specifies the number of buffers for receiving active messages.");
    MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_NUM_AM_BUFFERS", &(MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_NUM_AM_BUFFERS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_NUM_AM_BUFFERS", &(MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_NUM_AM_BUFFERS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS", &(MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS = %d\n", MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_NUM_OPTIMIZED_MEMORY_REGIONS, /* name */
        &MPIR_CVAR_CH4_OFI_NUM_OPTIMIZED_MEMORY_REGIONS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "Specifies the number of optimized memory regions supported by the provider. An optimized memory region is used for lower-overhead, unordered RMA operations. It uses a low-overhead RX path and additionally, a low-overhead packet format may be used to target an optimized memory region.");
    MPIR_CVAR_CH4_OFI_NUM_OPTIMIZED_MEMORY_REGIONS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_NUM_OPTIMIZED_MEMORY_REGIONS", &(MPIR_CVAR_CH4_OFI_NUM_OPTIMIZED_MEMORY_REGIONS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_NUM_OPTIMIZED_MEMORY_REGIONS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_NUM_OPTIMIZED_MEMORY_REGIONS", &(MPIR_CVAR_CH4_OFI_NUM_OPTIMIZED_MEMORY_REGIONS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_NUM_OPTIMIZED_MEMORY_REGIONS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_NUM_OPTIMIZED_MEMORY_REGIONS", &(MPIR_CVAR_CH4_OFI_NUM_OPTIMIZED_MEMORY_REGIONS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_NUM_OPTIMIZED_MEMORY_REGIONS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_NUM_OPTIMIZED_MEMORY_REGIONS = %d\n", MPIR_CVAR_CH4_OFI_NUM_OPTIMIZED_MEMORY_REGIONS);
    }

    defaultval.d = 100;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_RMA_PROGRESS_INTERVAL, /* name */
        &MPIR_CVAR_CH4_OFI_RMA_PROGRESS_INTERVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "Specifies the interval for manually flushing RMA operations when automatic progress is not enabled. It the underlying OFI provider supports auto data progress, this value is ignored. If the value is -1, this optimization will be turned off.");
    MPIR_CVAR_CH4_OFI_RMA_PROGRESS_INTERVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_RMA_PROGRESS_INTERVAL", &(MPIR_CVAR_CH4_OFI_RMA_PROGRESS_INTERVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_RMA_PROGRESS_INTERVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_RMA_PROGRESS_INTERVAL", &(MPIR_CVAR_CH4_OFI_RMA_PROGRESS_INTERVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_RMA_PROGRESS_INTERVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_RMA_PROGRESS_INTERVAL", &(MPIR_CVAR_CH4_OFI_RMA_PROGRESS_INTERVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_RMA_PROGRESS_INTERVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_RMA_PROGRESS_INTERVAL = %d\n", MPIR_CVAR_CH4_OFI_RMA_PROGRESS_INTERVAL);
    }

    defaultval.d = 16384;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_RMA_IOVEC_MAX, /* name */
        &MPIR_CVAR_CH4_OFI_RMA_IOVEC_MAX, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "Specifies the maximum number of iovecs to allocate for RMA operations to/from noncontiguous buffers.");
    MPIR_CVAR_CH4_OFI_RMA_IOVEC_MAX = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_RMA_IOVEC_MAX", &(MPIR_CVAR_CH4_OFI_RMA_IOVEC_MAX));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_RMA_IOVEC_MAX");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_RMA_IOVEC_MAX", &(MPIR_CVAR_CH4_OFI_RMA_IOVEC_MAX));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_RMA_IOVEC_MAX");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_RMA_IOVEC_MAX", &(MPIR_CVAR_CH4_OFI_RMA_IOVEC_MAX));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_RMA_IOVEC_MAX");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_RMA_IOVEC_MAX = %d\n", MPIR_CVAR_CH4_OFI_RMA_IOVEC_MAX);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_EAGER_MAX_MSG_SIZE, /* name */
        &MPIR_CVAR_CH4_OFI_EAGER_MAX_MSG_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "This cvar controls the message size at which OFI native path switches from eager to rendezvous mode. It does not affect the AM path eager limit. Having this gives a way to reliably test native non-path. If the number is positive, OFI will init the MPIDI_OFI_global.max_msg_size to the value of cvar. If the number is negative, OFI will init the MPIDI_OFI_globa.max_msg_size using whatever provider gives (which might be unlimited for socket provider).");
    MPIR_CVAR_CH4_OFI_EAGER_MAX_MSG_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_EAGER_MAX_MSG_SIZE", &(MPIR_CVAR_CH4_OFI_EAGER_MAX_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_EAGER_MAX_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_EAGER_MAX_MSG_SIZE", &(MPIR_CVAR_CH4_OFI_EAGER_MAX_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_EAGER_MAX_MSG_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_EAGER_MAX_MSG_SIZE", &(MPIR_CVAR_CH4_OFI_EAGER_MAX_MSG_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_EAGER_MAX_MSG_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_EAGER_MAX_MSG_SIZE = %d\n", MPIR_CVAR_CH4_OFI_EAGER_MAX_MSG_SIZE);
    }

#if defined MPID_CH4_OFI_MAX_NICS
    defaultval.d = MPID_CH4_OFI_MAX_NICS;
#else
    defaultval.d = -1;
#endif /* MPID_CH4_OFI_MAX_NICS */

    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_MAX_NICS, /* name */
        &MPIR_CVAR_CH4_OFI_MAX_NICS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4", /* category */
        "If set to positive number, this cvar determines the maximum number of physical nics to use (if more than one is available). If the number is -1, underlying netmod or shmmod automatically uses an optimal number depending on what is detected on the system up to the limit determined by MPIDI_MAX_NICS (in ofi_types.h).");
    MPIR_CVAR_CH4_OFI_MAX_NICS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_MAX_NICS", &(MPIR_CVAR_CH4_OFI_MAX_NICS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_MAX_NICS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_MAX_NICS", &(MPIR_CVAR_CH4_OFI_MAX_NICS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_MAX_NICS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_MAX_NICS", &(MPIR_CVAR_CH4_OFI_MAX_NICS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_MAX_NICS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_MAX_NICS = %d\n", MPIR_CVAR_CH4_OFI_MAX_NICS);
    }

#if defined MPID_CH4_OFI_ENABLE_MULTI_NIC_STRIPING
    defaultval.d = MPID_CH4_OFI_ENABLE_MULTI_NIC_STRIPING;
#else
    defaultval.d = 0;
#endif /* MPID_CH4_OFI_ENABLE_MULTI_NIC_STRIPING */

    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_STRIPING, /* name */
        &MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_STRIPING, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4", /* category */
        "If true, this cvar enables striping of large messages across multiple NICs.");
    MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_STRIPING = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_ENABLE_MULTI_NIC_STRIPING", &(MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_STRIPING));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_ENABLE_MULTI_NIC_STRIPING");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_ENABLE_MULTI_NIC_STRIPING", &(MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_STRIPING));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_ENABLE_MULTI_NIC_STRIPING");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_STRIPING", &(MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_STRIPING));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_STRIPING");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_STRIPING = %d\n", MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_STRIPING);
    }

#if defined MPID_CH4_OFI_MULTI_NIC_STRIPING_THRESHOLD
    defaultval.d = MPID_CH4_OFI_MULTI_NIC_STRIPING_THRESHOLD;
#else
    defaultval.d = 1048576;
#endif /* MPID_CH4_OFI_MULTI_NIC_STRIPING_THRESHOLD */

    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_MULTI_NIC_STRIPING_THRESHOLD, /* name */
        &MPIR_CVAR_CH4_OFI_MULTI_NIC_STRIPING_THRESHOLD, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4", /* category */
        "Striping will happen for message sizes beyond this threshold.");
    MPIR_CVAR_CH4_OFI_MULTI_NIC_STRIPING_THRESHOLD = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_MULTI_NIC_STRIPING_THRESHOLD", &(MPIR_CVAR_CH4_OFI_MULTI_NIC_STRIPING_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_MULTI_NIC_STRIPING_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_MULTI_NIC_STRIPING_THRESHOLD", &(MPIR_CVAR_CH4_OFI_MULTI_NIC_STRIPING_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_MULTI_NIC_STRIPING_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_MULTI_NIC_STRIPING_THRESHOLD", &(MPIR_CVAR_CH4_OFI_MULTI_NIC_STRIPING_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_MULTI_NIC_STRIPING_THRESHOLD");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_MULTI_NIC_STRIPING_THRESHOLD = %d\n", MPIR_CVAR_CH4_OFI_MULTI_NIC_STRIPING_THRESHOLD);
    }

#if defined MPID_CH4_OFI_ENABLE_MULTI_NIC_HASHING
    defaultval.d = MPID_CH4_OFI_ENABLE_MULTI_NIC_HASHING;
#else
    defaultval.d = 0;
#endif /* MPID_CH4_OFI_ENABLE_MULTI_NIC_HASHING */

    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_HASHING, /* name */
        &MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_HASHING, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4", /* category */
        "Multi-NIC hashing means to use more than one NIC to send and receive messages above a certain size.  If set to positive number, this feature will be turned on. If set to 0, this feature will be turned off. If the number is -1, MPICH automatically determines whether to use multi-nic hashing depending on what is detected on the system (e.g., number of NICs available, number of processes sharing the NICs).");
    MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_HASHING = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_ENABLE_MULTI_NIC_HASHING", &(MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_HASHING));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_ENABLE_MULTI_NIC_HASHING");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_ENABLE_MULTI_NIC_HASHING", &(MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_HASHING));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_ENABLE_MULTI_NIC_HASHING");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_HASHING", &(MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_HASHING));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_HASHING");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_HASHING = %d\n", MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_HASHING);
    }

#if defined MPID_CH4_OFI_MULTIRECV_BUFFER_SIZE
    defaultval.d = MPID_CH4_OFI_MULTIRECV_BUFFER_SIZE;
#else
    defaultval.d = 2097152;
#endif /* MPID_CH4_OFI_MULTIRECV_BUFFER_SIZE */

    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_MULTIRECV_BUFFER_SIZE, /* name */
        &MPIR_CVAR_CH4_OFI_MULTIRECV_BUFFER_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4", /* category */
        "Controls the multirecv am buffer size. It is recommended to match this to the hugepage size so that the buffer can be allocated at the page boundary.");
    MPIR_CVAR_CH4_OFI_MULTIRECV_BUFFER_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_MULTIRECV_BUFFER_SIZE", &(MPIR_CVAR_CH4_OFI_MULTIRECV_BUFFER_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_MULTIRECV_BUFFER_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_MULTIRECV_BUFFER_SIZE", &(MPIR_CVAR_CH4_OFI_MULTIRECV_BUFFER_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_MULTIRECV_BUFFER_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_MULTIRECV_BUFFER_SIZE", &(MPIR_CVAR_CH4_OFI_MULTIRECV_BUFFER_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_MULTIRECV_BUFFER_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_MULTIRECV_BUFFER_SIZE = %d\n", MPIR_CVAR_CH4_OFI_MULTIRECV_BUFFER_SIZE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_OFI_USE_MIN_NICS, /* name */
        &MPIR_CVAR_OFI_USE_MIN_NICS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "DEVELOPER", /* category */
        "If true and all nodes do not have the same number of NICs, MPICH will fall back to using the fewest number of NICs instead of returning an error.");
    MPIR_CVAR_OFI_USE_MIN_NICS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_OFI_USE_MIN_NICS", &(MPIR_CVAR_OFI_USE_MIN_NICS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_OFI_USE_MIN_NICS");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_OFI_USE_MIN_NICS", &(MPIR_CVAR_OFI_USE_MIN_NICS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_OFI_USE_MIN_NICS");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_OFI_USE_MIN_NICS", &(MPIR_CVAR_OFI_USE_MIN_NICS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_OFI_USE_MIN_NICS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_OFI_USE_MIN_NICS = %d\n", MPIR_CVAR_OFI_USE_MIN_NICS);
    }

#if defined MPID_CH4_OFI_ENABLE_TRIGGERED
    defaultval.d = MPID_CH4_OFI_ENABLE_TRIGGERED;
#else
    defaultval.d = -1;
#endif /* MPID_CH4_OFI_ENABLE_TRIGGERED */

    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_ENABLE_TRIGGERED, /* name */
        &MPIR_CVAR_CH4_OFI_ENABLE_TRIGGERED, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "If true, enable OFI triggered ops for MPI collectives.");
    MPIR_CVAR_CH4_OFI_ENABLE_TRIGGERED = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_ENABLE_TRIGGERED", &(MPIR_CVAR_CH4_OFI_ENABLE_TRIGGERED));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_ENABLE_TRIGGERED");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_ENABLE_TRIGGERED", &(MPIR_CVAR_CH4_OFI_ENABLE_TRIGGERED));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_ENABLE_TRIGGERED");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_ENABLE_TRIGGERED", &(MPIR_CVAR_CH4_OFI_ENABLE_TRIGGERED));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_ENABLE_TRIGGERED");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_ENABLE_TRIGGERED = %d\n", MPIR_CVAR_CH4_OFI_ENABLE_TRIGGERED);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_ENABLE_GPU_PIPELINE, /* name */
        &MPIR_CVAR_CH4_OFI_ENABLE_GPU_PIPELINE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "If true, enable pipeline for GPU data transfer. GPU pipeline does not support non-contiguous datatypes or mixed buffer types (i.e. GPU send buffer, host recv buffer). If GPU pipeline is enabled, the unsupported scenarios will cause undefined behavior if encountered.");
    MPIR_CVAR_CH4_OFI_ENABLE_GPU_PIPELINE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_CH4_OFI_ENABLE_GPU_PIPELINE", &(MPIR_CVAR_CH4_OFI_ENABLE_GPU_PIPELINE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_ENABLE_GPU_PIPELINE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_CH4_OFI_ENABLE_GPU_PIPELINE", &(MPIR_CVAR_CH4_OFI_ENABLE_GPU_PIPELINE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_ENABLE_GPU_PIPELINE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_CH4_OFI_ENABLE_GPU_PIPELINE", &(MPIR_CVAR_CH4_OFI_ENABLE_GPU_PIPELINE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_ENABLE_GPU_PIPELINE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_ENABLE_GPU_PIPELINE = %d\n", MPIR_CVAR_CH4_OFI_ENABLE_GPU_PIPELINE);
    }

    defaultval.d = 131072;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_GPU_PIPELINE_THRESHOLD, /* name */
        &MPIR_CVAR_CH4_OFI_GPU_PIPELINE_THRESHOLD, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "This is the threshold to start using GPU pipeline.");
    MPIR_CVAR_CH4_OFI_GPU_PIPELINE_THRESHOLD = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_GPU_PIPELINE_THRESHOLD", &(MPIR_CVAR_CH4_OFI_GPU_PIPELINE_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_GPU_PIPELINE_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_GPU_PIPELINE_THRESHOLD", &(MPIR_CVAR_CH4_OFI_GPU_PIPELINE_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_GPU_PIPELINE_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_GPU_PIPELINE_THRESHOLD", &(MPIR_CVAR_CH4_OFI_GPU_PIPELINE_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_GPU_PIPELINE_THRESHOLD");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_GPU_PIPELINE_THRESHOLD = %d\n", MPIR_CVAR_CH4_OFI_GPU_PIPELINE_THRESHOLD);
    }

    defaultval.d = 1048576;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_GPU_PIPELINE_BUFFER_SZ, /* name */
        &MPIR_CVAR_CH4_OFI_GPU_PIPELINE_BUFFER_SZ, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "Specifies the buffer size (in bytes) for GPU pipeline data transfer.");
    MPIR_CVAR_CH4_OFI_GPU_PIPELINE_BUFFER_SZ = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_GPU_PIPELINE_BUFFER_SZ", &(MPIR_CVAR_CH4_OFI_GPU_PIPELINE_BUFFER_SZ));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_GPU_PIPELINE_BUFFER_SZ");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_GPU_PIPELINE_BUFFER_SZ", &(MPIR_CVAR_CH4_OFI_GPU_PIPELINE_BUFFER_SZ));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_GPU_PIPELINE_BUFFER_SZ");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_GPU_PIPELINE_BUFFER_SZ", &(MPIR_CVAR_CH4_OFI_GPU_PIPELINE_BUFFER_SZ));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_GPU_PIPELINE_BUFFER_SZ");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_GPU_PIPELINE_BUFFER_SZ = %d\n", MPIR_CVAR_CH4_OFI_GPU_PIPELINE_BUFFER_SZ);
    }

    defaultval.d = 32;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_GPU_PIPELINE_NUM_BUFFERS_PER_CHUNK, /* name */
        &MPIR_CVAR_CH4_OFI_GPU_PIPELINE_NUM_BUFFERS_PER_CHUNK, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "Specifies the number of buffers for GPU pipeline data transfer in each block/chunk of the pool.");
    MPIR_CVAR_CH4_OFI_GPU_PIPELINE_NUM_BUFFERS_PER_CHUNK = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_GPU_PIPELINE_NUM_BUFFERS_PER_CHUNK", &(MPIR_CVAR_CH4_OFI_GPU_PIPELINE_NUM_BUFFERS_PER_CHUNK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_GPU_PIPELINE_NUM_BUFFERS_PER_CHUNK");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_GPU_PIPELINE_NUM_BUFFERS_PER_CHUNK", &(MPIR_CVAR_CH4_OFI_GPU_PIPELINE_NUM_BUFFERS_PER_CHUNK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_GPU_PIPELINE_NUM_BUFFERS_PER_CHUNK");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_GPU_PIPELINE_NUM_BUFFERS_PER_CHUNK", &(MPIR_CVAR_CH4_OFI_GPU_PIPELINE_NUM_BUFFERS_PER_CHUNK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_GPU_PIPELINE_NUM_BUFFERS_PER_CHUNK");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_GPU_PIPELINE_NUM_BUFFERS_PER_CHUNK = %d\n", MPIR_CVAR_CH4_OFI_GPU_PIPELINE_NUM_BUFFERS_PER_CHUNK);
    }

    defaultval.d = 32;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_GPU_PIPELINE_MAX_NUM_BUFFERS, /* name */
        &MPIR_CVAR_CH4_OFI_GPU_PIPELINE_MAX_NUM_BUFFERS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "Specifies the total number of buffers for GPU pipeline data transfer");
    MPIR_CVAR_CH4_OFI_GPU_PIPELINE_MAX_NUM_BUFFERS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_GPU_PIPELINE_MAX_NUM_BUFFERS", &(MPIR_CVAR_CH4_OFI_GPU_PIPELINE_MAX_NUM_BUFFERS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_GPU_PIPELINE_MAX_NUM_BUFFERS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_GPU_PIPELINE_MAX_NUM_BUFFERS", &(MPIR_CVAR_CH4_OFI_GPU_PIPELINE_MAX_NUM_BUFFERS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_GPU_PIPELINE_MAX_NUM_BUFFERS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_GPU_PIPELINE_MAX_NUM_BUFFERS", &(MPIR_CVAR_CH4_OFI_GPU_PIPELINE_MAX_NUM_BUFFERS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_GPU_PIPELINE_MAX_NUM_BUFFERS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_GPU_PIPELINE_MAX_NUM_BUFFERS = %d\n", MPIR_CVAR_CH4_OFI_GPU_PIPELINE_MAX_NUM_BUFFERS);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_GPU_PIPELINE_D2H_ENGINE_TYPE, /* name */
        &MPIR_CVAR_CH4_OFI_GPU_PIPELINE_D2H_ENGINE_TYPE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "Specifies the GPU engine type for GPU pipeline on the sender side, default is MPL_GPU_ENGINE_TYPE_COMPUTE");
    MPIR_CVAR_CH4_OFI_GPU_PIPELINE_D2H_ENGINE_TYPE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_GPU_PIPELINE_D2H_ENGINE_TYPE", &(MPIR_CVAR_CH4_OFI_GPU_PIPELINE_D2H_ENGINE_TYPE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_GPU_PIPELINE_D2H_ENGINE_TYPE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_GPU_PIPELINE_D2H_ENGINE_TYPE", &(MPIR_CVAR_CH4_OFI_GPU_PIPELINE_D2H_ENGINE_TYPE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_GPU_PIPELINE_D2H_ENGINE_TYPE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_GPU_PIPELINE_D2H_ENGINE_TYPE", &(MPIR_CVAR_CH4_OFI_GPU_PIPELINE_D2H_ENGINE_TYPE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_GPU_PIPELINE_D2H_ENGINE_TYPE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_GPU_PIPELINE_D2H_ENGINE_TYPE = %d\n", MPIR_CVAR_CH4_OFI_GPU_PIPELINE_D2H_ENGINE_TYPE);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_GPU_PIPELINE_H2D_ENGINE_TYPE, /* name */
        &MPIR_CVAR_CH4_OFI_GPU_PIPELINE_H2D_ENGINE_TYPE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "Specifies the GPU engine type for GPU pipeline on the receiver side, default is MPL_GPU_ENGINE_TYPE_COMPUTE");
    MPIR_CVAR_CH4_OFI_GPU_PIPELINE_H2D_ENGINE_TYPE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_GPU_PIPELINE_H2D_ENGINE_TYPE", &(MPIR_CVAR_CH4_OFI_GPU_PIPELINE_H2D_ENGINE_TYPE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_GPU_PIPELINE_H2D_ENGINE_TYPE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_GPU_PIPELINE_H2D_ENGINE_TYPE", &(MPIR_CVAR_CH4_OFI_GPU_PIPELINE_H2D_ENGINE_TYPE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_GPU_PIPELINE_H2D_ENGINE_TYPE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_GPU_PIPELINE_H2D_ENGINE_TYPE", &(MPIR_CVAR_CH4_OFI_GPU_PIPELINE_H2D_ENGINE_TYPE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_GPU_PIPELINE_H2D_ENGINE_TYPE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_GPU_PIPELINE_H2D_ENGINE_TYPE = %d\n", MPIR_CVAR_CH4_OFI_GPU_PIPELINE_H2D_ENGINE_TYPE);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_PREF_NIC, /* name */
        &MPIR_CVAR_CH4_OFI_PREF_NIC, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "Accept the NIC value from a user");
    MPIR_CVAR_CH4_OFI_PREF_NIC = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_PREF_NIC", &(MPIR_CVAR_CH4_OFI_PREF_NIC));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_PREF_NIC");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_PREF_NIC", &(MPIR_CVAR_CH4_OFI_PREF_NIC));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_PREF_NIC");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_PREF_NIC", &(MPIR_CVAR_CH4_OFI_PREF_NIC));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_PREF_NIC");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_PREF_NIC = %d\n", MPIR_CVAR_CH4_OFI_PREF_NIC);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_DISABLE_INJECT_WRITE, /* name */
        &MPIR_CVAR_CH4_OFI_DISABLE_INJECT_WRITE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "Avoid use fi_inject_write. For some provider, e.g. tcp;ofi_rxm, inject write may break the synchronization.");
    MPIR_CVAR_CH4_OFI_DISABLE_INJECT_WRITE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_CH4_OFI_DISABLE_INJECT_WRITE", &(MPIR_CVAR_CH4_OFI_DISABLE_INJECT_WRITE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_DISABLE_INJECT_WRITE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_CH4_OFI_DISABLE_INJECT_WRITE", &(MPIR_CVAR_CH4_OFI_DISABLE_INJECT_WRITE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_DISABLE_INJECT_WRITE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_CH4_OFI_DISABLE_INJECT_WRITE", &(MPIR_CVAR_CH4_OFI_DISABLE_INJECT_WRITE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_DISABLE_INJECT_WRITE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_DISABLE_INJECT_WRITE = %d\n", MPIR_CVAR_CH4_OFI_DISABLE_INJECT_WRITE);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_ENABLE_INJECT, /* name */
        &MPIR_CVAR_CH4_OFI_ENABLE_INJECT, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "DEVELOPER", /* category */
        "Set MPIR_CVAR_CH4_OFI_ENABLE_INJECT=0 to disable buffered send for small messages. This may help avoid hang due to lack of global progress.");
    MPIR_CVAR_CH4_OFI_ENABLE_INJECT = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_CH4_OFI_ENABLE_INJECT", &(MPIR_CVAR_CH4_OFI_ENABLE_INJECT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_ENABLE_INJECT");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_CH4_OFI_ENABLE_INJECT", &(MPIR_CVAR_CH4_OFI_ENABLE_INJECT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_ENABLE_INJECT");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_CH4_OFI_ENABLE_INJECT", &(MPIR_CVAR_CH4_OFI_ENABLE_INJECT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_ENABLE_INJECT");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_ENABLE_INJECT = %d\n", MPIR_CVAR_CH4_OFI_ENABLE_INJECT);
    }

    defaultval.d = MPIR_CVAR_CH4_OFI_GPU_SEND_ENGINE_TYPE_copy_low_latency;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_GPU_SEND_ENGINE_TYPE, /* name */
        &MPIR_CVAR_CH4_OFI_GPU_SEND_ENGINE_TYPE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "Specifies GPU engine type for GPU pt2pt on the sender side.\n"
"compute - use a compute engine\n"
"copy_high_bandwidth - use a high-bandwidth copy engine\n"
"copy_low_latency - use a low-latency copy engine\n"
"yaksa - use Yaksa");
    MPIR_CVAR_CH4_OFI_GPU_SEND_ENGINE_TYPE = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_CH4_OFI_GPU_SEND_ENGINE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_GPU_SEND_ENGINE_TYPE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_CH4_OFI_GPU_SEND_ENGINE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_GPU_SEND_ENGINE_TYPE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_CH4_OFI_GPU_SEND_ENGINE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_GPU_SEND_ENGINE_TYPE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_GPU_SEND_ENGINE_TYPE = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "compute"))
            MPIR_CVAR_CH4_OFI_GPU_SEND_ENGINE_TYPE = MPIR_CVAR_CH4_OFI_GPU_SEND_ENGINE_TYPE_compute;
        else if (0 == strcmp(tmp_str, "copy_high_bandwidth"))
            MPIR_CVAR_CH4_OFI_GPU_SEND_ENGINE_TYPE = MPIR_CVAR_CH4_OFI_GPU_SEND_ENGINE_TYPE_copy_high_bandwidth;
        else if (0 == strcmp(tmp_str, "copy_low_latency"))
            MPIR_CVAR_CH4_OFI_GPU_SEND_ENGINE_TYPE = MPIR_CVAR_CH4_OFI_GPU_SEND_ENGINE_TYPE_copy_low_latency;
        else if (0 == strcmp(tmp_str, "yaksa"))
            MPIR_CVAR_CH4_OFI_GPU_SEND_ENGINE_TYPE = MPIR_CVAR_CH4_OFI_GPU_SEND_ENGINE_TYPE_yaksa;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_CH4_OFI_GPU_SEND_ENGINE_TYPE", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_OFI_EAGER_THRESHOLD, /* name */
        &MPIR_CVAR_CH4_OFI_EAGER_THRESHOLD, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "Messages below MPIR_CVAR_CH4_OFI_EAGER_THRESHOLD will be sent eagerly using fi_tagged interfaces. Messages above the threshold will perform an MPICH-level rendezvous handshake before sending the data. If set to -1, MPICH will only perform rendezvous for messages larger than the provider max_msg_size. Note the MPICH eager/rendezvous threshold is independent of any internal libfabric provider threshold.");
    MPIR_CVAR_CH4_OFI_EAGER_THRESHOLD = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_OFI_EAGER_THRESHOLD", &(MPIR_CVAR_CH4_OFI_EAGER_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_OFI_EAGER_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_OFI_EAGER_THRESHOLD", &(MPIR_CVAR_CH4_OFI_EAGER_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_OFI_EAGER_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_OFI_EAGER_THRESHOLD", &(MPIR_CVAR_CH4_OFI_EAGER_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_OFI_EAGER_THRESHOLD");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_OFI_EAGER_THRESHOLD = %d\n", MPIR_CVAR_CH4_OFI_EAGER_THRESHOLD);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_UCX_DT_RECV, /* name */
        &MPIR_CVAR_UCX_DT_RECV, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH4_UCX", /* category */
        "Variable to select method for receiving noncontiguous data\n"
"true                - Use UCX datatype with pack/unpack callbacks\n"
"false               - MPICH will decide to pack/unpack at completion or use IOVs\n"
"based on the datatype");
    MPIR_CVAR_UCX_DT_RECV = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_UCX_DT_RECV", &(MPIR_CVAR_UCX_DT_RECV));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_UCX_DT_RECV");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_UCX_DT_RECV", &(MPIR_CVAR_UCX_DT_RECV));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_UCX_DT_RECV");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_UCX_DT_RECV", &(MPIR_CVAR_UCX_DT_RECV));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_UCX_DT_RECV");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_UCX_DT_RECV = %d\n", MPIR_CVAR_UCX_DT_RECV);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_CMA_ENABLE, /* name */
        &MPIR_CVAR_CH4_CMA_ENABLE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH4", /* category */
        "Set to 1 to manually enable CMA. It is disabled by default because the CMA requires the ptrace_scope permission, which is often disabled.");
    MPIR_CVAR_CH4_CMA_ENABLE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_CMA_ENABLE", &(MPIR_CVAR_CH4_CMA_ENABLE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_CMA_ENABLE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_CMA_ENABLE", &(MPIR_CVAR_CH4_CMA_ENABLE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_CMA_ENABLE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_CMA_ENABLE", &(MPIR_CVAR_CH4_CMA_ENABLE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_CMA_ENABLE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_CMA_ENABLE = %d\n", MPIR_CVAR_CH4_CMA_ENABLE);
    }

    defaultval.d = 16384;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_IPC_CMA_P2P_THRESHOLD, /* name */
        &MPIR_CVAR_CH4_IPC_CMA_P2P_THRESHOLD, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH4", /* category */
        "If a send message size is greater than or equal to MPIR_CVAR_CH4_IPC_CMA_P2P_THRESHOLD (in bytes), then enable CMA-based single copy protocol for intranode communication. The environment variable is valid only when the CMA submodule is enabled.");
    MPIR_CVAR_CH4_IPC_CMA_P2P_THRESHOLD = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_IPC_CMA_P2P_THRESHOLD", &(MPIR_CVAR_CH4_IPC_CMA_P2P_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_IPC_CMA_P2P_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_IPC_CMA_P2P_THRESHOLD", &(MPIR_CVAR_CH4_IPC_CMA_P2P_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_IPC_CMA_P2P_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_IPC_CMA_P2P_THRESHOLD", &(MPIR_CVAR_CH4_IPC_CMA_P2P_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_IPC_CMA_P2P_THRESHOLD");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_IPC_CMA_P2P_THRESHOLD = %d\n", MPIR_CVAR_CH4_IPC_CMA_P2P_THRESHOLD);
    }

    defaultval.d = MPIR_CVAR_CH4_IPC_GPU_HANDLE_CACHE_specialized;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_IPC_GPU_HANDLE_CACHE, /* name */
        &MPIR_CVAR_CH4_IPC_GPU_HANDLE_CACHE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH4", /* category */
        "By default, we will cache ipc handles using the specialized cache mechanism. If the\n"
"gpu-specific backend does not implement a specialized cache, then we will fallback to\n"
"the generic cache mechanism. Users can optionally force the generic cache mechanism or\n"
"disable ipc caching entirely.\n"
"generic - use the cache mechanism in the generic layer\n"
"specialized - use the cache mechanism in a gpu-specific mpl layer (if applicable)\n"
"disabled - disable caching completely");
    MPIR_CVAR_CH4_IPC_GPU_HANDLE_CACHE = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_CH4_IPC_GPU_HANDLE_CACHE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_IPC_GPU_HANDLE_CACHE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_CH4_IPC_GPU_HANDLE_CACHE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_IPC_GPU_HANDLE_CACHE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_CH4_IPC_GPU_HANDLE_CACHE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_IPC_GPU_HANDLE_CACHE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_IPC_GPU_HANDLE_CACHE = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "generic"))
            MPIR_CVAR_CH4_IPC_GPU_HANDLE_CACHE = MPIR_CVAR_CH4_IPC_GPU_HANDLE_CACHE_generic;
        else if (0 == strcmp(tmp_str, "specialized"))
            MPIR_CVAR_CH4_IPC_GPU_HANDLE_CACHE = MPIR_CVAR_CH4_IPC_GPU_HANDLE_CACHE_specialized;
        else if (0 == strcmp(tmp_str, "disabled"))
            MPIR_CVAR_CH4_IPC_GPU_HANDLE_CACHE = MPIR_CVAR_CH4_IPC_GPU_HANDLE_CACHE_disabled;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_CH4_IPC_GPU_HANDLE_CACHE", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 16;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_IPC_GPU_MAX_CACHE_ENTRIES, /* name */
        &MPIR_CVAR_CH4_IPC_GPU_MAX_CACHE_ENTRIES, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH4", /* category */
        "The maximum number of entries to hold per device in the cache containing IPC mapped buffers. When an entry is evicted, the corresponding IPC handle is closed. This value is relevant only when MPIR_CVAR_CH4_IPC_GPU_CACHE_SIZE=limited.");
    MPIR_CVAR_CH4_IPC_GPU_MAX_CACHE_ENTRIES = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_IPC_GPU_MAX_CACHE_ENTRIES", &(MPIR_CVAR_CH4_IPC_GPU_MAX_CACHE_ENTRIES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_IPC_GPU_MAX_CACHE_ENTRIES");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_IPC_GPU_MAX_CACHE_ENTRIES", &(MPIR_CVAR_CH4_IPC_GPU_MAX_CACHE_ENTRIES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_IPC_GPU_MAX_CACHE_ENTRIES");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_IPC_GPU_MAX_CACHE_ENTRIES", &(MPIR_CVAR_CH4_IPC_GPU_MAX_CACHE_ENTRIES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_IPC_GPU_MAX_CACHE_ENTRIES");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_IPC_GPU_MAX_CACHE_ENTRIES = %d\n", MPIR_CVAR_CH4_IPC_GPU_MAX_CACHE_ENTRIES);
    }

    defaultval.d = MPIR_CVAR_CH4_IPC_GPU_CACHE_SIZE_limited;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_IPC_GPU_CACHE_SIZE, /* name */
        &MPIR_CVAR_CH4_IPC_GPU_CACHE_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH4", /* category */
        "The behavior of the cache containing IPC mapped buffers.\n"
"unlimited - don't restrict the cache size\n"
"limited - limit the cache size based on MPIR_CVAR_CH4_IPC_GPU_MAX_CACHE_ENTRIES\n"
"disabled - don't cache mapped IPC buffers");
    MPIR_CVAR_CH4_IPC_GPU_CACHE_SIZE = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_CH4_IPC_GPU_CACHE_SIZE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_IPC_GPU_CACHE_SIZE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_CH4_IPC_GPU_CACHE_SIZE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_IPC_GPU_CACHE_SIZE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_CH4_IPC_GPU_CACHE_SIZE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_IPC_GPU_CACHE_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_IPC_GPU_CACHE_SIZE = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "unlimited"))
            MPIR_CVAR_CH4_IPC_GPU_CACHE_SIZE = MPIR_CVAR_CH4_IPC_GPU_CACHE_SIZE_unlimited;
        else if (0 == strcmp(tmp_str, "limited"))
            MPIR_CVAR_CH4_IPC_GPU_CACHE_SIZE = MPIR_CVAR_CH4_IPC_GPU_CACHE_SIZE_limited;
        else if (0 == strcmp(tmp_str, "disabled"))
            MPIR_CVAR_CH4_IPC_GPU_CACHE_SIZE = MPIR_CVAR_CH4_IPC_GPU_CACHE_SIZE_disabled;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_CH4_IPC_GPU_CACHE_SIZE", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 1048576;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_IPC_GPU_P2P_THRESHOLD, /* name */
        &MPIR_CVAR_CH4_IPC_GPU_P2P_THRESHOLD, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH4", /* category */
        "If a send message size is greater than or equal to MPIR_CVAR_CH4_IPC_GPU_P2P_THRESHOLD (in bytes), then enable GPU-based single copy protocol for intranode communication. The environment variable is valid only when then GPU IPC shmmod is enabled.");
    MPIR_CVAR_CH4_IPC_GPU_P2P_THRESHOLD = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_IPC_GPU_P2P_THRESHOLD", &(MPIR_CVAR_CH4_IPC_GPU_P2P_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_IPC_GPU_P2P_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_IPC_GPU_P2P_THRESHOLD", &(MPIR_CVAR_CH4_IPC_GPU_P2P_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_IPC_GPU_P2P_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_IPC_GPU_P2P_THRESHOLD", &(MPIR_CVAR_CH4_IPC_GPU_P2P_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_IPC_GPU_P2P_THRESHOLD");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_IPC_GPU_P2P_THRESHOLD = %d\n", MPIR_CVAR_CH4_IPC_GPU_P2P_THRESHOLD);
    }

    defaultval.d = MPIR_CVAR_CH4_IPC_ZE_SHAREABLE_HANDLE_drmfd;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_IPC_ZE_SHAREABLE_HANDLE, /* name */
        &MPIR_CVAR_CH4_IPC_ZE_SHAREABLE_HANDLE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH4", /* category */
        "Variable to select implementation for ZE shareable IPC handle\n"
"pidfd - use pidfd_getfd syscall to implement shareable IPC handle\n"
"drmfd - force to use device fd-based shareable IPC handle");
    MPIR_CVAR_CH4_IPC_ZE_SHAREABLE_HANDLE = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_CH4_IPC_ZE_SHAREABLE_HANDLE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_IPC_ZE_SHAREABLE_HANDLE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_CH4_IPC_ZE_SHAREABLE_HANDLE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_IPC_ZE_SHAREABLE_HANDLE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_CH4_IPC_ZE_SHAREABLE_HANDLE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_IPC_ZE_SHAREABLE_HANDLE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_IPC_ZE_SHAREABLE_HANDLE = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "pidfd"))
            MPIR_CVAR_CH4_IPC_ZE_SHAREABLE_HANDLE = MPIR_CVAR_CH4_IPC_ZE_SHAREABLE_HANDLE_pidfd;
        else if (0 == strcmp(tmp_str, "drmfd"))
            MPIR_CVAR_CH4_IPC_ZE_SHAREABLE_HANDLE = MPIR_CVAR_CH4_IPC_ZE_SHAREABLE_HANDLE_drmfd;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_CH4_IPC_ZE_SHAREABLE_HANDLE", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_CH4_IPC_GPU_ENGINE_TYPE_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_IPC_GPU_ENGINE_TYPE, /* name */
        &MPIR_CVAR_CH4_IPC_GPU_ENGINE_TYPE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH4", /* category */
        "By default, select engine type automatically\n"
"auto - select automatically\n"
"compute - use compute engine\n"
"copy_high_bandwidth - use high-bandwidth copy engine\n"
"copy_low_latency - use low-latency copy engine");
    MPIR_CVAR_CH4_IPC_GPU_ENGINE_TYPE = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_CH4_IPC_GPU_ENGINE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_IPC_GPU_ENGINE_TYPE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_CH4_IPC_GPU_ENGINE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_IPC_GPU_ENGINE_TYPE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_CH4_IPC_GPU_ENGINE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_IPC_GPU_ENGINE_TYPE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_IPC_GPU_ENGINE_TYPE = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_CH4_IPC_GPU_ENGINE_TYPE = MPIR_CVAR_CH4_IPC_GPU_ENGINE_TYPE_auto;
        else if (0 == strcmp(tmp_str, "compute"))
            MPIR_CVAR_CH4_IPC_GPU_ENGINE_TYPE = MPIR_CVAR_CH4_IPC_GPU_ENGINE_TYPE_compute;
        else if (0 == strcmp(tmp_str, "copy_high_bandwidth"))
            MPIR_CVAR_CH4_IPC_GPU_ENGINE_TYPE = MPIR_CVAR_CH4_IPC_GPU_ENGINE_TYPE_copy_high_bandwidth;
        else if (0 == strcmp(tmp_str, "copy_low_latency"))
            MPIR_CVAR_CH4_IPC_GPU_ENGINE_TYPE = MPIR_CVAR_CH4_IPC_GPU_ENGINE_TYPE_copy_low_latency;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_CH4_IPC_GPU_ENGINE_TYPE", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_CH4_IPC_GPU_READ_WRITE_PROTOCOL_read;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_IPC_GPU_READ_WRITE_PROTOCOL, /* name */
        &MPIR_CVAR_CH4_IPC_GPU_READ_WRITE_PROTOCOL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH4", /* category */
        "By default, use read protocol.\n"
"auto - select automatically\n"
"read - use read protocol\n"
"write - use write protocol if remote device is visible");
    MPIR_CVAR_CH4_IPC_GPU_READ_WRITE_PROTOCOL = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_CH4_IPC_GPU_READ_WRITE_PROTOCOL", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_IPC_GPU_READ_WRITE_PROTOCOL");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_CH4_IPC_GPU_READ_WRITE_PROTOCOL", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_IPC_GPU_READ_WRITE_PROTOCOL");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_CH4_IPC_GPU_READ_WRITE_PROTOCOL", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_IPC_GPU_READ_WRITE_PROTOCOL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_IPC_GPU_READ_WRITE_PROTOCOL = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_CH4_IPC_GPU_READ_WRITE_PROTOCOL = MPIR_CVAR_CH4_IPC_GPU_READ_WRITE_PROTOCOL_auto;
        else if (0 == strcmp(tmp_str, "read"))
            MPIR_CVAR_CH4_IPC_GPU_READ_WRITE_PROTOCOL = MPIR_CVAR_CH4_IPC_GPU_READ_WRITE_PROTOCOL_read;
        else if (0 == strcmp(tmp_str, "write"))
            MPIR_CVAR_CH4_IPC_GPU_READ_WRITE_PROTOCOL = MPIR_CVAR_CH4_IPC_GPU_READ_WRITE_PROTOCOL_write;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_CH4_IPC_GPU_READ_WRITE_PROTOCOL", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_CH4_IPC_GPU_RMA_ENGINE_TYPE_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_IPC_GPU_RMA_ENGINE_TYPE, /* name */
        &MPIR_CVAR_CH4_IPC_GPU_RMA_ENGINE_TYPE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH4", /* category */
        "By default, select engine type automatically\n"
"yaksa - don't select, use yaksa\n"
"auto - select automatically\n"
"compute - use compute engine\n"
"copy_high_bandwidth - use high-bandwidth copy engine\n"
"copy_low_latency - use low-latency copy engine");
    MPIR_CVAR_CH4_IPC_GPU_RMA_ENGINE_TYPE = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_CH4_IPC_GPU_RMA_ENGINE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_IPC_GPU_RMA_ENGINE_TYPE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_CH4_IPC_GPU_RMA_ENGINE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_IPC_GPU_RMA_ENGINE_TYPE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_CH4_IPC_GPU_RMA_ENGINE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_IPC_GPU_RMA_ENGINE_TYPE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_IPC_GPU_RMA_ENGINE_TYPE = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "yaksa"))
            MPIR_CVAR_CH4_IPC_GPU_RMA_ENGINE_TYPE = MPIR_CVAR_CH4_IPC_GPU_RMA_ENGINE_TYPE_yaksa;
        else if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_CH4_IPC_GPU_RMA_ENGINE_TYPE = MPIR_CVAR_CH4_IPC_GPU_RMA_ENGINE_TYPE_auto;
        else if (0 == strcmp(tmp_str, "compute"))
            MPIR_CVAR_CH4_IPC_GPU_RMA_ENGINE_TYPE = MPIR_CVAR_CH4_IPC_GPU_RMA_ENGINE_TYPE_compute;
        else if (0 == strcmp(tmp_str, "copy_high_bandwidth"))
            MPIR_CVAR_CH4_IPC_GPU_RMA_ENGINE_TYPE = MPIR_CVAR_CH4_IPC_GPU_RMA_ENGINE_TYPE_copy_high_bandwidth;
        else if (0 == strcmp(tmp_str, "copy_low_latency"))
            MPIR_CVAR_CH4_IPC_GPU_RMA_ENGINE_TYPE = MPIR_CVAR_CH4_IPC_GPU_RMA_ENGINE_TYPE_copy_low_latency;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_CH4_IPC_GPU_RMA_ENGINE_TYPE", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_IPC_MAP_REPEAT_ADDR, /* name */
        &MPIR_CVAR_CH4_IPC_MAP_REPEAT_ADDR, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH4", /* category */
        "Enable to track how often a buffer is being sent repeatedly. This will be used in determine whether to use IPC algorithm to deliver the message. The choice will depend on the IPC driver. In the case of high-latency buffers such as GPU device buffer, we will enable IPC if EITHER the message size is above a threshold or the message buffer is being repeated. On the other hand, if the address mapping overhead is relatively high, such as the case for XPMEM, we will enable IPC when BOTH conditions -- message size and repeat count -- are met.");
    MPIR_CVAR_CH4_IPC_MAP_REPEAT_ADDR = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_CH4_IPC_MAP_REPEAT_ADDR", &(MPIR_CVAR_CH4_IPC_MAP_REPEAT_ADDR));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_IPC_MAP_REPEAT_ADDR");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_CH4_IPC_MAP_REPEAT_ADDR", &(MPIR_CVAR_CH4_IPC_MAP_REPEAT_ADDR));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_IPC_MAP_REPEAT_ADDR");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_CH4_IPC_MAP_REPEAT_ADDR", &(MPIR_CVAR_CH4_IPC_MAP_REPEAT_ADDR));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_IPC_MAP_REPEAT_ADDR");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_IPC_MAP_REPEAT_ADDR = %d\n", MPIR_CVAR_CH4_IPC_MAP_REPEAT_ADDR);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_XPMEM_ENABLE, /* name */
        &MPIR_CVAR_CH4_XPMEM_ENABLE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH4", /* category */
        "To manually disable XPMEM set to 0. The environment variable is valid only when the XPMEM submodule is enabled.");
    MPIR_CVAR_CH4_XPMEM_ENABLE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_XPMEM_ENABLE", &(MPIR_CVAR_CH4_XPMEM_ENABLE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_XPMEM_ENABLE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_XPMEM_ENABLE", &(MPIR_CVAR_CH4_XPMEM_ENABLE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_XPMEM_ENABLE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_XPMEM_ENABLE", &(MPIR_CVAR_CH4_XPMEM_ENABLE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_XPMEM_ENABLE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_XPMEM_ENABLE = %d\n", MPIR_CVAR_CH4_XPMEM_ENABLE);
    }

    defaultval.d = 65536;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_IPC_XPMEM_P2P_THRESHOLD, /* name */
        &MPIR_CVAR_CH4_IPC_XPMEM_P2P_THRESHOLD, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH4", /* category */
        "If a send message size is greater than or equal to MPIR_CVAR_CH4_IPC_XPMEM_P2P_THRESHOLD (in bytes), then enable XPMEM-based single copy protocol for intranode communication. The environment variable is valid only when the XPMEM submodule is enabled.");
    MPIR_CVAR_CH4_IPC_XPMEM_P2P_THRESHOLD = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_IPC_XPMEM_P2P_THRESHOLD", &(MPIR_CVAR_CH4_IPC_XPMEM_P2P_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_IPC_XPMEM_P2P_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_IPC_XPMEM_P2P_THRESHOLD", &(MPIR_CVAR_CH4_IPC_XPMEM_P2P_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_IPC_XPMEM_P2P_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_IPC_XPMEM_P2P_THRESHOLD", &(MPIR_CVAR_CH4_IPC_XPMEM_P2P_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_IPC_XPMEM_P2P_THRESHOLD");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_IPC_XPMEM_P2P_THRESHOLD = %d\n", MPIR_CVAR_CH4_IPC_XPMEM_P2P_THRESHOLD);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_IPC_XPMEM_P2P_UPPER_THRESHOLD, /* name */
        &MPIR_CVAR_CH4_IPC_XPMEM_P2P_UPPER_THRESHOLD, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH4", /* category */
        "If a send message size is greater than or equal to MPIR_CVAR_CH4_IPC_XPMEM_P2P_UPPER_THRESHOLD (in bytes), then skip XPMEM-based single copy protocol for intranode communication. The environment variable is valid only when the XPMEM submodule is enabled. The default is -1, which does not limit the upper threshold.");
    MPIR_CVAR_CH4_IPC_XPMEM_P2P_UPPER_THRESHOLD = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_IPC_XPMEM_P2P_UPPER_THRESHOLD", &(MPIR_CVAR_CH4_IPC_XPMEM_P2P_UPPER_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_IPC_XPMEM_P2P_UPPER_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_IPC_XPMEM_P2P_UPPER_THRESHOLD", &(MPIR_CVAR_CH4_IPC_XPMEM_P2P_UPPER_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_IPC_XPMEM_P2P_UPPER_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_IPC_XPMEM_P2P_UPPER_THRESHOLD", &(MPIR_CVAR_CH4_IPC_XPMEM_P2P_UPPER_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_IPC_XPMEM_P2P_UPPER_THRESHOLD");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_IPC_XPMEM_P2P_UPPER_THRESHOLD = %d\n", MPIR_CVAR_CH4_IPC_XPMEM_P2P_UPPER_THRESHOLD);
    }

    defaultval.d = MPIR_CVAR_BCAST_POSIX_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BCAST_POSIX_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_BCAST_POSIX_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select algorithm for intra-node bcast\n"
"mpir           - Fallback to MPIR collectives\n"
"release_gather - Force shm optimized algo using release, gather primitives\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)\n"
"ipc_read - Uses read-based collective with ipc");
    MPIR_CVAR_BCAST_POSIX_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_BCAST_POSIX_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BCAST_POSIX_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_BCAST_POSIX_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BCAST_POSIX_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_BCAST_POSIX_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BCAST_POSIX_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BCAST_POSIX_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "mpir"))
            MPIR_CVAR_BCAST_POSIX_INTRA_ALGORITHM = MPIR_CVAR_BCAST_POSIX_INTRA_ALGORITHM_mpir;
        else if (0 == strcmp(tmp_str, "release_gather"))
            MPIR_CVAR_BCAST_POSIX_INTRA_ALGORITHM = MPIR_CVAR_BCAST_POSIX_INTRA_ALGORITHM_release_gather;
        else if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_BCAST_POSIX_INTRA_ALGORITHM = MPIR_CVAR_BCAST_POSIX_INTRA_ALGORITHM_auto;
        else if (0 == strcmp(tmp_str, "ipc_read"))
            MPIR_CVAR_BCAST_POSIX_INTRA_ALGORITHM = MPIR_CVAR_BCAST_POSIX_INTRA_ALGORITHM_ipc_read;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_BCAST_POSIX_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_IBCAST_POSIX_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IBCAST_POSIX_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_IBCAST_POSIX_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select algorithm for intra-node bcast\n"
"mpir           - Fallback to MPIR collectives\n"
"release_gather - Force shm optimized algo using release, gather primitives\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)");
    MPIR_CVAR_IBCAST_POSIX_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IBCAST_POSIX_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IBCAST_POSIX_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IBCAST_POSIX_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IBCAST_POSIX_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IBCAST_POSIX_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IBCAST_POSIX_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IBCAST_POSIX_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "mpir"))
            MPIR_CVAR_IBCAST_POSIX_INTRA_ALGORITHM = MPIR_CVAR_IBCAST_POSIX_INTRA_ALGORITHM_mpir;
        else if (0 == strcmp(tmp_str, "release_gather"))
            MPIR_CVAR_IBCAST_POSIX_INTRA_ALGORITHM = MPIR_CVAR_IBCAST_POSIX_INTRA_ALGORITHM_release_gather;
        else if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IBCAST_POSIX_INTRA_ALGORITHM = MPIR_CVAR_IBCAST_POSIX_INTRA_ALGORITHM_auto;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IBCAST_POSIX_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_REDUCE_POSIX_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_REDUCE_POSIX_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_REDUCE_POSIX_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select algorithm for intra-node reduce\n"
"mpir           - Fallback to MPIR collectives\n"
"release_gather - Force shm optimized algo using release, gather primitives\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)");
    MPIR_CVAR_REDUCE_POSIX_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_REDUCE_POSIX_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_REDUCE_POSIX_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_REDUCE_POSIX_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_REDUCE_POSIX_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_REDUCE_POSIX_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_REDUCE_POSIX_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_REDUCE_POSIX_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "mpir"))
            MPIR_CVAR_REDUCE_POSIX_INTRA_ALGORITHM = MPIR_CVAR_REDUCE_POSIX_INTRA_ALGORITHM_mpir;
        else if (0 == strcmp(tmp_str, "release_gather"))
            MPIR_CVAR_REDUCE_POSIX_INTRA_ALGORITHM = MPIR_CVAR_REDUCE_POSIX_INTRA_ALGORITHM_release_gather;
        else if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_REDUCE_POSIX_INTRA_ALGORITHM = MPIR_CVAR_REDUCE_POSIX_INTRA_ALGORITHM_auto;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_REDUCE_POSIX_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_IREDUCE_POSIX_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_IREDUCE_POSIX_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_IREDUCE_POSIX_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select algorithm for intra-node reduce\n"
"mpir           - Fallback to MPIR collectives\n"
"release_gather - Force shm optimized algo using release, gather primitives\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)");
    MPIR_CVAR_IREDUCE_POSIX_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_IREDUCE_POSIX_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_IREDUCE_POSIX_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_IREDUCE_POSIX_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_IREDUCE_POSIX_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_IREDUCE_POSIX_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_IREDUCE_POSIX_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_IREDUCE_POSIX_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "mpir"))
            MPIR_CVAR_IREDUCE_POSIX_INTRA_ALGORITHM = MPIR_CVAR_IREDUCE_POSIX_INTRA_ALGORITHM_mpir;
        else if (0 == strcmp(tmp_str, "release_gather"))
            MPIR_CVAR_IREDUCE_POSIX_INTRA_ALGORITHM = MPIR_CVAR_IREDUCE_POSIX_INTRA_ALGORITHM_release_gather;
        else if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_IREDUCE_POSIX_INTRA_ALGORITHM = MPIR_CVAR_IREDUCE_POSIX_INTRA_ALGORITHM_auto;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_IREDUCE_POSIX_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_ALLREDUCE_POSIX_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLREDUCE_POSIX_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_ALLREDUCE_POSIX_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select algorithm for intra-node allreduce\n"
"mpir           - Fallback to MPIR collectives\n"
"release_gather - Force shm optimized algo using release, gather primitives\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)");
    MPIR_CVAR_ALLREDUCE_POSIX_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_ALLREDUCE_POSIX_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLREDUCE_POSIX_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_ALLREDUCE_POSIX_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLREDUCE_POSIX_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_ALLREDUCE_POSIX_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLREDUCE_POSIX_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLREDUCE_POSIX_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "mpir"))
            MPIR_CVAR_ALLREDUCE_POSIX_INTRA_ALGORITHM = MPIR_CVAR_ALLREDUCE_POSIX_INTRA_ALGORITHM_mpir;
        else if (0 == strcmp(tmp_str, "release_gather"))
            MPIR_CVAR_ALLREDUCE_POSIX_INTRA_ALGORITHM = MPIR_CVAR_ALLREDUCE_POSIX_INTRA_ALGORITHM_release_gather;
        else if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_ALLREDUCE_POSIX_INTRA_ALGORITHM = MPIR_CVAR_ALLREDUCE_POSIX_INTRA_ALGORITHM_auto;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_ALLREDUCE_POSIX_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_BARRIER_POSIX_INTRA_ALGORITHM_auto;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BARRIER_POSIX_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_BARRIER_POSIX_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select algorithm for intra-node barrier\n"
"mpir           - Fallback to MPIR collectives\n"
"release_gather - Force shm optimized algo using release, gather primitives\n"
"auto - Internal algorithm selection (can be overridden with MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE)");
    MPIR_CVAR_BARRIER_POSIX_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_BARRIER_POSIX_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BARRIER_POSIX_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_BARRIER_POSIX_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BARRIER_POSIX_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_BARRIER_POSIX_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BARRIER_POSIX_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BARRIER_POSIX_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "mpir"))
            MPIR_CVAR_BARRIER_POSIX_INTRA_ALGORITHM = MPIR_CVAR_BARRIER_POSIX_INTRA_ALGORITHM_mpir;
        else if (0 == strcmp(tmp_str, "release_gather"))
            MPIR_CVAR_BARRIER_POSIX_INTRA_ALGORITHM = MPIR_CVAR_BARRIER_POSIX_INTRA_ALGORITHM_release_gather;
        else if (0 == strcmp(tmp_str, "auto"))
            MPIR_CVAR_BARRIER_POSIX_INTRA_ALGORITHM = MPIR_CVAR_BARRIER_POSIX_INTRA_ALGORITHM_auto;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_BARRIER_POSIX_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_ALLTOALL_POSIX_INTRA_ALGORITHM_mpir;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLTOALL_POSIX_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_ALLTOALL_POSIX_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select algorithm for intra-node alltoall\n"
"mpir           - Fallback to MPIR collectives (default)\n"
"ipc_read    - Uses read-based collective with ipc");
    MPIR_CVAR_ALLTOALL_POSIX_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_ALLTOALL_POSIX_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLTOALL_POSIX_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_ALLTOALL_POSIX_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLTOALL_POSIX_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_ALLTOALL_POSIX_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLTOALL_POSIX_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLTOALL_POSIX_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "mpir"))
            MPIR_CVAR_ALLTOALL_POSIX_INTRA_ALGORITHM = MPIR_CVAR_ALLTOALL_POSIX_INTRA_ALGORITHM_mpir;
        else if (0 == strcmp(tmp_str, "ipc_read"))
            MPIR_CVAR_ALLTOALL_POSIX_INTRA_ALGORITHM = MPIR_CVAR_ALLTOALL_POSIX_INTRA_ALGORITHM_ipc_read;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_ALLTOALL_POSIX_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_ALLGATHER_POSIX_INTRA_ALGORITHM_mpir;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLGATHER_POSIX_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_ALLGATHER_POSIX_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select algorithm for intra-node allgather\n"
"mpir        - Fallback to MPIR collectives (default)\n"
"ipc_read    - Uses read-based collective with ipc");
    MPIR_CVAR_ALLGATHER_POSIX_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_ALLGATHER_POSIX_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLGATHER_POSIX_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_ALLGATHER_POSIX_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLGATHER_POSIX_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_ALLGATHER_POSIX_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLGATHER_POSIX_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLGATHER_POSIX_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "mpir"))
            MPIR_CVAR_ALLGATHER_POSIX_INTRA_ALGORITHM = MPIR_CVAR_ALLGATHER_POSIX_INTRA_ALGORITHM_mpir;
        else if (0 == strcmp(tmp_str, "ipc_read"))
            MPIR_CVAR_ALLGATHER_POSIX_INTRA_ALGORITHM = MPIR_CVAR_ALLGATHER_POSIX_INTRA_ALGORITHM_ipc_read;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_ALLGATHER_POSIX_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = MPIR_CVAR_ALLGATHERV_POSIX_INTRA_ALGORITHM_mpir;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLGATHERV_POSIX_INTRA_ALGORITHM, /* name */
        &MPIR_CVAR_ALLGATHERV_POSIX_INTRA_ALGORITHM, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Variable to select algorithm for intra-node allgatherv\n"
"mpir        - Fallback to MPIR collectives (default)\n"
"ipc_read    - Uses read-based collective with ipc");
    MPIR_CVAR_ALLGATHERV_POSIX_INTRA_ALGORITHM = defaultval.d;
    tmp_str=NULL;
    got_rc = 0;
    rc = MPL_env2str("MPICH_ALLGATHERV_POSIX_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLGATHERV_POSIX_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_ALLGATHERV_POSIX_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLGATHERV_POSIX_INTRA_ALGORITHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_ALLGATHERV_POSIX_INTRA_ALGORITHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLGATHERV_POSIX_INTRA_ALGORITHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLGATHERV_POSIX_INTRA_ALGORITHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        if (0 == strcmp(tmp_str, "mpir"))
            MPIR_CVAR_ALLGATHERV_POSIX_INTRA_ALGORITHM = MPIR_CVAR_ALLGATHERV_POSIX_INTRA_ALGORITHM_mpir;
        else if (0 == strcmp(tmp_str, "ipc_read"))
            MPIR_CVAR_ALLGATHERV_POSIX_INTRA_ALGORITHM = MPIR_CVAR_ALLGATHERV_POSIX_INTRA_ALGORITHM_ipc_read;
        else {
            mpi_errno = MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, __func__, __LINE__,MPI_ERR_OTHER, "**cvar_val", "**cvar_val %s %s", "MPIR_CVAR_ALLGATHERV_POSIX_INTRA_ALGORITHM", tmp_str);
            goto fn_fail;
        }
    }

    defaultval.d = 1000;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_POSIX_POLL_FREQUENCY, /* name */
        &MPIR_CVAR_POSIX_POLL_FREQUENCY, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "This cvar sets the number of loops before the yield function is called.  A value of 0 disables yielding.");
    MPIR_CVAR_POSIX_POLL_FREQUENCY = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_POSIX_POLL_FREQUENCY", &(MPIR_CVAR_POSIX_POLL_FREQUENCY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_POSIX_POLL_FREQUENCY");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_POSIX_POLL_FREQUENCY", &(MPIR_CVAR_POSIX_POLL_FREQUENCY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_POSIX_POLL_FREQUENCY");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_POSIX_POLL_FREQUENCY", &(MPIR_CVAR_POSIX_POLL_FREQUENCY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_POSIX_POLL_FREQUENCY");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_POSIX_POLL_FREQUENCY = %d\n", MPIR_CVAR_POSIX_POLL_FREQUENCY);
    }

    defaultval.d = 256;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BCAST_IPC_READ_MSG_SIZE_THRESHOLD, /* name */
        &MPIR_CVAR_BCAST_IPC_READ_MSG_SIZE_THRESHOLD, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Use gpu ipc read bcast only when the message size is larger than this threshold.");
    MPIR_CVAR_BCAST_IPC_READ_MSG_SIZE_THRESHOLD = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_BCAST_IPC_READ_MSG_SIZE_THRESHOLD", &(MPIR_CVAR_BCAST_IPC_READ_MSG_SIZE_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BCAST_IPC_READ_MSG_SIZE_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_BCAST_IPC_READ_MSG_SIZE_THRESHOLD", &(MPIR_CVAR_BCAST_IPC_READ_MSG_SIZE_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BCAST_IPC_READ_MSG_SIZE_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_BCAST_IPC_READ_MSG_SIZE_THRESHOLD", &(MPIR_CVAR_BCAST_IPC_READ_MSG_SIZE_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BCAST_IPC_READ_MSG_SIZE_THRESHOLD");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BCAST_IPC_READ_MSG_SIZE_THRESHOLD = %d\n", MPIR_CVAR_BCAST_IPC_READ_MSG_SIZE_THRESHOLD);
    }

    defaultval.d = 256;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLTOALL_IPC_READ_MSG_SIZE_THRESHOLD, /* name */
        &MPIR_CVAR_ALLTOALL_IPC_READ_MSG_SIZE_THRESHOLD, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Use gpu ipc read alltoall only when the message size is larger than this threshold.");
    MPIR_CVAR_ALLTOALL_IPC_READ_MSG_SIZE_THRESHOLD = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLTOALL_IPC_READ_MSG_SIZE_THRESHOLD", &(MPIR_CVAR_ALLTOALL_IPC_READ_MSG_SIZE_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLTOALL_IPC_READ_MSG_SIZE_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLTOALL_IPC_READ_MSG_SIZE_THRESHOLD", &(MPIR_CVAR_ALLTOALL_IPC_READ_MSG_SIZE_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLTOALL_IPC_READ_MSG_SIZE_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLTOALL_IPC_READ_MSG_SIZE_THRESHOLD", &(MPIR_CVAR_ALLTOALL_IPC_READ_MSG_SIZE_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLTOALL_IPC_READ_MSG_SIZE_THRESHOLD");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLTOALL_IPC_READ_MSG_SIZE_THRESHOLD = %d\n", MPIR_CVAR_ALLTOALL_IPC_READ_MSG_SIZE_THRESHOLD);
    }

    defaultval.d = 256;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLGATHER_IPC_READ_MSG_SIZE_THRESHOLD, /* name */
        &MPIR_CVAR_ALLGATHER_IPC_READ_MSG_SIZE_THRESHOLD, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Use gpu ipc read allgather only when the message size is larger than this threshold.");
    MPIR_CVAR_ALLGATHER_IPC_READ_MSG_SIZE_THRESHOLD = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLGATHER_IPC_READ_MSG_SIZE_THRESHOLD", &(MPIR_CVAR_ALLGATHER_IPC_READ_MSG_SIZE_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLGATHER_IPC_READ_MSG_SIZE_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLGATHER_IPC_READ_MSG_SIZE_THRESHOLD", &(MPIR_CVAR_ALLGATHER_IPC_READ_MSG_SIZE_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLGATHER_IPC_READ_MSG_SIZE_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLGATHER_IPC_READ_MSG_SIZE_THRESHOLD", &(MPIR_CVAR_ALLGATHER_IPC_READ_MSG_SIZE_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLGATHER_IPC_READ_MSG_SIZE_THRESHOLD");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLGATHER_IPC_READ_MSG_SIZE_THRESHOLD = %d\n", MPIR_CVAR_ALLGATHER_IPC_READ_MSG_SIZE_THRESHOLD);
    }

    defaultval.d = 256;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLGATHERV_IPC_READ_MSG_SIZE_THRESHOLD, /* name */
        &MPIR_CVAR_ALLGATHERV_IPC_READ_MSG_SIZE_THRESHOLD, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Use gpu ipc read allgatherv only when the message size is larger than this threshold.");
    MPIR_CVAR_ALLGATHERV_IPC_READ_MSG_SIZE_THRESHOLD = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLGATHERV_IPC_READ_MSG_SIZE_THRESHOLD", &(MPIR_CVAR_ALLGATHERV_IPC_READ_MSG_SIZE_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLGATHERV_IPC_READ_MSG_SIZE_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLGATHERV_IPC_READ_MSG_SIZE_THRESHOLD", &(MPIR_CVAR_ALLGATHERV_IPC_READ_MSG_SIZE_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLGATHERV_IPC_READ_MSG_SIZE_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLGATHERV_IPC_READ_MSG_SIZE_THRESHOLD", &(MPIR_CVAR_ALLGATHERV_IPC_READ_MSG_SIZE_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLGATHERV_IPC_READ_MSG_SIZE_THRESHOLD");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLGATHERV_IPC_READ_MSG_SIZE_THRESHOLD = %d\n", MPIR_CVAR_ALLGATHERV_IPC_READ_MSG_SIZE_THRESHOLD);
    }

    defaultval.d = 5;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_POSIX_NUM_COLLS_THRESHOLD, /* name */
        &MPIR_CVAR_POSIX_NUM_COLLS_THRESHOLD, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Use posix optimized collectives (release_gather) only when the total number of Bcast, Reduce, Barrier, and Allreduce calls on the node level communicator is more than this threshold.");
    MPIR_CVAR_POSIX_NUM_COLLS_THRESHOLD = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_POSIX_NUM_COLLS_THRESHOLD", &(MPIR_CVAR_POSIX_NUM_COLLS_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_POSIX_NUM_COLLS_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_POSIX_NUM_COLLS_THRESHOLD", &(MPIR_CVAR_POSIX_NUM_COLLS_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_POSIX_NUM_COLLS_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_POSIX_NUM_COLLS_THRESHOLD", &(MPIR_CVAR_POSIX_NUM_COLLS_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_POSIX_NUM_COLLS_THRESHOLD");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_POSIX_NUM_COLLS_THRESHOLD = %d\n", MPIR_CVAR_POSIX_NUM_COLLS_THRESHOLD);
    }

    defaultval.str = (const char *) "";
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_CHAR,
        MPIR_CVAR_CH4_SHM_POSIX_EAGER, /* name */
        &MPIR_CVAR_CH4_SHM_POSIX_EAGER, /* address */
        MPIR_CVAR_MAX_STRLEN, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH4", /* category */
        "If non-empty, this cvar specifies which shm posix eager module to use");
    tmp_str = defaultval.str;
    got_rc = 0;
    rc = MPL_env2str("MPICH_CH4_SHM_POSIX_EAGER", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_SHM_POSIX_EAGER");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_CH4_SHM_POSIX_EAGER", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_SHM_POSIX_EAGER");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_CH4_SHM_POSIX_EAGER", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_SHM_POSIX_EAGER");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_SHM_POSIX_EAGER = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        MPIR_CVAR_CH4_SHM_POSIX_EAGER = MPL_strdup(tmp_str);
        MPIR_CVAR_assert(MPIR_CVAR_CH4_SHM_POSIX_EAGER);
        if (MPIR_CVAR_CH4_SHM_POSIX_EAGER == NULL) {
            MPIR_CHKMEM_SETERR(mpi_errno, strlen(tmp_str), "dup of string for MPIR_CVAR_CH4_SHM_POSIX_EAGER");
            goto fn_fail;
        }
    }
    else {
        MPIR_CVAR_CH4_SHM_POSIX_EAGER = NULL;
    }

    defaultval.str = (const char *) "";
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_CHAR,
        MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE, /* name */
        &MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE, /* address */
        MPIR_CVAR_MAX_STRLEN, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Defines the location of tuning file.");
    tmp_str = defaultval.str;
    got_rc = 0;
    rc = MPL_env2str("MPICH_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE = MPL_strdup(tmp_str);
        MPIR_CVAR_assert(MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE);
        if (MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE == NULL) {
            MPIR_CHKMEM_SETERR(mpi_errno, strlen(tmp_str), "dup of string for MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE");
            goto fn_fail;
        }
    }
    else {
        MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE = NULL;
    }

    defaultval.str = (const char *) "";
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_CHAR,
        MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE_GPU, /* name */
        &MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE_GPU, /* address */
        MPIR_CVAR_MAX_STRLEN, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Defines the location of tuning file for GPU.");
    tmp_str = defaultval.str;
    got_rc = 0;
    rc = MPL_env2str("MPICH_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE_GPU", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE_GPU");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE_GPU", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE_GPU");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE_GPU", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE_GPU");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE_GPU = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE_GPU = MPL_strdup(tmp_str);
        MPIR_CVAR_assert(MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE_GPU);
        if (MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE_GPU == NULL) {
            MPIR_CHKMEM_SETERR(mpi_errno, strlen(tmp_str), "dup of string for MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE_GPU");
            goto fn_fail;
        }
    }
    else {
        MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE_GPU = NULL;
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_SHM_POSIX_TOPO_ENABLE, /* name */
        &MPIR_CVAR_CH4_SHM_POSIX_TOPO_ENABLE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH4", /* category */
        "Controls topology-aware communication in POSIX.");
    MPIR_CVAR_CH4_SHM_POSIX_TOPO_ENABLE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_CH4_SHM_POSIX_TOPO_ENABLE", &(MPIR_CVAR_CH4_SHM_POSIX_TOPO_ENABLE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_SHM_POSIX_TOPO_ENABLE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_CH4_SHM_POSIX_TOPO_ENABLE", &(MPIR_CVAR_CH4_SHM_POSIX_TOPO_ENABLE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_SHM_POSIX_TOPO_ENABLE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_CH4_SHM_POSIX_TOPO_ENABLE", &(MPIR_CVAR_CH4_SHM_POSIX_TOPO_ENABLE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_SHM_POSIX_TOPO_ENABLE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_SHM_POSIX_TOPO_ENABLE = %d\n", MPIR_CVAR_CH4_SHM_POSIX_TOPO_ENABLE);
    }

    defaultval.d = 64;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_NUM_CELLS, /* name */
        &MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_NUM_CELLS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH4", /* category */
        "The number of cells used for the depth of the iqueue.");
    MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_NUM_CELLS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_SHM_POSIX_IQUEUE_NUM_CELLS", &(MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_NUM_CELLS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_SHM_POSIX_IQUEUE_NUM_CELLS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_SHM_POSIX_IQUEUE_NUM_CELLS", &(MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_NUM_CELLS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_SHM_POSIX_IQUEUE_NUM_CELLS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_NUM_CELLS", &(MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_NUM_CELLS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_NUM_CELLS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_NUM_CELLS = %d\n", MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_NUM_CELLS);
    }

    defaultval.d = 16384;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_CELL_SIZE, /* name */
        &MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_CELL_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH4", /* category */
        "Size of each cell.");
    MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_CELL_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_SHM_POSIX_IQUEUE_CELL_SIZE", &(MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_CELL_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_SHM_POSIX_IQUEUE_CELL_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_SHM_POSIX_IQUEUE_CELL_SIZE", &(MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_CELL_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_SHM_POSIX_IQUEUE_CELL_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_CELL_SIZE", &(MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_CELL_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_CELL_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_CELL_SIZE = %d\n", MPIR_CVAR_CH4_SHM_POSIX_IQUEUE_CELL_SIZE);
    }

    defaultval.d = 65536;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_COLL_SHM_LIMIT_PER_NODE, /* name */
        &MPIR_CVAR_COLL_SHM_LIMIT_PER_NODE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Maximum shared memory created per node for optimized intra-node collectives (in KB)");
    MPIR_CVAR_COLL_SHM_LIMIT_PER_NODE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_COLL_SHM_LIMIT_PER_NODE", &(MPIR_CVAR_COLL_SHM_LIMIT_PER_NODE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_COLL_SHM_LIMIT_PER_NODE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_COLL_SHM_LIMIT_PER_NODE", &(MPIR_CVAR_COLL_SHM_LIMIT_PER_NODE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_COLL_SHM_LIMIT_PER_NODE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_COLL_SHM_LIMIT_PER_NODE", &(MPIR_CVAR_COLL_SHM_LIMIT_PER_NODE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_COLL_SHM_LIMIT_PER_NODE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_COLL_SHM_LIMIT_PER_NODE = %d\n", MPIR_CVAR_COLL_SHM_LIMIT_PER_NODE);
    }

    defaultval.d = 32768;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BCAST_INTRANODE_BUFFER_TOTAL_SIZE, /* name */
        &MPIR_CVAR_BCAST_INTRANODE_BUFFER_TOTAL_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Total size of the bcast buffer (in bytes)");
    MPIR_CVAR_BCAST_INTRANODE_BUFFER_TOTAL_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_BCAST_INTRANODE_BUFFER_TOTAL_SIZE", &(MPIR_CVAR_BCAST_INTRANODE_BUFFER_TOTAL_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BCAST_INTRANODE_BUFFER_TOTAL_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_BCAST_INTRANODE_BUFFER_TOTAL_SIZE", &(MPIR_CVAR_BCAST_INTRANODE_BUFFER_TOTAL_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BCAST_INTRANODE_BUFFER_TOTAL_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_BCAST_INTRANODE_BUFFER_TOTAL_SIZE", &(MPIR_CVAR_BCAST_INTRANODE_BUFFER_TOTAL_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BCAST_INTRANODE_BUFFER_TOTAL_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BCAST_INTRANODE_BUFFER_TOTAL_SIZE = %d\n", MPIR_CVAR_BCAST_INTRANODE_BUFFER_TOTAL_SIZE);
    }

    defaultval.d = 4;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BCAST_INTRANODE_NUM_CELLS, /* name */
        &MPIR_CVAR_BCAST_INTRANODE_NUM_CELLS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Number of cells the bcast buffer is divided into");
    MPIR_CVAR_BCAST_INTRANODE_NUM_CELLS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_BCAST_INTRANODE_NUM_CELLS", &(MPIR_CVAR_BCAST_INTRANODE_NUM_CELLS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BCAST_INTRANODE_NUM_CELLS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_BCAST_INTRANODE_NUM_CELLS", &(MPIR_CVAR_BCAST_INTRANODE_NUM_CELLS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BCAST_INTRANODE_NUM_CELLS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_BCAST_INTRANODE_NUM_CELLS", &(MPIR_CVAR_BCAST_INTRANODE_NUM_CELLS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BCAST_INTRANODE_NUM_CELLS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BCAST_INTRANODE_NUM_CELLS = %d\n", MPIR_CVAR_BCAST_INTRANODE_NUM_CELLS);
    }

    defaultval.d = 32768;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_REDUCE_INTRANODE_BUFFER_TOTAL_SIZE, /* name */
        &MPIR_CVAR_REDUCE_INTRANODE_BUFFER_TOTAL_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Total size of the reduce buffer per rank (in bytes)");
    MPIR_CVAR_REDUCE_INTRANODE_BUFFER_TOTAL_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_REDUCE_INTRANODE_BUFFER_TOTAL_SIZE", &(MPIR_CVAR_REDUCE_INTRANODE_BUFFER_TOTAL_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_REDUCE_INTRANODE_BUFFER_TOTAL_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_REDUCE_INTRANODE_BUFFER_TOTAL_SIZE", &(MPIR_CVAR_REDUCE_INTRANODE_BUFFER_TOTAL_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_REDUCE_INTRANODE_BUFFER_TOTAL_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_REDUCE_INTRANODE_BUFFER_TOTAL_SIZE", &(MPIR_CVAR_REDUCE_INTRANODE_BUFFER_TOTAL_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_REDUCE_INTRANODE_BUFFER_TOTAL_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_REDUCE_INTRANODE_BUFFER_TOTAL_SIZE = %d\n", MPIR_CVAR_REDUCE_INTRANODE_BUFFER_TOTAL_SIZE);
    }

    defaultval.d = 4;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_REDUCE_INTRANODE_NUM_CELLS, /* name */
        &MPIR_CVAR_REDUCE_INTRANODE_NUM_CELLS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Number of cells the reduce buffer is divided into, for each rank");
    MPIR_CVAR_REDUCE_INTRANODE_NUM_CELLS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_REDUCE_INTRANODE_NUM_CELLS", &(MPIR_CVAR_REDUCE_INTRANODE_NUM_CELLS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_REDUCE_INTRANODE_NUM_CELLS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_REDUCE_INTRANODE_NUM_CELLS", &(MPIR_CVAR_REDUCE_INTRANODE_NUM_CELLS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_REDUCE_INTRANODE_NUM_CELLS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_REDUCE_INTRANODE_NUM_CELLS", &(MPIR_CVAR_REDUCE_INTRANODE_NUM_CELLS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_REDUCE_INTRANODE_NUM_CELLS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_REDUCE_INTRANODE_NUM_CELLS = %d\n", MPIR_CVAR_REDUCE_INTRANODE_NUM_CELLS);
    }

    defaultval.d = 64;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BCAST_INTRANODE_TREE_KVAL, /* name */
        &MPIR_CVAR_BCAST_INTRANODE_TREE_KVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "K value for the kary/knomial tree for intra-node bcast");
    MPIR_CVAR_BCAST_INTRANODE_TREE_KVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_BCAST_INTRANODE_TREE_KVAL", &(MPIR_CVAR_BCAST_INTRANODE_TREE_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BCAST_INTRANODE_TREE_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_BCAST_INTRANODE_TREE_KVAL", &(MPIR_CVAR_BCAST_INTRANODE_TREE_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BCAST_INTRANODE_TREE_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_BCAST_INTRANODE_TREE_KVAL", &(MPIR_CVAR_BCAST_INTRANODE_TREE_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BCAST_INTRANODE_TREE_KVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BCAST_INTRANODE_TREE_KVAL = %d\n", MPIR_CVAR_BCAST_INTRANODE_TREE_KVAL);
    }

    defaultval.str = (const char *) "kary";
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_CHAR,
        MPIR_CVAR_BCAST_INTRANODE_TREE_TYPE, /* name */
        &MPIR_CVAR_BCAST_INTRANODE_TREE_TYPE, /* address */
        MPIR_CVAR_MAX_STRLEN, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Tree type for intra-node bcast tree kary      - kary tree type knomial_1 - knomial_1 tree type (ranks are added in order from the left side) knomial_2 - knomial_2 tree type (ranks are added in order from the right side) knomial_2 is only supported with non topology aware trees.");
    tmp_str = defaultval.str;
    got_rc = 0;
    rc = MPL_env2str("MPICH_BCAST_INTRANODE_TREE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BCAST_INTRANODE_TREE_TYPE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_BCAST_INTRANODE_TREE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BCAST_INTRANODE_TREE_TYPE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_BCAST_INTRANODE_TREE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BCAST_INTRANODE_TREE_TYPE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BCAST_INTRANODE_TREE_TYPE = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        MPIR_CVAR_BCAST_INTRANODE_TREE_TYPE = MPL_strdup(tmp_str);
        MPIR_CVAR_assert(MPIR_CVAR_BCAST_INTRANODE_TREE_TYPE);
        if (MPIR_CVAR_BCAST_INTRANODE_TREE_TYPE == NULL) {
            MPIR_CHKMEM_SETERR(mpi_errno, strlen(tmp_str), "dup of string for MPIR_CVAR_BCAST_INTRANODE_TREE_TYPE");
            goto fn_fail;
        }
    }
    else {
        MPIR_CVAR_BCAST_INTRANODE_TREE_TYPE = NULL;
    }

    defaultval.d = 2048;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_REDUCE_INTRANODE_MSG_SIZE_THRESHOLD, /* name */
        &MPIR_CVAR_REDUCE_INTRANODE_MSG_SIZE_THRESHOLD, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL and MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE are used when the message size is smaller than or equal to this threshold; MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL_LARGE and MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE_LARGE are used when the message size is larger than this threshold.");
    MPIR_CVAR_REDUCE_INTRANODE_MSG_SIZE_THRESHOLD = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_REDUCE_INTRANODE_MSG_SIZE_THRESHOLD", &(MPIR_CVAR_REDUCE_INTRANODE_MSG_SIZE_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_REDUCE_INTRANODE_MSG_SIZE_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_REDUCE_INTRANODE_MSG_SIZE_THRESHOLD", &(MPIR_CVAR_REDUCE_INTRANODE_MSG_SIZE_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_REDUCE_INTRANODE_MSG_SIZE_THRESHOLD");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_REDUCE_INTRANODE_MSG_SIZE_THRESHOLD", &(MPIR_CVAR_REDUCE_INTRANODE_MSG_SIZE_THRESHOLD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_REDUCE_INTRANODE_MSG_SIZE_THRESHOLD");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_REDUCE_INTRANODE_MSG_SIZE_THRESHOLD = %d\n", MPIR_CVAR_REDUCE_INTRANODE_MSG_SIZE_THRESHOLD);
    }

    defaultval.d = 4;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL, /* name */
        &MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "K value for the kary/knomial tree for intra-node reduce");
    MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_REDUCE_INTRANODE_TREE_KVAL", &(MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_REDUCE_INTRANODE_TREE_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_REDUCE_INTRANODE_TREE_KVAL", &(MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_REDUCE_INTRANODE_TREE_KVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL", &(MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL = %d\n", MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL);
    }

    defaultval.d = 2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL_LARGE, /* name */
        &MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL_LARGE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "K value for the kary/knomial tree for intra-node reduce. Used for large messages.");
    MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL_LARGE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_REDUCE_INTRANODE_TREE_KVAL_LARGE", &(MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL_LARGE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_REDUCE_INTRANODE_TREE_KVAL_LARGE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_REDUCE_INTRANODE_TREE_KVAL_LARGE", &(MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL_LARGE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_REDUCE_INTRANODE_TREE_KVAL_LARGE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL_LARGE", &(MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL_LARGE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL_LARGE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL_LARGE = %d\n", MPIR_CVAR_REDUCE_INTRANODE_TREE_KVAL_LARGE);
    }

    defaultval.str = (const char *) "kary";
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_CHAR,
        MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE, /* name */
        &MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE, /* address */
        MPIR_CVAR_MAX_STRLEN, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Tree type for intra-node reduce tree kary      - kary tree type knomial_1 - knomial_1 tree type (ranks are added in order from the left side) knomial_2 - knomial_2 tree type (ranks are added in order from the right side) knomial_2 is only supported with non topology aware trees.");
    tmp_str = defaultval.str;
    got_rc = 0;
    rc = MPL_env2str("MPICH_REDUCE_INTRANODE_TREE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_REDUCE_INTRANODE_TREE_TYPE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_REDUCE_INTRANODE_TREE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_REDUCE_INTRANODE_TREE_TYPE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE = MPL_strdup(tmp_str);
        MPIR_CVAR_assert(MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE);
        if (MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE == NULL) {
            MPIR_CHKMEM_SETERR(mpi_errno, strlen(tmp_str), "dup of string for MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE");
            goto fn_fail;
        }
    }
    else {
        MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE = NULL;
    }

    defaultval.str = (const char *) "kary";
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_CHAR,
        MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE_LARGE, /* name */
        &MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE_LARGE, /* address */
        MPIR_CVAR_MAX_STRLEN, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Tree type for intra-node reduce tree. Used for large messages. kary      - kary tree type knomial_1 - knomial_1 tree type (ranks are added in order from the left side) knomial_2 - knomial_2 tree type (ranks are added in order from the right side) knomial_2 is only supported with non topology aware trees.");
    tmp_str = defaultval.str;
    got_rc = 0;
    rc = MPL_env2str("MPICH_REDUCE_INTRANODE_TREE_TYPE_LARGE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_REDUCE_INTRANODE_TREE_TYPE_LARGE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_REDUCE_INTRANODE_TREE_TYPE_LARGE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_REDUCE_INTRANODE_TREE_TYPE_LARGE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE_LARGE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE_LARGE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE_LARGE = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE_LARGE = MPL_strdup(tmp_str);
        MPIR_CVAR_assert(MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE_LARGE);
        if (MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE_LARGE == NULL) {
            MPIR_CHKMEM_SETERR(mpi_errno, strlen(tmp_str), "dup of string for MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE_LARGE");
            goto fn_fail;
        }
    }
    else {
        MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE_LARGE = NULL;
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ENABLE_INTRANODE_TOPOLOGY_AWARE_TREES, /* name */
        &MPIR_CVAR_ENABLE_INTRANODE_TOPOLOGY_AWARE_TREES, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Enable collective specific intra-node trees which leverage the memory hierarchy of a machine. Depends on hwloc to extract the binding information of each rank. Pick a leader rank per package (socket), then create a per_package tree for ranks on a same package, package leaders tree for package leaders. For Bcast - Assemble the per_package and package_leaders tree in such a way that leaders interact among themselves first before interacting with package local ranks. Both the package_leaders and per_package trees are left skewed (children are added from left to right, first child to be added is the first one to be processed in traversal) For Reduce - Assemble the per_package and package_leaders tree in such a way that a leader rank interacts with its package local ranks first, then with the other package leaders. Both the per_package and package_leaders tree is right skewed (children are added in reverse order, first child to be added is the last one to be processed in traversal) The tree radix and tree type of package_leaders and per_package tree is MPIR_CVAR_BCAST{REDUCE}_INTRANODE_TREE_KVAL and MPIR_CVAR_BCAST{REDUCE}_INTRANODE_TREE_TYPE respectively for bast and reduce. But of as now topology aware trees are only kary and knomial_1. knomial_2 is not implemented.");
    MPIR_CVAR_ENABLE_INTRANODE_TOPOLOGY_AWARE_TREES = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ENABLE_INTRANODE_TOPOLOGY_AWARE_TREES", &(MPIR_CVAR_ENABLE_INTRANODE_TOPOLOGY_AWARE_TREES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ENABLE_INTRANODE_TOPOLOGY_AWARE_TREES");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ENABLE_INTRANODE_TOPOLOGY_AWARE_TREES", &(MPIR_CVAR_ENABLE_INTRANODE_TOPOLOGY_AWARE_TREES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ENABLE_INTRANODE_TOPOLOGY_AWARE_TREES");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ENABLE_INTRANODE_TOPOLOGY_AWARE_TREES", &(MPIR_CVAR_ENABLE_INTRANODE_TOPOLOGY_AWARE_TREES));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ENABLE_INTRANODE_TOPOLOGY_AWARE_TREES");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ENABLE_INTRANODE_TOPOLOGY_AWARE_TREES = %d\n", MPIR_CVAR_ENABLE_INTRANODE_TOPOLOGY_AWARE_TREES);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BARRIER_COMPOSITION, /* name */
        &MPIR_CVAR_BARRIER_COMPOSITION, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Select composition (inter_node + intra_node) for Barrier 0 Auto selection 1 NM + SHM 2 NM only");
    MPIR_CVAR_BARRIER_COMPOSITION = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_BARRIER_COMPOSITION", &(MPIR_CVAR_BARRIER_COMPOSITION));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BARRIER_COMPOSITION");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_BARRIER_COMPOSITION", &(MPIR_CVAR_BARRIER_COMPOSITION));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BARRIER_COMPOSITION");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_BARRIER_COMPOSITION", &(MPIR_CVAR_BARRIER_COMPOSITION));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BARRIER_COMPOSITION");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BARRIER_COMPOSITION = %d\n", MPIR_CVAR_BARRIER_COMPOSITION);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_BCAST_COMPOSITION, /* name */
        &MPIR_CVAR_BCAST_COMPOSITION, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Select composition (inter_node + intra_node) for Bcast 0 Auto selection 1 NM + SHM with explicit send-recv between rank 0 and root 2 NM + SHM without the explicit send-recv 3 NM only");
    MPIR_CVAR_BCAST_COMPOSITION = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_BCAST_COMPOSITION", &(MPIR_CVAR_BCAST_COMPOSITION));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_BCAST_COMPOSITION");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_BCAST_COMPOSITION", &(MPIR_CVAR_BCAST_COMPOSITION));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_BCAST_COMPOSITION");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_BCAST_COMPOSITION", &(MPIR_CVAR_BCAST_COMPOSITION));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_BCAST_COMPOSITION");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_BCAST_COMPOSITION = %d\n", MPIR_CVAR_BCAST_COMPOSITION);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLREDUCE_COMPOSITION, /* name */
        &MPIR_CVAR_ALLREDUCE_COMPOSITION, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Select composition (inter_node + intra_node) for Allreduce 0 Auto selection 1 NM + SHM with reduce + bcast 2 NM only composition 3 SHM only composition 4 Multi leaders based inter node + intra node composition");
    MPIR_CVAR_ALLREDUCE_COMPOSITION = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLREDUCE_COMPOSITION", &(MPIR_CVAR_ALLREDUCE_COMPOSITION));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLREDUCE_COMPOSITION");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLREDUCE_COMPOSITION", &(MPIR_CVAR_ALLREDUCE_COMPOSITION));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLREDUCE_COMPOSITION");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLREDUCE_COMPOSITION", &(MPIR_CVAR_ALLREDUCE_COMPOSITION));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLREDUCE_COMPOSITION");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLREDUCE_COMPOSITION = %d\n", MPIR_CVAR_ALLREDUCE_COMPOSITION);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLGATHER_COMPOSITION, /* name */
        &MPIR_CVAR_ALLGATHER_COMPOSITION, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Select composition (inter_node + intra_node) for Allgather 0 Auto selection 1 Multi leaders based inter node + intra node composition 2 NM only composition");
    MPIR_CVAR_ALLGATHER_COMPOSITION = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLGATHER_COMPOSITION", &(MPIR_CVAR_ALLGATHER_COMPOSITION));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLGATHER_COMPOSITION");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLGATHER_COMPOSITION", &(MPIR_CVAR_ALLGATHER_COMPOSITION));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLGATHER_COMPOSITION");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLGATHER_COMPOSITION", &(MPIR_CVAR_ALLGATHER_COMPOSITION));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLGATHER_COMPOSITION");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLGATHER_COMPOSITION = %d\n", MPIR_CVAR_ALLGATHER_COMPOSITION);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLTOALL_COMPOSITION, /* name */
        &MPIR_CVAR_ALLTOALL_COMPOSITION, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Select composition (inter_node + intra_node) for Alltoall 0 Auto selection 1 Multi leaders based inter node + intra node composition 2 NM only composition");
    MPIR_CVAR_ALLTOALL_COMPOSITION = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLTOALL_COMPOSITION", &(MPIR_CVAR_ALLTOALL_COMPOSITION));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLTOALL_COMPOSITION");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLTOALL_COMPOSITION", &(MPIR_CVAR_ALLTOALL_COMPOSITION));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLTOALL_COMPOSITION");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLTOALL_COMPOSITION", &(MPIR_CVAR_ALLTOALL_COMPOSITION));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLTOALL_COMPOSITION");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLTOALL_COMPOSITION = %d\n", MPIR_CVAR_ALLTOALL_COMPOSITION);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_REDUCE_COMPOSITION, /* name */
        &MPIR_CVAR_REDUCE_COMPOSITION, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Select composition (inter_node + intra_node) for Reduce 0 Auto selection 1 NM + SHM with explicit send-recv between rank 0 and root 2 NM + SHM without the explicit send-recv 3 NM only");
    MPIR_CVAR_REDUCE_COMPOSITION = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_REDUCE_COMPOSITION", &(MPIR_CVAR_REDUCE_COMPOSITION));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_REDUCE_COMPOSITION");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_REDUCE_COMPOSITION", &(MPIR_CVAR_REDUCE_COMPOSITION));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_REDUCE_COMPOSITION");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_REDUCE_COMPOSITION", &(MPIR_CVAR_REDUCE_COMPOSITION));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_REDUCE_COMPOSITION");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_REDUCE_COMPOSITION = %d\n", MPIR_CVAR_REDUCE_COMPOSITION);
    }

    defaultval.d = 4096;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLTOALL_SHM_PER_RANK, /* name */
        &MPIR_CVAR_ALLTOALL_SHM_PER_RANK, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Shared memory region per rank for multi-leaders based composition for MPI_Alltoall (in bytes)");
    MPIR_CVAR_ALLTOALL_SHM_PER_RANK = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLTOALL_SHM_PER_RANK", &(MPIR_CVAR_ALLTOALL_SHM_PER_RANK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLTOALL_SHM_PER_RANK");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLTOALL_SHM_PER_RANK", &(MPIR_CVAR_ALLTOALL_SHM_PER_RANK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLTOALL_SHM_PER_RANK");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLTOALL_SHM_PER_RANK", &(MPIR_CVAR_ALLTOALL_SHM_PER_RANK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLTOALL_SHM_PER_RANK");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLTOALL_SHM_PER_RANK = %d\n", MPIR_CVAR_ALLTOALL_SHM_PER_RANK);
    }

    defaultval.d = 4096;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLGATHER_SHM_PER_RANK, /* name */
        &MPIR_CVAR_ALLGATHER_SHM_PER_RANK, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Shared memory region per rank for multi-leaders based composition for MPI_Allgather (in bytes)");
    MPIR_CVAR_ALLGATHER_SHM_PER_RANK = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLGATHER_SHM_PER_RANK", &(MPIR_CVAR_ALLGATHER_SHM_PER_RANK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLGATHER_SHM_PER_RANK");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLGATHER_SHM_PER_RANK", &(MPIR_CVAR_ALLGATHER_SHM_PER_RANK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLGATHER_SHM_PER_RANK");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLGATHER_SHM_PER_RANK", &(MPIR_CVAR_ALLGATHER_SHM_PER_RANK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLGATHER_SHM_PER_RANK");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLGATHER_SHM_PER_RANK = %d\n", MPIR_CVAR_ALLGATHER_SHM_PER_RANK);
    }

    defaultval.d = 4;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_NUM_MULTI_LEADS, /* name */
        &MPIR_CVAR_NUM_MULTI_LEADS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Number of leader ranks per node to be used for multi-leaders based collective algorithms");
    MPIR_CVAR_NUM_MULTI_LEADS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_NUM_MULTI_LEADS", &(MPIR_CVAR_NUM_MULTI_LEADS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_NUM_MULTI_LEADS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_NUM_MULTI_LEADS", &(MPIR_CVAR_NUM_MULTI_LEADS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_NUM_MULTI_LEADS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_NUM_MULTI_LEADS", &(MPIR_CVAR_NUM_MULTI_LEADS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_NUM_MULTI_LEADS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_NUM_MULTI_LEADS = %d\n", MPIR_CVAR_NUM_MULTI_LEADS);
    }

    defaultval.d = -1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLREDUCE_SHM_PER_LEADER, /* name */
        &MPIR_CVAR_ALLREDUCE_SHM_PER_LEADER, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Shared memory region per node-leader for multi-leaders based composition for MPI_Allreduce (in bytes) If it is undefined by the user, it is set to the message size of the first call to the algorithm. Max shared memory size is limited to 4MB.");
    MPIR_CVAR_ALLREDUCE_SHM_PER_LEADER = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLREDUCE_SHM_PER_LEADER", &(MPIR_CVAR_ALLREDUCE_SHM_PER_LEADER));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLREDUCE_SHM_PER_LEADER");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLREDUCE_SHM_PER_LEADER", &(MPIR_CVAR_ALLREDUCE_SHM_PER_LEADER));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLREDUCE_SHM_PER_LEADER");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLREDUCE_SHM_PER_LEADER", &(MPIR_CVAR_ALLREDUCE_SHM_PER_LEADER));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLREDUCE_SHM_PER_LEADER");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLREDUCE_SHM_PER_LEADER = %d\n", MPIR_CVAR_ALLREDUCE_SHM_PER_LEADER);
    }

    defaultval.d = 512;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLREDUCE_CACHE_PER_LEADER, /* name */
        &MPIR_CVAR_ALLREDUCE_CACHE_PER_LEADER, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Amount of data reduced in allreduce delta composition's reduce local step (in bytes). Smaller msg size per leader avoids cache misses and improves performance. Experiments indicate 512 to be the best value.");
    MPIR_CVAR_ALLREDUCE_CACHE_PER_LEADER = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLREDUCE_CACHE_PER_LEADER", &(MPIR_CVAR_ALLREDUCE_CACHE_PER_LEADER));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLREDUCE_CACHE_PER_LEADER");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLREDUCE_CACHE_PER_LEADER", &(MPIR_CVAR_ALLREDUCE_CACHE_PER_LEADER));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLREDUCE_CACHE_PER_LEADER");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLREDUCE_CACHE_PER_LEADER", &(MPIR_CVAR_ALLREDUCE_CACHE_PER_LEADER));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLREDUCE_CACHE_PER_LEADER");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLREDUCE_CACHE_PER_LEADER = %d\n", MPIR_CVAR_ALLREDUCE_CACHE_PER_LEADER);
    }

    defaultval.d = 2;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ALLREDUCE_LOCAL_COPY_OFFSETS, /* name */
        &MPIR_CVAR_ALLREDUCE_LOCAL_COPY_OFFSETS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "number of offsets in the allreduce delta composition's local copy The value of 2 performed the best in our 2 NIC test cases.");
    MPIR_CVAR_ALLREDUCE_LOCAL_COPY_OFFSETS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_ALLREDUCE_LOCAL_COPY_OFFSETS", &(MPIR_CVAR_ALLREDUCE_LOCAL_COPY_OFFSETS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ALLREDUCE_LOCAL_COPY_OFFSETS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_ALLREDUCE_LOCAL_COPY_OFFSETS", &(MPIR_CVAR_ALLREDUCE_LOCAL_COPY_OFFSETS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ALLREDUCE_LOCAL_COPY_OFFSETS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_ALLREDUCE_LOCAL_COPY_OFFSETS", &(MPIR_CVAR_ALLREDUCE_LOCAL_COPY_OFFSETS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ALLREDUCE_LOCAL_COPY_OFFSETS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ALLREDUCE_LOCAL_COPY_OFFSETS = %d\n", MPIR_CVAR_ALLREDUCE_LOCAL_COPY_OFFSETS);
    }

    defaultval.str = (const char *) "";
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_CHAR,
        MPIR_CVAR_CH4_NETMOD, /* name */
        &MPIR_CVAR_CH4_NETMOD, /* address */
        MPIR_CVAR_MAX_STRLEN, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH4", /* category */
        "If non-empty, this cvar specifies which network module to use");
    tmp_str = defaultval.str;
    got_rc = 0;
    rc = MPL_env2str("MPICH_CH4_NETMOD", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_NETMOD");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_CH4_NETMOD", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_NETMOD");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_CH4_NETMOD", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_NETMOD");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_NETMOD = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        MPIR_CVAR_CH4_NETMOD = MPL_strdup(tmp_str);
        MPIR_CVAR_assert(MPIR_CVAR_CH4_NETMOD);
        if (MPIR_CVAR_CH4_NETMOD == NULL) {
            MPIR_CHKMEM_SETERR(mpi_errno, strlen(tmp_str), "dup of string for MPIR_CVAR_CH4_NETMOD");
            goto fn_fail;
        }
    }
    else {
        MPIR_CVAR_CH4_NETMOD = NULL;
    }

    defaultval.str = (const char *) "";
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_CHAR,
        MPIR_CVAR_CH4_SHM, /* name */
        &MPIR_CVAR_CH4_SHM, /* address */
        MPIR_CVAR_MAX_STRLEN, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH4", /* category */
        "If non-empty, this cvar specifies which shm module to use");
    tmp_str = defaultval.str;
    got_rc = 0;
    rc = MPL_env2str("MPICH_CH4_SHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_SHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_CH4_SHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_SHM");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_CH4_SHM", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_SHM");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_SHM = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        MPIR_CVAR_CH4_SHM = MPL_strdup(tmp_str);
        MPIR_CVAR_assert(MPIR_CVAR_CH4_SHM);
        if (MPIR_CVAR_CH4_SHM == NULL) {
            MPIR_CHKMEM_SETERR(mpi_errno, strlen(tmp_str), "dup of string for MPIR_CVAR_CH4_SHM");
            goto fn_fail;
        }
    }
    else {
        MPIR_CVAR_CH4_SHM = NULL;
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_ROOTS_ONLY_PMI, /* name */
        &MPIR_CVAR_CH4_ROOTS_ONLY_PMI, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4", /* category */
        "Enables an optimized business card exchange over PMI for node root processes only.");
    MPIR_CVAR_CH4_ROOTS_ONLY_PMI = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_CH4_ROOTS_ONLY_PMI", &(MPIR_CVAR_CH4_ROOTS_ONLY_PMI));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_ROOTS_ONLY_PMI");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_CH4_ROOTS_ONLY_PMI", &(MPIR_CVAR_CH4_ROOTS_ONLY_PMI));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_ROOTS_ONLY_PMI");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_CH4_ROOTS_ONLY_PMI", &(MPIR_CVAR_CH4_ROOTS_ONLY_PMI));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_ROOTS_ONLY_PMI");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_ROOTS_ONLY_PMI = %d\n", MPIR_CVAR_CH4_ROOTS_ONLY_PMI);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_RUNTIME_CONF_DEBUG, /* name */
        &MPIR_CVAR_CH4_RUNTIME_CONF_DEBUG, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH4", /* category */
        "If enabled, CH4-level runtime configurations are printed out");
    MPIR_CVAR_CH4_RUNTIME_CONF_DEBUG = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_CH4_RUNTIME_CONF_DEBUG", &(MPIR_CVAR_CH4_RUNTIME_CONF_DEBUG));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_RUNTIME_CONF_DEBUG");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_CH4_RUNTIME_CONF_DEBUG", &(MPIR_CVAR_CH4_RUNTIME_CONF_DEBUG));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_RUNTIME_CONF_DEBUG");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_CH4_RUNTIME_CONF_DEBUG", &(MPIR_CVAR_CH4_RUNTIME_CONF_DEBUG));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_RUNTIME_CONF_DEBUG");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_RUNTIME_CONF_DEBUG = %d\n", MPIR_CVAR_CH4_RUNTIME_CONF_DEBUG);
    }

    defaultval.str = (const char *) "";
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_CHAR,
        MPIR_CVAR_CH4_MT_MODEL, /* name */
        &MPIR_CVAR_CH4_MT_MODEL, /* address */
        MPIR_CVAR_MAX_STRLEN, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH4", /* category */
        "Specifies the CH4 multi-threading model. Possible values are: direct (default) lockless");
    tmp_str = defaultval.str;
    got_rc = 0;
    rc = MPL_env2str("MPICH_CH4_MT_MODEL", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_MT_MODEL");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_CH4_MT_MODEL", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_MT_MODEL");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_CH4_MT_MODEL", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_MT_MODEL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_MT_MODEL = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        MPIR_CVAR_CH4_MT_MODEL = MPL_strdup(tmp_str);
        MPIR_CVAR_assert(MPIR_CVAR_CH4_MT_MODEL);
        if (MPIR_CVAR_CH4_MT_MODEL == NULL) {
            MPIR_CHKMEM_SETERR(mpi_errno, strlen(tmp_str), "dup of string for MPIR_CVAR_CH4_MT_MODEL");
            goto fn_fail;
        }
    }
    else {
        MPIR_CVAR_CH4_MT_MODEL = NULL;
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_NUM_VCIS, /* name */
        &MPIR_CVAR_CH4_NUM_VCIS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4", /* category */
        "Sets the number of VCIs to be implicitly used (should be a subset of MPIDI_CH4_MAX_VCIS).");
    MPIR_CVAR_CH4_NUM_VCIS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_NUM_VCIS", &(MPIR_CVAR_CH4_NUM_VCIS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_NUM_VCIS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_NUM_VCIS", &(MPIR_CVAR_CH4_NUM_VCIS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_NUM_VCIS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_NUM_VCIS", &(MPIR_CVAR_CH4_NUM_VCIS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_NUM_VCIS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_NUM_VCIS = %d\n", MPIR_CVAR_CH4_NUM_VCIS);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_RESERVE_VCIS, /* name */
        &MPIR_CVAR_CH4_RESERVE_VCIS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4", /* category */
        "Sets the number of VCIs that user can explicitly allocate (should be a subset of MPIDI_CH4_MAX_VCIS).");
    MPIR_CVAR_CH4_RESERVE_VCIS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_RESERVE_VCIS", &(MPIR_CVAR_CH4_RESERVE_VCIS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_RESERVE_VCIS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_RESERVE_VCIS", &(MPIR_CVAR_CH4_RESERVE_VCIS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_RESERVE_VCIS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_RESERVE_VCIS", &(MPIR_CVAR_CH4_RESERVE_VCIS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_RESERVE_VCIS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_RESERVE_VCIS = %d\n", MPIR_CVAR_CH4_RESERVE_VCIS);
    }

    defaultval.str = (const char *) "";
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_CHAR,
        MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE, /* name */
        &MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE, /* address */
        MPIR_CVAR_MAX_STRLEN, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Defines the location of tuning file.");
    tmp_str = defaultval.str;
    got_rc = 0;
    rc = MPL_env2str("MPICH_CH4_COLL_SELECTION_TUNING_JSON_FILE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_COLL_SELECTION_TUNING_JSON_FILE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_CH4_COLL_SELECTION_TUNING_JSON_FILE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_COLL_SELECTION_TUNING_JSON_FILE");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE = MPL_strdup(tmp_str);
        MPIR_CVAR_assert(MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE);
        if (MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE == NULL) {
            MPIR_CHKMEM_SETERR(mpi_errno, strlen(tmp_str), "dup of string for MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE");
            goto fn_fail;
        }
    }
    else {
        MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE = NULL;
    }

    defaultval.str = (const char *) "";
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_CHAR,
        MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE_GPU, /* name */
        &MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE_GPU, /* address */
        MPIR_CVAR_MAX_STRLEN, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Defines the location of tuning file for GPU.");
    tmp_str = defaultval.str;
    got_rc = 0;
    rc = MPL_env2str("MPICH_CH4_COLL_SELECTION_TUNING_JSON_FILE_GPU", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_COLL_SELECTION_TUNING_JSON_FILE_GPU");
    got_rc += rc;
    rc = MPL_env2str("MPIR_PARAM_CH4_COLL_SELECTION_TUNING_JSON_FILE_GPU", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_COLL_SELECTION_TUNING_JSON_FILE_GPU");
    got_rc += rc;
    rc = MPL_env2str("MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE_GPU", &tmp_str);
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE_GPU");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE_GPU = %s\n", tmp_str);
    }
    if (tmp_str != NULL) {
        MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE_GPU = MPL_strdup(tmp_str);
        MPIR_CVAR_assert(MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE_GPU);
        if (MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE_GPU == NULL) {
            MPIR_CHKMEM_SETERR(mpi_errno, strlen(tmp_str), "dup of string for MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE_GPU");
            goto fn_fail;
        }
    }
    else {
        MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE_GPU = NULL;
    }

    defaultval.d = 16384;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_IOV_DENSITY_MIN, /* name */
        &MPIR_CVAR_CH4_IOV_DENSITY_MIN, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4", /* category */
        "Defines the threshold of high-density datatype. The density is calculated by (datatype_size / datatype_num_contig_blocks).");
    MPIR_CVAR_CH4_IOV_DENSITY_MIN = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_IOV_DENSITY_MIN", &(MPIR_CVAR_CH4_IOV_DENSITY_MIN));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_IOV_DENSITY_MIN");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_IOV_DENSITY_MIN", &(MPIR_CVAR_CH4_IOV_DENSITY_MIN));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_IOV_DENSITY_MIN");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_IOV_DENSITY_MIN", &(MPIR_CVAR_CH4_IOV_DENSITY_MIN));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_IOV_DENSITY_MIN");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_IOV_DENSITY_MIN = %d\n", MPIR_CVAR_CH4_IOV_DENSITY_MIN);
    }

    defaultval.d = 16384;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_PACK_BUFFER_SIZE, /* name */
        &MPIR_CVAR_CH4_PACK_BUFFER_SIZE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4", /* category */
        "Specifies the number of buffers for packing/unpacking active messages in each block of the pool. The size here should be greater or equal to the max of the eager buffer limit of SHM and NETMOD.");
    MPIR_CVAR_CH4_PACK_BUFFER_SIZE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_PACK_BUFFER_SIZE", &(MPIR_CVAR_CH4_PACK_BUFFER_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_PACK_BUFFER_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_PACK_BUFFER_SIZE", &(MPIR_CVAR_CH4_PACK_BUFFER_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_PACK_BUFFER_SIZE");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_PACK_BUFFER_SIZE", &(MPIR_CVAR_CH4_PACK_BUFFER_SIZE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_PACK_BUFFER_SIZE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_PACK_BUFFER_SIZE = %d\n", MPIR_CVAR_CH4_PACK_BUFFER_SIZE);
    }

    defaultval.d = 64;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_NUM_PACK_BUFFERS_PER_CHUNK, /* name */
        &MPIR_CVAR_CH4_NUM_PACK_BUFFERS_PER_CHUNK, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4", /* category */
        "Specifies the number of buffers for packing/unpacking active messages in each block of the pool.");
    MPIR_CVAR_CH4_NUM_PACK_BUFFERS_PER_CHUNK = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_NUM_PACK_BUFFERS_PER_CHUNK", &(MPIR_CVAR_CH4_NUM_PACK_BUFFERS_PER_CHUNK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_NUM_PACK_BUFFERS_PER_CHUNK");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_NUM_PACK_BUFFERS_PER_CHUNK", &(MPIR_CVAR_CH4_NUM_PACK_BUFFERS_PER_CHUNK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_NUM_PACK_BUFFERS_PER_CHUNK");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_NUM_PACK_BUFFERS_PER_CHUNK", &(MPIR_CVAR_CH4_NUM_PACK_BUFFERS_PER_CHUNK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_NUM_PACK_BUFFERS_PER_CHUNK");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_NUM_PACK_BUFFERS_PER_CHUNK = %d\n", MPIR_CVAR_CH4_NUM_PACK_BUFFERS_PER_CHUNK);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_MAX_NUM_PACK_BUFFERS, /* name */
        &MPIR_CVAR_CH4_MAX_NUM_PACK_BUFFERS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4", /* category */
        "Specifies the max number of buffers for packing/unpacking buffers in the pool. Use 0 for unlimited.");
    MPIR_CVAR_CH4_MAX_NUM_PACK_BUFFERS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_MAX_NUM_PACK_BUFFERS", &(MPIR_CVAR_CH4_MAX_NUM_PACK_BUFFERS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_MAX_NUM_PACK_BUFFERS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_MAX_NUM_PACK_BUFFERS", &(MPIR_CVAR_CH4_MAX_NUM_PACK_BUFFERS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_MAX_NUM_PACK_BUFFERS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_MAX_NUM_PACK_BUFFERS", &(MPIR_CVAR_CH4_MAX_NUM_PACK_BUFFERS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_MAX_NUM_PACK_BUFFERS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_MAX_NUM_PACK_BUFFERS = %d\n", MPIR_CVAR_CH4_MAX_NUM_PACK_BUFFERS);
    }

    defaultval.d = 1048576;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_GPU_COLL_SWAP_BUFFER_SZ, /* name */
        &MPIR_CVAR_CH4_GPU_COLL_SWAP_BUFFER_SZ, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "Specifies the buffer size (in bytes) for GPU collectives data transfer.");
    MPIR_CVAR_CH4_GPU_COLL_SWAP_BUFFER_SZ = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_GPU_COLL_SWAP_BUFFER_SZ", &(MPIR_CVAR_CH4_GPU_COLL_SWAP_BUFFER_SZ));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_GPU_COLL_SWAP_BUFFER_SZ");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_GPU_COLL_SWAP_BUFFER_SZ", &(MPIR_CVAR_CH4_GPU_COLL_SWAP_BUFFER_SZ));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_GPU_COLL_SWAP_BUFFER_SZ");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_GPU_COLL_SWAP_BUFFER_SZ", &(MPIR_CVAR_CH4_GPU_COLL_SWAP_BUFFER_SZ));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_GPU_COLL_SWAP_BUFFER_SZ");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_GPU_COLL_SWAP_BUFFER_SZ = %d\n", MPIR_CVAR_CH4_GPU_COLL_SWAP_BUFFER_SZ);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_GPU_COLL_NUM_BUFFERS_PER_CHUNK, /* name */
        &MPIR_CVAR_CH4_GPU_COLL_NUM_BUFFERS_PER_CHUNK, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "Specifies the number of buffers for GPU collectives data transfer in each block/chunk of the pool.");
    MPIR_CVAR_CH4_GPU_COLL_NUM_BUFFERS_PER_CHUNK = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_GPU_COLL_NUM_BUFFERS_PER_CHUNK", &(MPIR_CVAR_CH4_GPU_COLL_NUM_BUFFERS_PER_CHUNK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_GPU_COLL_NUM_BUFFERS_PER_CHUNK");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_GPU_COLL_NUM_BUFFERS_PER_CHUNK", &(MPIR_CVAR_CH4_GPU_COLL_NUM_BUFFERS_PER_CHUNK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_GPU_COLL_NUM_BUFFERS_PER_CHUNK");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_GPU_COLL_NUM_BUFFERS_PER_CHUNK", &(MPIR_CVAR_CH4_GPU_COLL_NUM_BUFFERS_PER_CHUNK));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_GPU_COLL_NUM_BUFFERS_PER_CHUNK");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_GPU_COLL_NUM_BUFFERS_PER_CHUNK = %d\n", MPIR_CVAR_CH4_GPU_COLL_NUM_BUFFERS_PER_CHUNK);
    }

    defaultval.d = 256;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_GPU_COLL_MAX_NUM_BUFFERS, /* name */
        &MPIR_CVAR_CH4_GPU_COLL_MAX_NUM_BUFFERS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4_OFI", /* category */
        "Specifies the total number of buffers for GPU collectives data transfer.");
    MPIR_CVAR_CH4_GPU_COLL_MAX_NUM_BUFFERS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_GPU_COLL_MAX_NUM_BUFFERS", &(MPIR_CVAR_CH4_GPU_COLL_MAX_NUM_BUFFERS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_GPU_COLL_MAX_NUM_BUFFERS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_GPU_COLL_MAX_NUM_BUFFERS", &(MPIR_CVAR_CH4_GPU_COLL_MAX_NUM_BUFFERS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_GPU_COLL_MAX_NUM_BUFFERS");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_GPU_COLL_MAX_NUM_BUFFERS", &(MPIR_CVAR_CH4_GPU_COLL_MAX_NUM_BUFFERS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_GPU_COLL_MAX_NUM_BUFFERS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_GPU_COLL_MAX_NUM_BUFFERS = %d\n", MPIR_CVAR_CH4_GPU_COLL_MAX_NUM_BUFFERS);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_GLOBAL_PROGRESS, /* name */
        &MPIR_CVAR_CH4_GLOBAL_PROGRESS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4", /* category */
        "If on, poll global progress every once a while. With per-vci configuration, turning global progress off may improve the threading performance.");
    MPIR_CVAR_CH4_GLOBAL_PROGRESS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_CH4_GLOBAL_PROGRESS", &(MPIR_CVAR_CH4_GLOBAL_PROGRESS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_GLOBAL_PROGRESS");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_CH4_GLOBAL_PROGRESS", &(MPIR_CVAR_CH4_GLOBAL_PROGRESS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_GLOBAL_PROGRESS");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_CH4_GLOBAL_PROGRESS", &(MPIR_CVAR_CH4_GLOBAL_PROGRESS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_GLOBAL_PROGRESS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_GLOBAL_PROGRESS = %d\n", MPIR_CVAR_CH4_GLOBAL_PROGRESS);
    }

    defaultval.d = 180;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_COMM_CONNECT_TIMEOUT, /* name */
        &MPIR_CVAR_CH4_COMM_CONNECT_TIMEOUT, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_GROUP_EQ,
        defaultval,
        "CH4", /* category */
        "The default time out period in seconds for a connection attempt to the server communicator where the named port exists but no pending accept. User can change the value for a specified connection through its info argument.");
    MPIR_CVAR_CH4_COMM_CONNECT_TIMEOUT = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_COMM_CONNECT_TIMEOUT", &(MPIR_CVAR_CH4_COMM_CONNECT_TIMEOUT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_COMM_CONNECT_TIMEOUT");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_COMM_CONNECT_TIMEOUT", &(MPIR_CVAR_CH4_COMM_CONNECT_TIMEOUT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_COMM_CONNECT_TIMEOUT");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_COMM_CONNECT_TIMEOUT", &(MPIR_CVAR_CH4_COMM_CONNECT_TIMEOUT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_COMM_CONNECT_TIMEOUT");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_COMM_CONNECT_TIMEOUT = %d\n", MPIR_CVAR_CH4_COMM_CONNECT_TIMEOUT);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_ENABLE_STREAM_WORKQ, /* name */
        &MPIR_CVAR_CH4_ENABLE_STREAM_WORKQ, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4", /* category */
        "Enable stream enqueue operations via stream work queue. Requires progress thread on the corresponding MPIX stream. Reference: MPIX_Stream_progress and MPIX_Start_progress_thread.");
    MPIR_CVAR_CH4_ENABLE_STREAM_WORKQ = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_CH4_ENABLE_STREAM_WORKQ", &(MPIR_CVAR_CH4_ENABLE_STREAM_WORKQ));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_ENABLE_STREAM_WORKQ");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_CH4_ENABLE_STREAM_WORKQ", &(MPIR_CVAR_CH4_ENABLE_STREAM_WORKQ));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_ENABLE_STREAM_WORKQ");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_CH4_ENABLE_STREAM_WORKQ", &(MPIR_CVAR_CH4_ENABLE_STREAM_WORKQ));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_ENABLE_STREAM_WORKQ");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_ENABLE_STREAM_WORKQ = %d\n", MPIR_CVAR_CH4_ENABLE_STREAM_WORKQ);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_RMA_MEM_EFFICIENT, /* name */
        &MPIR_CVAR_CH4_RMA_MEM_EFFICIENT, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_GROUP_EQ,
        defaultval,
        "CH4", /* category */
        "If true, memory-saving mode is on, per-target object is released at the epoch end call. If false, performance-efficient mode is on, all allocated target objects are cached and freed at win_finalize.");
    MPIR_CVAR_CH4_RMA_MEM_EFFICIENT = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_CH4_RMA_MEM_EFFICIENT", &(MPIR_CVAR_CH4_RMA_MEM_EFFICIENT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_RMA_MEM_EFFICIENT");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_CH4_RMA_MEM_EFFICIENT", &(MPIR_CVAR_CH4_RMA_MEM_EFFICIENT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_RMA_MEM_EFFICIENT");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_CH4_RMA_MEM_EFFICIENT", &(MPIR_CVAR_CH4_RMA_MEM_EFFICIENT));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_RMA_MEM_EFFICIENT");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_RMA_MEM_EFFICIENT = %d\n", MPIR_CVAR_CH4_RMA_MEM_EFFICIENT);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS, /* name */
        &MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4", /* category */
        "If true, allows RMA synchronization calls to dynamically reduce the frequency of internal progress polling for incoming RMA active messages received on the target process. The RMA synchronization call initially polls progress with a low frequency (defined by MPIR_CVAR_CH4_RMA_AM_PROGRESS_LOW_FREQ_INTERVAL) to reduce synchronization overhead. Once any RMA active message has been received, it will always poll progress once at every synchronization call to ensure prompt target-side progress. Effective only for passive target synchronization MPI_Win_flush{_all} and MPI_Win_flush_local{_all}.");
    MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS", &(MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS", &(MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS", &(MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS = %d\n", MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_RMA_AM_PROGRESS_INTERVAL, /* name */
        &MPIR_CVAR_CH4_RMA_AM_PROGRESS_INTERVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4", /* category */
        "Specifies a static interval of progress polling for incoming RMA active messages received on the target process. Effective only for passive-target synchronization MPI_Win_flush{_all} and MPI_Win_flush_local{_all}. Interval indicates the number of performed flush calls before polling. It is counted globally across all windows. Invalid when MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS is true.");
    MPIR_CVAR_CH4_RMA_AM_PROGRESS_INTERVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_RMA_AM_PROGRESS_INTERVAL", &(MPIR_CVAR_CH4_RMA_AM_PROGRESS_INTERVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_RMA_AM_PROGRESS_INTERVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_RMA_AM_PROGRESS_INTERVAL", &(MPIR_CVAR_CH4_RMA_AM_PROGRESS_INTERVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_RMA_AM_PROGRESS_INTERVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_RMA_AM_PROGRESS_INTERVAL", &(MPIR_CVAR_CH4_RMA_AM_PROGRESS_INTERVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_RMA_AM_PROGRESS_INTERVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_RMA_AM_PROGRESS_INTERVAL = %d\n", MPIR_CVAR_CH4_RMA_AM_PROGRESS_INTERVAL);
    }

    defaultval.d = 100;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_CH4_RMA_AM_PROGRESS_LOW_FREQ_INTERVAL, /* name */
        &MPIR_CVAR_CH4_RMA_AM_PROGRESS_LOW_FREQ_INTERVAL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "CH4", /* category */
        "Specifies the interval of progress polling with low frequency for incoming RMA active message received on the target process. Effective only for passive-target synchronization MPI_Win_flush{_all} and MPI_Win_flush_local{_all}. Interval indicates the number of performed flush calls before polling. It is counted globally across all windows. Used when MPIR_CVAR_CH4_RMA_ENABLE_DYNAMIC_AM_PROGRESS is true.");
    MPIR_CVAR_CH4_RMA_AM_PROGRESS_LOW_FREQ_INTERVAL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_CH4_RMA_AM_PROGRESS_LOW_FREQ_INTERVAL", &(MPIR_CVAR_CH4_RMA_AM_PROGRESS_LOW_FREQ_INTERVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_CH4_RMA_AM_PROGRESS_LOW_FREQ_INTERVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_CH4_RMA_AM_PROGRESS_LOW_FREQ_INTERVAL", &(MPIR_CVAR_CH4_RMA_AM_PROGRESS_LOW_FREQ_INTERVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_CH4_RMA_AM_PROGRESS_LOW_FREQ_INTERVAL");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_CH4_RMA_AM_PROGRESS_LOW_FREQ_INTERVAL", &(MPIR_CVAR_CH4_RMA_AM_PROGRESS_LOW_FREQ_INTERVAL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_CH4_RMA_AM_PROGRESS_LOW_FREQ_INTERVAL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_CH4_RMA_AM_PROGRESS_LOW_FREQ_INTERVAL = %d\n", MPIR_CVAR_CH4_RMA_AM_PROGRESS_LOW_FREQ_INTERVAL);
    }

    defaultval.d = 1;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_GENQ_SHMEM_POOL_FREE_QUEUE_SENDER_SIDE, /* name */
        &MPIR_CVAR_GENQ_SHMEM_POOL_FREE_QUEUE_SENDER_SIDE, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "CH4", /* category */
        "The genq shmem code allocates pools of cells on each process and, when needed, a cell is removed from the pool and passed to another process. This can happen by either removing a cell from the pool of the sending process or from the pool of the receiving process. This CVAR determines which pool to use. If true, the cell will come from the sender-side. If false, the cell will com from the receiver-side. There are specific advantages of using receiver-side cells when combined with the \"avx\" fast configure option, which allows MPICH to use AVX streaming copy intrintrinsics, when available, to avoid polluting the cache of the sender with the data being copied to the receiver. Using receiver-side cells does have the trade-off of requiring an MPMC lock for the free queue rather than an MPSC lock, which is used for sender-side cells. Initial performance analysis shows that using the MPMC lock in this case had no significant performance loss. By default, the queue will continue to use sender-side queues until the performance impact is verified.");
    MPIR_CVAR_GENQ_SHMEM_POOL_FREE_QUEUE_SENDER_SIDE = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_GENQ_SHMEM_POOL_FREE_QUEUE_SENDER_SIDE", &(MPIR_CVAR_GENQ_SHMEM_POOL_FREE_QUEUE_SENDER_SIDE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_GENQ_SHMEM_POOL_FREE_QUEUE_SENDER_SIDE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_GENQ_SHMEM_POOL_FREE_QUEUE_SENDER_SIDE", &(MPIR_CVAR_GENQ_SHMEM_POOL_FREE_QUEUE_SENDER_SIDE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_GENQ_SHMEM_POOL_FREE_QUEUE_SENDER_SIDE");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_GENQ_SHMEM_POOL_FREE_QUEUE_SENDER_SIDE", &(MPIR_CVAR_GENQ_SHMEM_POOL_FREE_QUEUE_SENDER_SIDE));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_GENQ_SHMEM_POOL_FREE_QUEUE_SENDER_SIDE");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_GENQ_SHMEM_POOL_FREE_QUEUE_SENDER_SIDE = %d\n", MPIR_CVAR_GENQ_SHMEM_POOL_FREE_QUEUE_SENDER_SIDE);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ENABLE_HCOLL, /* name */
        &MPIR_CVAR_ENABLE_HCOLL, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "COLLECTIVE", /* category */
        "Enable hcoll collective support.");
    MPIR_CVAR_ENABLE_HCOLL = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_ENABLE_HCOLL", &(MPIR_CVAR_ENABLE_HCOLL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ENABLE_HCOLL");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_ENABLE_HCOLL", &(MPIR_CVAR_ENABLE_HCOLL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ENABLE_HCOLL");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_ENABLE_HCOLL", &(MPIR_CVAR_ENABLE_HCOLL));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ENABLE_HCOLL");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ENABLE_HCOLL = %d\n", MPIR_CVAR_ENABLE_HCOLL);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_COLL_SCHED_DUMP, /* name */
        &MPIR_CVAR_COLL_SCHED_DUMP, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_ALL_EQ,
        defaultval,
        "COLLECTIVE", /* category */
        "Print schedule data for nonblocking collective operations.");
    MPIR_CVAR_COLL_SCHED_DUMP = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_COLL_SCHED_DUMP", &(MPIR_CVAR_COLL_SCHED_DUMP));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_COLL_SCHED_DUMP");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_COLL_SCHED_DUMP", &(MPIR_CVAR_COLL_SCHED_DUMP));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_COLL_SCHED_DUMP");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_COLL_SCHED_DUMP", &(MPIR_CVAR_COLL_SCHED_DUMP));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_COLL_SCHED_DUMP");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_COLL_SCHED_DUMP = %d\n", MPIR_CVAR_COLL_SCHED_DUMP);
    }

    defaultval.d = 100;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_SHM_RANDOM_ADDR_RETRY, /* name */
        &MPIR_CVAR_SHM_RANDOM_ADDR_RETRY, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_GROUP,
        defaultval,
        "MEMORY", /* category */
        "The default number of retries for generating a random address. A retrying involves only local operations.");
    MPIR_CVAR_SHM_RANDOM_ADDR_RETRY = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_SHM_RANDOM_ADDR_RETRY", &(MPIR_CVAR_SHM_RANDOM_ADDR_RETRY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_SHM_RANDOM_ADDR_RETRY");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_SHM_RANDOM_ADDR_RETRY", &(MPIR_CVAR_SHM_RANDOM_ADDR_RETRY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_SHM_RANDOM_ADDR_RETRY");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_SHM_RANDOM_ADDR_RETRY", &(MPIR_CVAR_SHM_RANDOM_ADDR_RETRY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_SHM_RANDOM_ADDR_RETRY");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_SHM_RANDOM_ADDR_RETRY = %d\n", MPIR_CVAR_SHM_RANDOM_ADDR_RETRY);
    }

    defaultval.d = 100;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_SHM_SYMHEAP_RETRY, /* name */
        &MPIR_CVAR_SHM_SYMHEAP_RETRY, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_GROUP,
        defaultval,
        "MEMORY", /* category */
        "The default number of retries for allocating a symmetric heap in shared memory. A retrying involves collective communication over the group in the shared memory.");
    MPIR_CVAR_SHM_SYMHEAP_RETRY = defaultval.d;
    got_rc = 0;
    rc = MPL_env2int("MPICH_SHM_SYMHEAP_RETRY", &(MPIR_CVAR_SHM_SYMHEAP_RETRY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_SHM_SYMHEAP_RETRY");
    got_rc += rc;
    rc = MPL_env2int("MPIR_PARAM_SHM_SYMHEAP_RETRY", &(MPIR_CVAR_SHM_SYMHEAP_RETRY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_SHM_SYMHEAP_RETRY");
    got_rc += rc;
    rc = MPL_env2int("MPIR_CVAR_SHM_SYMHEAP_RETRY", &(MPIR_CVAR_SHM_SYMHEAP_RETRY));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_SHM_SYMHEAP_RETRY");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_SHM_SYMHEAP_RETRY = %d\n", MPIR_CVAR_SHM_SYMHEAP_RETRY);
    }

    defaultval.d = 0;
    MPIR_T_CVAR_REGISTER_STATIC(
        MPI_INT,
        MPIR_CVAR_ENABLE_HEAVY_YIELD, /* name */
        &MPIR_CVAR_ENABLE_HEAVY_YIELD, /* address */
        1, /* count */
        MPI_T_VERBOSITY_USER_BASIC,
        MPI_T_SCOPE_LOCAL,
        defaultval,
        "THREADS", /* category */
        "If enabled, use nanosleep to ensure other threads have a chance to grab the lock. Note: this may not work with some thread runtimes, e.g. non-preemptive user-level threads.");
    MPIR_CVAR_ENABLE_HEAVY_YIELD = defaultval.d;
    got_rc = 0;
    rc = MPL_env2bool("MPICH_ENABLE_HEAVY_YIELD", &(MPIR_CVAR_ENABLE_HEAVY_YIELD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPICH_ENABLE_HEAVY_YIELD");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_PARAM_ENABLE_HEAVY_YIELD", &(MPIR_CVAR_ENABLE_HEAVY_YIELD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_PARAM_ENABLE_HEAVY_YIELD");
    got_rc += rc;
    rc = MPL_env2bool("MPIR_CVAR_ENABLE_HEAVY_YIELD", &(MPIR_CVAR_ENABLE_HEAVY_YIELD));
    MPIR_ERR_CHKANDJUMP1((-1 == rc),mpi_errno,MPI_ERR_OTHER,"**envvarparse","**envvarparse %s","MPIR_CVAR_ENABLE_HEAVY_YIELD");
    got_rc += rc;
    if (got_rc && debug) {
        printf("CVAR: MPIR_CVAR_ENABLE_HEAVY_YIELD = %d\n", MPIR_CVAR_ENABLE_HEAVY_YIELD);
    }

fn_exit:
    return mpi_errno;
fn_fail:
    goto fn_exit;
}

int MPIR_T_cvar_finalize(void)
{
    int mpi_errno = MPI_SUCCESS;

    MPL_free((char *)MPIR_CVAR_BCAST_TREE_TYPE);
    MPIR_CVAR_BCAST_TREE_TYPE = NULL;

    MPL_free((char *)MPIR_CVAR_IBCAST_TREE_TYPE);
    MPIR_CVAR_IBCAST_TREE_TYPE = NULL;

    MPL_free((char *)MPIR_CVAR_IREDUCE_TREE_TYPE);
    MPIR_CVAR_IREDUCE_TREE_TYPE = NULL;

    MPL_free((char *)MPIR_CVAR_ALLREDUCE_TREE_TYPE);
    MPIR_CVAR_ALLREDUCE_TREE_TYPE = NULL;

    MPL_free((char *)MPIR_CVAR_IALLREDUCE_TREE_TYPE);
    MPIR_CVAR_IALLREDUCE_TREE_TYPE = NULL;

    MPL_free((char *)MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE);
    MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE = NULL;

    MPL_free((char *)MPIR_CVAR_COORDINATES_FILE);
    MPIR_CVAR_COORDINATES_FILE = NULL;

    MPL_free((char *)MPIR_CVAR_PROGRESS_THREAD_AFFINITY);
    MPIR_CVAR_PROGRESS_THREAD_AFFINITY = NULL;

    MPL_free((char *)MPIR_CVAR_DEFAULT_THREAD_LEVEL);
    MPIR_CVAR_DEFAULT_THREAD_LEVEL = NULL;

    MPL_free((char *)MPIR_CVAR_QMPI_TOOL_LIST);
    MPIR_CVAR_QMPI_TOOL_LIST = NULL;

    MPL_free((char *)MPIR_CVAR_NAMESERV_FILE_PUBDIR);
    MPIR_CVAR_NAMESERV_FILE_PUBDIR = NULL;

    MPL_free((char *)MPIR_CVAR_NETLOC_NODE_FILE);
    MPIR_CVAR_NETLOC_NODE_FILE = NULL;

    MPL_free((char *)MPIR_CVAR_CH3_INTERFACE_HOSTNAME);
    MPIR_CVAR_CH3_INTERFACE_HOSTNAME = NULL;

    MPL_free((char *)MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE);
    MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE = NULL;

    MPL_free((char *)MPIR_CVAR_NEMESIS_NETMOD);
    MPIR_CVAR_NEMESIS_NETMOD = NULL;

    MPL_free((char *)MPIR_CVAR_OFI_USE_PROVIDER);
    MPIR_CVAR_OFI_USE_PROVIDER = NULL;

    MPL_free((char *)MPIR_CVAR_CH4_SHM_POSIX_EAGER);
    MPIR_CVAR_CH4_SHM_POSIX_EAGER = NULL;

    MPL_free((char *)MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE);
    MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE = NULL;

    MPL_free((char *)MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE_GPU);
    MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE_GPU = NULL;

    MPL_free((char *)MPIR_CVAR_BCAST_INTRANODE_TREE_TYPE);
    MPIR_CVAR_BCAST_INTRANODE_TREE_TYPE = NULL;

    MPL_free((char *)MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE);
    MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE = NULL;

    MPL_free((char *)MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE_LARGE);
    MPIR_CVAR_REDUCE_INTRANODE_TREE_TYPE_LARGE = NULL;

    MPL_free((char *)MPIR_CVAR_CH4_NETMOD);
    MPIR_CVAR_CH4_NETMOD = NULL;

    MPL_free((char *)MPIR_CVAR_CH4_SHM);
    MPIR_CVAR_CH4_SHM = NULL;

    MPL_free((char *)MPIR_CVAR_CH4_MT_MODEL);
    MPIR_CVAR_CH4_MT_MODEL = NULL;

    MPL_free((char *)MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE);
    MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE = NULL;

    MPL_free((char *)MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE_GPU);
    MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE_GPU = NULL;

    return mpi_errno;
}

int MPIR_MPIR_CVAR_GROUP_COLL_ALGO_from_str(const char *s) {
    if (strcmp(s, "    MPIR_CVAR_BARRIER_POSIX_INTRA_ALGORITHM_mpir")==0) return     MPIR_CVAR_BARRIER_POSIX_INTRA_ALGORITHM_mpir;
    else if (strcmp(s, "    MPIR_CVAR_BARRIER_POSIX_INTRA_ALGORITHM_release_gather")==0) return     MPIR_CVAR_BARRIER_POSIX_INTRA_ALGORITHM_release_gather;
    else if (strcmp(s, "    MPIR_CVAR_BARRIER_POSIX_INTRA_ALGORITHM_auto")==0) return     MPIR_CVAR_BARRIER_POSIX_INTRA_ALGORITHM_auto;
    else return -1;
}
